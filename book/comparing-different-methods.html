<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Comparing Different Methods | Predictive Learning in R</title>
  <meta name="description" content="Chapter 9 Comparing Different Methods | Predictive Learning in R" />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Comparing Different Methods | Predictive Learning in R" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Comparing Different Methods | Predictive Learning in R" />
  
  
  

<meta name="author" content="Steve Pittard - wsp@emory.edu" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="decision-trees.html"/>
<link rel="next" href="selecting-the-best-model.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#machine-learning"><i class="fa fa-check"></i><b>1.1</b> Machine Learning</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#predictive-modeling"><i class="fa fa-check"></i><b>1.2</b> Predictive Modeling</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#in-sample-vs-out-of-sample-error"><i class="fa fa-check"></i><b>1.3</b> In-Sample vs Out-Of-Sample Error</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#performance-metrics"><i class="fa fa-check"></i><b>1.4</b> Performance Metrics</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#black-box"><i class="fa fa-check"></i><b>1.5</b> Black Box</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html"><i class="fa fa-check"></i><b>2</b> Predictive / Supervised Learning</a><ul>
<li class="chapter" data-level="2.1" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#explanation-vs-prediction"><i class="fa fa-check"></i><b>2.1</b> Explanation vs Prediction</a><ul>
<li class="chapter" data-level="2.1.1" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#titanic-data"><i class="fa fa-check"></i><b>2.1.1</b> Titanic Data</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#bias-vs-variance"><i class="fa fa-check"></i><b>2.2</b> Bias vs Variance</a><ul>
<li class="chapter" data-level="2.2.1" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#bias"><i class="fa fa-check"></i><b>2.2.1</b> Bias</a></li>
<li class="chapter" data-level="2.2.2" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#variance"><i class="fa fa-check"></i><b>2.2.2</b> Variance</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#overfitting-and-underfitting"><i class="fa fa-check"></i><b>2.3</b> Overfitting and Underfitting</a></li>
<li class="chapter" data-level="2.4" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#some-important-terminology"><i class="fa fa-check"></i><b>2.4</b> Some Important Terminology</a></li>
<li class="chapter" data-level="2.5" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#levels-of-measurement"><i class="fa fa-check"></i><b>2.5</b> Levels of Measurement</a><ul>
<li class="chapter" data-level="2.5.1" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#nominal"><i class="fa fa-check"></i><b>2.5.1</b> Nominal</a></li>
<li class="chapter" data-level="2.5.2" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#ordinal"><i class="fa fa-check"></i><b>2.5.2</b> Ordinal</a></li>
<li class="chapter" data-level="2.5.3" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#interval"><i class="fa fa-check"></i><b>2.5.3</b> Interval</a></li>
<li class="chapter" data-level="2.5.4" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#ratio"><i class="fa fa-check"></i><b>2.5.4</b> Ratio</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="a-motivating-example.html"><a href="a-motivating-example.html"><i class="fa fa-check"></i><b>3</b> A Motivating Example</a><ul>
<li class="chapter" data-level="3.1" data-path="a-motivating-example.html"><a href="a-motivating-example.html#a-more-detailed-workflow"><i class="fa fa-check"></i><b>3.1</b> A More Detailed Workflow</a></li>
<li class="chapter" data-level="3.2" data-path="a-motivating-example.html"><a href="a-motivating-example.html#visualizations"><i class="fa fa-check"></i><b>3.2</b> Visualizations</a><ul>
<li class="chapter" data-level="3.2.1" data-path="a-motivating-example.html"><a href="a-motivating-example.html#scatterplots"><i class="fa fa-check"></i><b>3.2.1</b> Scatterplots</a></li>
<li class="chapter" data-level="3.2.2" data-path="a-motivating-example.html"><a href="a-motivating-example.html#boxplots"><i class="fa fa-check"></i><b>3.2.2</b> Boxplots</a></li>
<li class="chapter" data-level="3.2.3" data-path="a-motivating-example.html"><a href="a-motivating-example.html#histograms"><i class="fa fa-check"></i><b>3.2.3</b> Histograms</a></li>
<li class="chapter" data-level="3.2.4" data-path="a-motivating-example.html"><a href="a-motivating-example.html#tables"><i class="fa fa-check"></i><b>3.2.4</b> Tables</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="a-motivating-example.html"><a href="a-motivating-example.html#correlations"><i class="fa fa-check"></i><b>3.3</b> Correlations</a></li>
<li class="chapter" data-level="3.4" data-path="a-motivating-example.html"><a href="a-motivating-example.html#building-a-model---in-sample-error"><i class="fa fa-check"></i><b>3.4</b> Building A Model - In Sample Error</a></li>
<li class="chapter" data-level="3.5" data-path="a-motivating-example.html"><a href="a-motivating-example.html#out-of-sample-data"><i class="fa fa-check"></i><b>3.5</b> Out Of Sample Data</a></li>
<li class="chapter" data-level="3.6" data-path="a-motivating-example.html"><a href="a-motivating-example.html#some-additional-considerations"><i class="fa fa-check"></i><b>3.6</b> Some Additional Considerations</a></li>
<li class="chapter" data-level="3.7" data-path="a-motivating-example.html"><a href="a-motivating-example.html#other-methods"><i class="fa fa-check"></i><b>3.7</b> Other Methods ?</a></li>
<li class="chapter" data-level="3.8" data-path="a-motivating-example.html"><a href="a-motivating-example.html#summary"><i class="fa fa-check"></i><b>3.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="training-test-data.html"><a href="training-test-data.html"><i class="fa fa-check"></i><b>4</b> Training / Test Data</a><ul>
<li class="chapter" data-level="4.1" data-path="training-test-data.html"><a href="training-test-data.html#cross-fold-validation"><i class="fa fa-check"></i><b>4.1</b> Cross Fold Validation</a></li>
<li class="chapter" data-level="4.2" data-path="training-test-data.html"><a href="training-test-data.html#create-a-function-to-automate-things"><i class="fa fa-check"></i><b>4.2</b> Create A Function To Automate Things</a></li>
<li class="chapter" data-level="4.3" data-path="training-test-data.html"><a href="training-test-data.html#repeated-cross-validation"><i class="fa fa-check"></i><b>4.3</b> Repeated Cross Validation</a></li>
<li class="chapter" data-level="4.4" data-path="training-test-data.html"><a href="training-test-data.html#bootstrap"><i class="fa fa-check"></i><b>4.4</b> Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="caret-package.html"><a href="caret-package.html"><i class="fa fa-check"></i><b>5</b> Caret Package</a><ul>
<li class="chapter" data-level="5.1" data-path="caret-package.html"><a href="caret-package.html#putting-caret-to-work"><i class="fa fa-check"></i><b>5.1</b> Putting caret To Work</a></li>
<li class="chapter" data-level="5.2" data-path="caret-package.html"><a href="caret-package.html#back-to-the-beginning"><i class="fa fa-check"></i><b>5.2</b> Back To The Beginning</a></li>
<li class="chapter" data-level="5.3" data-path="caret-package.html"><a href="caret-package.html#splitting"><i class="fa fa-check"></i><b>5.3</b> Splitting</a></li>
<li class="chapter" data-level="5.4" data-path="caret-package.html"><a href="caret-package.html#calling-the-train-function"><i class="fa fa-check"></i><b>5.4</b> Calling The train() Function</a></li>
<li class="chapter" data-level="5.5" data-path="caret-package.html"><a href="caret-package.html#reproducible-results"><i class="fa fa-check"></i><b>5.5</b> Reproducible Results</a></li>
<li class="chapter" data-level="5.6" data-path="caret-package.html"><a href="caret-package.html#one-size-fits-all"><i class="fa fa-check"></i><b>5.6</b> One Size Fits All</a></li>
<li class="chapter" data-level="5.7" data-path="caret-package.html"><a href="caret-package.html#alternative-calling-sequence"><i class="fa fa-check"></i><b>5.7</b> Alternative Calling Sequence</a></li>
<li class="chapter" data-level="5.8" data-path="caret-package.html"><a href="caret-package.html#hyperparameters"><i class="fa fa-check"></i><b>5.8</b> Hyperparameters</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="classification-problems.html"><a href="classification-problems.html"><i class="fa fa-check"></i><b>6</b> Classification Problems</a><ul>
<li class="chapter" data-level="6.1" data-path="classification-problems.html"><a href="classification-problems.html#performance-measures"><i class="fa fa-check"></i><b>6.1</b> Performance Measures</a></li>
<li class="chapter" data-level="6.2" data-path="classification-problems.html"><a href="classification-problems.html#important-terminology"><i class="fa fa-check"></i><b>6.2</b> Important Terminology</a></li>
<li class="chapter" data-level="6.3" data-path="classification-problems.html"><a href="classification-problems.html#a-basic-model"><i class="fa fa-check"></i><b>6.3</b> A Basic Model</a></li>
<li class="chapter" data-level="6.4" data-path="classification-problems.html"><a href="classification-problems.html#selecting-the-correct-threshold-alpha"><i class="fa fa-check"></i><b>6.4</b> Selecting The Correct Threshold / Alpha</a><ul>
<li class="chapter" data-level="6.4.1" data-path="classification-problems.html"><a href="classification-problems.html#moving-the-threshold"><i class="fa fa-check"></i><b>6.4.1</b> Moving The Threshold</a></li>
<li class="chapter" data-level="6.4.2" data-path="classification-problems.html"><a href="classification-problems.html#distribution-of-predicted-probabilities"><i class="fa fa-check"></i><b>6.4.2</b> Distribution of Predicted Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="classification-problems.html"><a href="classification-problems.html#hypothesis-testing"><i class="fa fa-check"></i><b>6.5</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="6.6" data-path="classification-problems.html"><a href="classification-problems.html#confusion-matrix"><i class="fa fa-check"></i><b>6.6</b> Confusion Matrix</a><ul>
<li class="chapter" data-level="6.6.1" data-path="classification-problems.html"><a href="classification-problems.html#computing-performance-metrics"><i class="fa fa-check"></i><b>6.6.1</b> Computing Performance Metrics</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="classification-problems.html"><a href="classification-problems.html#picking-the-right-metric"><i class="fa fa-check"></i><b>6.7</b> Picking the Right Metric</a></li>
<li class="chapter" data-level="6.8" data-path="classification-problems.html"><a href="classification-problems.html#wait.-where-are-we"><i class="fa fa-check"></i><b>6.8</b> Wait. Where Are We ?</a></li>
<li class="chapter" data-level="6.9" data-path="classification-problems.html"><a href="classification-problems.html#other-ways-to-compute-the-roc-curve"><i class="fa fa-check"></i><b>6.9</b> Other Ways To Compute The ROC Curve</a></li>
<li class="chapter" data-level="6.10" data-path="classification-problems.html"><a href="classification-problems.html#roc-curve-summary"><i class="fa fa-check"></i><b>6.10</b> ROC Curve Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="classification-example.html"><a href="classification-example.html"><i class="fa fa-check"></i><b>7</b> Classification Example</a><ul>
<li class="chapter" data-level="7.1" data-path="classification-example.html"><a href="classification-example.html#exploratory-plots"><i class="fa fa-check"></i><b>7.1</b> Exploratory Plots</a></li>
<li class="chapter" data-level="7.2" data-path="classification-example.html"><a href="classification-example.html#generalized-linear-models"><i class="fa fa-check"></i><b>7.2</b> Generalized Linear Models</a></li>
<li class="chapter" data-level="7.3" data-path="classification-example.html"><a href="classification-example.html#random-forests"><i class="fa fa-check"></i><b>7.3</b> Random Forests</a></li>
<li class="chapter" data-level="7.4" data-path="classification-example.html"><a href="classification-example.html#feature-importance"><i class="fa fa-check"></i><b>7.4</b> Feature Importance</a></li>
<li class="chapter" data-level="7.5" data-path="classification-example.html"><a href="classification-example.html#target-variable-format"><i class="fa fa-check"></i><b>7.5</b> Target Variable Format</a></li>
<li class="chapter" data-level="7.6" data-path="classification-example.html"><a href="classification-example.html#addressing-class-imbalance"><i class="fa fa-check"></i><b>7.6</b> Addressing Class Imbalance</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>8</b> Decision Trees</a><ul>
<li class="chapter" data-level="8.1" data-path="decision-trees.html"><a href="decision-trees.html#advantages"><i class="fa fa-check"></i><b>8.1</b> Advantages</a></li>
<li class="chapter" data-level="8.2" data-path="decision-trees.html"><a href="decision-trees.html#a-classification-example"><i class="fa fa-check"></i><b>8.2</b> A Classification Example</a></li>
<li class="chapter" data-level="8.3" data-path="decision-trees.html"><a href="decision-trees.html#digging-deeper"><i class="fa fa-check"></i><b>8.3</b> Digging Deeper</a><ul>
<li class="chapter" data-level="8.3.1" data-path="decision-trees.html"><a href="decision-trees.html#evaluating-performance"><i class="fa fa-check"></i><b>8.3.1</b> Evaluating performance</a></li>
<li class="chapter" data-level="8.3.2" data-path="decision-trees.html"><a href="decision-trees.html#tree-splitting"><i class="fa fa-check"></i><b>8.3.2</b> Tree Splitting</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="decision-trees.html"><a href="decision-trees.html#gini-index"><i class="fa fa-check"></i><b>8.4</b> Gini Index</a></li>
<li class="chapter" data-level="8.5" data-path="decision-trees.html"><a href="decision-trees.html#regression-trees"><i class="fa fa-check"></i><b>8.5</b> Regression Trees</a><ul>
<li class="chapter" data-level="8.5.1" data-path="decision-trees.html"><a href="decision-trees.html#performance-measure"><i class="fa fa-check"></i><b>8.5.1</b> Performance Measure</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="decision-trees.html"><a href="decision-trees.html#parameters-vs-hyperparameters"><i class="fa fa-check"></i><b>8.6</b> Parameters vs Hyperparameters</a></li>
<li class="chapter" data-level="8.7" data-path="decision-trees.html"><a href="decision-trees.html#grid-searching"><i class="fa fa-check"></i><b>8.7</b> Grid Searching</a></li>
<li class="chapter" data-level="8.8" data-path="decision-trees.html"><a href="decision-trees.html#bagged-trees"><i class="fa fa-check"></i><b>8.8</b> Bagged Trees</a></li>
<li class="chapter" data-level="8.9" data-path="decision-trees.html"><a href="decision-trees.html#random-forests-1"><i class="fa fa-check"></i><b>8.9</b> Random Forests</a></li>
<li class="chapter" data-level="8.10" data-path="decision-trees.html"><a href="decision-trees.html#boosted-trees"><i class="fa fa-check"></i><b>8.10</b> Boosted Trees</a></li>
<li class="chapter" data-level="8.11" data-path="decision-trees.html"><a href="decision-trees.html#using-caret"><i class="fa fa-check"></i><b>8.11</b> Using caret</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="comparing-different-methods.html"><a href="comparing-different-methods.html"><i class="fa fa-check"></i><b>9</b> Comparing Different Methods</a><ul>
<li class="chapter" data-level="9.1" data-path="comparing-different-methods.html"><a href="comparing-different-methods.html#parameters-vs-hyperparameters-1"><i class="fa fa-check"></i><b>9.1</b> Parameters vs Hyperparameters</a></li>
<li class="chapter" data-level="9.2" data-path="comparing-different-methods.html"><a href="comparing-different-methods.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>9.2</b> Hyperparameter Tuning</a><ul>
<li class="chapter" data-level="9.2.1" data-path="comparing-different-methods.html"><a href="comparing-different-methods.html#multiple-hyperparameters"><i class="fa fa-check"></i><b>9.2.1</b> Multiple Hyperparameters ?</a></li>
<li class="chapter" data-level="9.2.2" data-path="comparing-different-methods.html"><a href="comparing-different-methods.html#custom-tuning-grid"><i class="fa fa-check"></i><b>9.2.2</b> Custom Tuning Grid</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="comparing-different-methods.html"><a href="comparing-different-methods.html#applied-to-classification"><i class="fa fa-check"></i><b>9.3</b> Applied To Classification</a></li>
<li class="chapter" data-level="9.4" data-path="comparing-different-methods.html"><a href="comparing-different-methods.html#using-validation-data-sets"><i class="fa fa-check"></i><b>9.4</b> Using Validation Data Sets</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="selecting-the-best-model.html"><a href="selecting-the-best-model.html"><i class="fa fa-check"></i><b>10</b> Selecting The Best Model</a><ul>
<li class="chapter" data-level="10.1" data-path="selecting-the-best-model.html"><a href="selecting-the-best-model.html#an-example"><i class="fa fa-check"></i><b>10.1</b> An Example</a></li>
<li class="chapter" data-level="10.2" data-path="selecting-the-best-model.html"><a href="selecting-the-best-model.html#more-comparisons"><i class="fa fa-check"></i><b>10.2</b> More Comparisons</a></li>
<li class="chapter" data-level="10.3" data-path="selecting-the-best-model.html"><a href="selecting-the-best-model.html#using-the-resamples-function"><i class="fa fa-check"></i><b>10.3</b> Using the resamples() function</a></li>
<li class="chapter" data-level="10.4" data-path="selecting-the-best-model.html"><a href="selecting-the-best-model.html#model-performance"><i class="fa fa-check"></i><b>10.4</b> Model Performance</a></li>
<li class="chapter" data-level="10.5" data-path="selecting-the-best-model.html"><a href="selecting-the-best-model.html#feature-evaluation"><i class="fa fa-check"></i><b>10.5</b> Feature Evaluation</a><ul>
<li class="chapter" data-level="10.5.1" data-path="selecting-the-best-model.html"><a href="selecting-the-best-model.html#identifying-redundant-features"><i class="fa fa-check"></i><b>10.5.1</b> Identifying Redundant Features</a></li>
<li class="chapter" data-level="10.5.2" data-path="selecting-the-best-model.html"><a href="selecting-the-best-model.html#highly-correlated-variables"><i class="fa fa-check"></i><b>10.5.2</b> Highly Correlated Variables</a></li>
<li class="chapter" data-level="10.5.3" data-path="selecting-the-best-model.html"><a href="selecting-the-best-model.html#ranking-features"><i class="fa fa-check"></i><b>10.5.3</b> Ranking Features</a></li>
<li class="chapter" data-level="10.5.4" data-path="selecting-the-best-model.html"><a href="selecting-the-best-model.html#feature-selection"><i class="fa fa-check"></i><b>10.5.4</b> Feature Selection</a></li>
<li class="chapter" data-level="10.5.5" data-path="selecting-the-best-model.html"><a href="selecting-the-best-model.html#recursive-feature-elimination"><i class="fa fa-check"></i><b>10.5.5</b> Recursive Feature Elimination</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="data-pre-processing.html"><a href="data-pre-processing.html"><i class="fa fa-check"></i><b>11</b> Data Pre Processing</a><ul>
<li class="chapter" data-level="11.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#types-of-pre-processing"><i class="fa fa-check"></i><b>11.1</b> Types of Pre Processing</a></li>
<li class="chapter" data-level="11.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#missing-values"><i class="fa fa-check"></i><b>11.2</b> Missing Values</a><ul>
<li class="chapter" data-level="11.2.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#finding-rows-with-missing-data"><i class="fa fa-check"></i><b>11.2.1</b> Finding Rows with Missing Data</a></li>
<li class="chapter" data-level="11.2.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#finding-columns-with-missing-data"><i class="fa fa-check"></i><b>11.2.2</b> Finding Columns With Missing Data</a></li>
<li class="chapter" data-level="11.2.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#use-the-median-approach"><i class="fa fa-check"></i><b>11.2.3</b> Use the Median Approach</a></li>
<li class="chapter" data-level="11.2.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#package-based-approach"><i class="fa fa-check"></i><b>11.2.4</b> Package-based Approach</a></li>
<li class="chapter" data-level="11.2.5" data-path="data-pre-processing.html"><a href="data-pre-processing.html#using-caret-1"><i class="fa fa-check"></i><b>11.2.5</b> Using caret</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#scaling"><i class="fa fa-check"></i><b>11.3</b> Scaling</a><ul>
<li class="chapter" data-level="11.3.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#methods-that-benefit-from-scaling"><i class="fa fa-check"></i><b>11.3.1</b> Methods That Benefit From Scaling</a></li>
<li class="chapter" data-level="11.3.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#methods-that-do-not-require-scaling"><i class="fa fa-check"></i><b>11.3.2</b> Methods That Do Not Require Scaling</a></li>
<li class="chapter" data-level="11.3.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#how-to-scale"><i class="fa fa-check"></i><b>11.3.3</b> How To Scale</a></li>
<li class="chapter" data-level="11.3.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#order-of-processing"><i class="fa fa-check"></i><b>11.3.4</b> Order of Processing</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#low-variance-variables"><i class="fa fa-check"></i><b>11.4</b> Low Variance Variables</a></li>
<li class="chapter" data-level="11.5" data-path="data-pre-processing.html"><a href="data-pre-processing.html#pca---principal-components-analysis"><i class="fa fa-check"></i><b>11.5</b> PCA - Principal Components Analysis</a><ul>
<li class="chapter" data-level="11.5.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#identify-the-factors"><i class="fa fa-check"></i><b>11.5.1</b> Identify The Factors</a></li>
<li class="chapter" data-level="11.5.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#check-for-high-correlations"><i class="fa fa-check"></i><b>11.5.2</b> Check For High Correlations</a></li>
<li class="chapter" data-level="11.5.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#so-why-use-pca"><i class="fa fa-check"></i><b>11.5.3</b> So Why Use PCA ?</a></li>
<li class="chapter" data-level="11.5.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#check-the-biplot"><i class="fa fa-check"></i><b>11.5.4</b> Check The BiPlot</a></li>
<li class="chapter" data-level="11.5.5" data-path="data-pre-processing.html"><a href="data-pre-processing.html#check-the-screeplot"><i class="fa fa-check"></i><b>11.5.5</b> Check The ScreePlot</a></li>
<li class="chapter" data-level="11.5.6" data-path="data-pre-processing.html"><a href="data-pre-processing.html#use-the-transformed-data"><i class="fa fa-check"></i><b>11.5.6</b> Use The Transformed Data</a></li>
<li class="chapter" data-level="11.5.7" data-path="data-pre-processing.html"><a href="data-pre-processing.html#pls"><i class="fa fa-check"></i><b>11.5.7</b> PLS</a></li>
<li class="chapter" data-level="11.5.8" data-path="data-pre-processing.html"><a href="data-pre-processing.html#summary-1"><i class="fa fa-check"></i><b>11.5.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="data-pre-processing.html"><a href="data-pre-processing.html#order-of-pre-processing"><i class="fa fa-check"></i><b>11.6</b> Order of Pre-Processing</a></li>
<li class="chapter" data-level="11.7" data-path="data-pre-processing.html"><a href="data-pre-processing.html#handling-categories"><i class="fa fa-check"></i><b>11.7</b> Handling Categories</a><ul>
<li class="chapter" data-level="11.7.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#examples"><i class="fa fa-check"></i><b>11.7.1</b> Examples</a></li>
<li class="chapter" data-level="11.7.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#admissions-data"><i class="fa fa-check"></i><b>11.7.2</b> Admissions Data</a></li>
<li class="chapter" data-level="11.7.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#is-rank-a-category"><i class="fa fa-check"></i><b>11.7.3</b> Is Rank A Category ?</a></li>
<li class="chapter" data-level="11.7.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#relationship-to-one-hot-encoding"><i class="fa fa-check"></i><b>11.7.4</b> Relationship To One Hot Encoding</a></li>
</ul></li>
<li class="chapter" data-level="11.8" data-path="data-pre-processing.html"><a href="data-pre-processing.html#binning"><i class="fa fa-check"></i><b>11.8</b> Binning</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html"><i class="fa fa-check"></i><b>12</b> Using External ML Frameworks</a><ul>
<li class="chapter" data-level="12.1" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#using-h2o"><i class="fa fa-check"></i><b>12.1</b> Using h2o</a></li>
<li class="chapter" data-level="12.2" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#create-some-h20-models"><i class="fa fa-check"></i><b>12.2</b> Create Some h20 Models</a></li>
<li class="chapter" data-level="12.3" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#saving-a-model"><i class="fa fa-check"></i><b>12.3</b> Saving A Model</a></li>
<li class="chapter" data-level="12.4" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#using-the-h2o-auto-ml-feature"><i class="fa fa-check"></i><b>12.4</b> Using The h2o Auto ML Feature</a></li>
<li class="chapter" data-level="12.5" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#launching-a-job"><i class="fa fa-check"></i><b>12.5</b> Launching a Job</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Predictive Learning in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="comparing-different-methods" class="section level1">
<h1><span class="header-section-number">Chapter 9</span> Comparing Different Methods</h1>
<p>We could look at random forests to see how they perfom. It also provides an opportunity to introduce the concept of hyperparameters which are important to consider since many ML methods will use them. Many people are surprised to learn that random Forests (or even a single Decision Tree) can be used to predict a numeric outcome, but they can be. The advantages of using random Forests include the following:</p>
<pre><code>- easy to use
- resistant to overfitting
- accurate use for non linear models
- can be used to predict numeric or binary outcomes </code></pre>
<p>Some problems include:</p>
<pre><code>- the rf function requires setting hyperparameters 
- adjustment of hyperparameters can be specific to the data set
- default vlaues will requie adjustment or &quot;tuning&quot;</code></pre>
<p>Random Forests are a generalization, and frequently an improvement, over a single decsion tree although a single decision tree is usually more intuitive and easier to inperpret. As an example, let’s use the <strong>rpart</strong> function to predict the <strong>mpg</strong> variable from the mtcars data frame:</p>
<div class="sourceCode" id="cb416"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb416-1" data-line-number="1"><span class="kw">library</span>(rpart)</a>
<a class="sourceLine" id="cb416-2" data-line-number="2"><span class="kw">library</span>(rpart.plot)</a>
<a class="sourceLine" id="cb416-3" data-line-number="3"><span class="co">#</span></a>
<a class="sourceLine" id="cb416-4" data-line-number="4"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb416-5" data-line-number="5">my_small_tree &lt;-<span class="st"> </span><span class="kw">rpart</span>(mpg<span class="op">~</span>.,<span class="dt">data=</span>mtcars)</a>
<a class="sourceLine" id="cb416-6" data-line-number="6">my_small_tree</a></code></pre></div>
<pre><code>## n= 32 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
## 1) root 32 1126.04700 20.09062  
##   2) cyl&gt;=5 21  198.47240 16.64762  
##     4) hp&gt;=192.5 7   28.82857 13.41429 *
##     5) hp&lt; 192.5 14   59.87214 18.26429 *
##   3) cyl&lt; 5 11  203.38550 26.66364 *</code></pre>
<div class="sourceCode" id="cb418"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb418-1" data-line-number="1"><span class="co"># Plot it</span></a>
<a class="sourceLine" id="cb418-2" data-line-number="2"><span class="kw">rpart.plot</span>(my_small_tree)</a></code></pre></div>
<p><img src="biosml_files/figure-html/examrpart-1.png" width="672" /></p>
<p>Pretty basic and easy to use to get predictions and compute an RMSE.</p>
<div class="sourceCode" id="cb419"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb419-1" data-line-number="1">tree_preds &lt;-<span class="st"> </span><span class="kw">predict</span>(my_small_tree, mtcars)</a>
<a class="sourceLine" id="cb419-2" data-line-number="2">Metrics<span class="op">::</span><span class="kw">rmse</span>(<span class="dt">actual=</span> mtcars<span class="op">$</span>mpg, <span class="dt">predicted =</span> tree_preds)</a></code></pre></div>
<pre><code>## [1] 3.021207</code></pre>
<p>Using a single tree, though easy to understand, can be improvded upon by looking at multiple trees on the same data. Better yet, maybe sample some number of the features, especially the more important ones, and then somehow combine the trees to product an aggregate or “bagged” tree to get a better model - at least that is the goal. So we can use a function such as <strong>randomForest</strong> to do this. In this case, the performance of the RMSE is better.</p>
<div class="sourceCode" id="cb421"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb421-1" data-line-number="1"><span class="kw">library</span>(randomForest)</a>
<a class="sourceLine" id="cb421-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb421-3" data-line-number="3">mpg_rf &lt;-<span class="st"> </span><span class="kw">randomForest</span>(mpg<span class="op">~</span>.,<span class="dt">data=</span>mtcars)</a>
<a class="sourceLine" id="cb421-4" data-line-number="4"></a>
<a class="sourceLine" id="cb421-5" data-line-number="5"><span class="co"># </span></a>
<a class="sourceLine" id="cb421-6" data-line-number="6">tree_preds &lt;-<span class="st"> </span><span class="kw">predict</span>(mpg_rf, mtcars)</a>
<a class="sourceLine" id="cb421-7" data-line-number="7">Metrics<span class="op">::</span><span class="kw">rmse</span>(<span class="dt">actual=</span> mtcars<span class="op">$</span>mpg, <span class="dt">predicted =</span> tree_preds)</a></code></pre></div>
<pre><code>## [1] 1.209968</code></pre>
<p>The way bagged trees work is to use bootstrap sampling to look at different versions of the input for each tree that it makes. The number of trees to make is specified before running the function. Since the sampling is bootstrap-based, some of the rows from the input will be duplicated perhaps more than once whereas other rows will not be involved at all in the training. These are commonly known as Out of Bag samples. After a give tree is formed it is then used to predict performance on the OOB samples to arrive at an OOB error.</p>
<div class="sourceCode" id="cb423"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb423-1" data-line-number="1"><span class="co"># This generates row numbers for indexing into the mtcars data frame</span></a>
<a class="sourceLine" id="cb423-2" data-line-number="2"></a>
<a class="sourceLine" id="cb423-3" data-line-number="3">tre_samp &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(mtcars),<span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb423-4" data-line-number="4"></a>
<a class="sourceLine" id="cb423-5" data-line-number="5"><span class="co">#</span></a>
<a class="sourceLine" id="cb423-6" data-line-number="6">in_samp &lt;-<span class="st"> </span>mtcars[tre_samp,]</a>
<a class="sourceLine" id="cb423-7" data-line-number="7"><span class="kw">sum</span>(<span class="kw">duplicated</span>(in_samp))</a></code></pre></div>
<pre><code>## [1] 11</code></pre>
<div class="sourceCode" id="cb425"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb425-1" data-line-number="1"><span class="co"># The indices corresponding to the Out of Band rows would be</span></a>
<a class="sourceLine" id="cb425-2" data-line-number="2">out_of_bag &lt;-<span class="st"> </span>mtcars[<span class="op">-</span>tre_samp,]</a>
<a class="sourceLine" id="cb425-3" data-line-number="3"><span class="kw">head</span>(out_of_bag)</a></code></pre></div>
<pre><code>##                    mpg cyl  disp  hp drat   wt  qsec vs am gear carb
## Hornet Sportabout 18.7   8 360.0 175 3.15 3.44 17.02  0  0    3    2
## Valiant           18.1   6 225.0 105 2.76 3.46 20.22  1  0    3    1
## Duster 360        14.3   8 360.0 245 3.21 3.57 15.84  0  0    3    4
## Merc 230          22.8   4 140.8  95 3.92 3.15 22.90  1  0    4    2
## Merc 280          19.2   6 167.6 123 3.92 3.44 18.30  1  0    4    4
## Merc 450SL        17.3   8 275.8 180 3.07 3.73 17.60  0  0    3    3</code></pre>
<p>For now, let’s set up a “shoot out” between the <strong>lm</strong> function and the <strong>rf</strong> function to see if the latter yields a lower RMSE than the former (or vice versa). You will do a lot of this when building different models. To do this would involve the normal steps of creating a train / test pair upon which to train and then test the model</p>
<div class="sourceCode" id="cb427"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb427-1" data-line-number="1">control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">5</span>)</a>
<a class="sourceLine" id="cb427-2" data-line-number="2"></a>
<a class="sourceLine" id="cb427-3" data-line-number="3">new_idx &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(mtcars<span class="op">$</span>mpg,<span class="dt">p=</span>.<span class="dv">80</span>,<span class="dt">list=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb427-4" data-line-number="4">new_train &lt;-<span class="st"> </span>mtcars[new_idx,]</a>
<a class="sourceLine" id="cb427-5" data-line-number="5">new_test  &lt;-<span class="st"> </span>mtcars[<span class="op">-</span>new_idx,]</a>
<a class="sourceLine" id="cb427-6" data-line-number="6"></a>
<a class="sourceLine" id="cb427-7" data-line-number="7"><span class="kw">set.seed</span>(<span class="dv">124</span>)</a>
<a class="sourceLine" id="cb427-8" data-line-number="8">caret_lm &lt;-<span class="st"> </span><span class="kw">train</span>(mpg <span class="op">~</span><span class="st"> </span>.,</a>
<a class="sourceLine" id="cb427-9" data-line-number="9">                  <span class="dt">data =</span> new_train,</a>
<a class="sourceLine" id="cb427-10" data-line-number="10">                  <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>,</a>
<a class="sourceLine" id="cb427-11" data-line-number="11">                  <span class="dt">trControl =</span> control)</a>
<a class="sourceLine" id="cb427-12" data-line-number="12"></a>
<a class="sourceLine" id="cb427-13" data-line-number="13">caret_lm<span class="op">$</span>results</a></code></pre></div>
<pre><code>##   intercept     RMSE  Rsquared      MAE    RMSESD RsquaredSD     MAESD
## 1      TRUE 4.573874 0.6361404 3.855823 0.9690615  0.1414053 0.5058377</code></pre>
<div class="sourceCode" id="cb429"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb429-1" data-line-number="1">Metrics<span class="op">::</span><span class="kw">rmse</span>(new_test<span class="op">$</span>mpg, <span class="kw">predict</span>(caret_lm,new_test))</a></code></pre></div>
<pre><code>## [1] 2.860341</code></pre>
<p>The results of the <strong>lm</strong> function are pretty straightforward as are the predictions. So let’s see what the <strong>randomForest</strong> function will give us with the same data. Because we are using caret, all we have to is sub in the desired method which is <strong>rf</strong>.</p>
<div class="sourceCode" id="cb431"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb431-1" data-line-number="1">control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">5</span>)</a>
<a class="sourceLine" id="cb431-2" data-line-number="2"></a>
<a class="sourceLine" id="cb431-3" data-line-number="3"><span class="kw">set.seed</span>(<span class="dv">124</span>)</a>
<a class="sourceLine" id="cb431-4" data-line-number="4">caret_rf &lt;-<span class="st"> </span><span class="kw">train</span>(mpg <span class="op">~</span><span class="st"> </span>.,</a>
<a class="sourceLine" id="cb431-5" data-line-number="5">                  <span class="dt">data =</span> new_train,</a>
<a class="sourceLine" id="cb431-6" data-line-number="6">                  <span class="dt">method =</span> <span class="st">&quot;rf&quot;</span>,</a>
<a class="sourceLine" id="cb431-7" data-line-number="7">                  <span class="dt">metric=</span><span class="st">&quot;RMSE&quot;</span>,</a>
<a class="sourceLine" id="cb431-8" data-line-number="8">                  <span class="dt">trControl =</span> control)</a>
<a class="sourceLine" id="cb431-9" data-line-number="9"></a>
<a class="sourceLine" id="cb431-10" data-line-number="10">caret_rf<span class="op">$</span>results</a></code></pre></div>
<pre><code>##   mtry     RMSE  Rsquared      MAE    RMSESD RsquaredSD     MAESD
## 1    2 2.507818 0.8721280 2.259318 0.5737457 0.09704552 0.5607226
## 2    6 2.453846 0.8953322 2.179824 0.6079187 0.05249586 0.6871186
## 3   10 2.465066 0.8936361 2.171276 0.5441641 0.05752109 0.6256309</code></pre>
<div class="sourceCode" id="cb433"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb433-1" data-line-number="1">Metrics<span class="op">::</span><span class="kw">rmse</span>(new_test<span class="op">$</span>mpg, <span class="kw">predict</span>(caret_rf,new_test))</a></code></pre></div>
<pre><code>## [1] 3.037587</code></pre>
<p>So the Random Forest approach, in this case, produces a lower out of sample RMSE estimate for the test data frame. The larger question relates to why there is more information in the output for the rf model ? What is the <strong>mtry</strong> argument and why does it take on three different values during the execution of the function ?</p>
<div class="sourceCode" id="cb435"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb435-1" data-line-number="1">caret_rf</a></code></pre></div>
<pre><code>## Random Forest 
## 
## 28 samples
## 10 predictors
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 23, 22, 23, 21, 23 
## Resampling results across tuning parameters:
## 
##   mtry  RMSE      Rsquared   MAE     
##    2    2.507818  0.8721280  2.259318
##    6    2.453846  0.8953322  2.179824
##   10    2.465066  0.8936361  2.171276
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was mtry = 6.</code></pre>
<p>The <strong>mtry</strong> argument is a <strong>hyperparameter</strong> which represents information that is supplied in the form of an argument prior to the call to the function. These parameters might not be something one can intelligently set without some experimentation. Here is some more specific information.</p>
<div id="parameters-vs-hyperparameters-1" class="section level2">
<h2><span class="header-section-number">9.1</span> Parameters vs Hyperparameters</h2>
<p><strong>Model parameters</strong> are things that are generated as part of the modeling process. They are the product or result of model fitting. These might be things like slope and intercept from a linear model - or coefficients.</p>
<p><strong>Hyperparameters</strong> have default values for various arguments but this does not mean that the defaults are appropriate for all cases.</p>
<p>So with <strong>rf</strong> there is a hyperarameter called <strong>mtry</strong> that influences the outcome but is not necessarily something that we know how to optimally set. The <strong>mtry</strong> value is the number of variables that are randomly sampled at each tree split.</p>
<p>To put this into perspective, if we had called the random Forest function without using caret we would have to supply a value for <strong>mtry</strong> or be prepared to accept whatever the default value is.</p>
<div class="sourceCode" id="cb437"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb437-1" data-line-number="1"><span class="kw">library</span>(randomForest)</a>
<a class="sourceLine" id="cb437-2" data-line-number="2"></a>
<a class="sourceLine" id="cb437-3" data-line-number="3">non_caret_rf &lt;-<span class="st"> </span><span class="kw">randomForest</span>(mpg<span class="op">~</span>.,</a>
<a class="sourceLine" id="cb437-4" data-line-number="4">                             <span class="dt">data =</span> new_train,</a>
<a class="sourceLine" id="cb437-5" data-line-number="5">                             <span class="dt">mtry =</span> <span class="dv">3</span>,      <span class="co"># This is the default </span></a>
<a class="sourceLine" id="cb437-6" data-line-number="6">                             <span class="dt">importance =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb437-7" data-line-number="7"></a>
<a class="sourceLine" id="cb437-8" data-line-number="8"><span class="co"># Check out the RMSE of the preditcions </span></a>
<a class="sourceLine" id="cb437-9" data-line-number="9">Metrics<span class="op">::</span><span class="kw">rmse</span>(new_test<span class="op">$</span>mpg,<span class="kw">predict</span>(non_caret_rf,new_test))</a></code></pre></div>
<pre><code>## [1] 2.851458</code></pre>
<p>This provides information corresponding to one value of <strong>mtry</strong> - in this case 3. To get an idea about how the model performs with other values of <strong>mtry</strong> we could write a function to accept this value as an argument. Our function will return the RMSE corresponding to the supplied value of <strong>mtry</strong>.</p>
<div class="sourceCode" id="cb439"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb439-1" data-line-number="1">make_mtcars_rf &lt;-<span class="st"> </span><span class="cf">function</span>(<span class="dt">mtry=</span><span class="dv">3</span>) {</a>
<a class="sourceLine" id="cb439-2" data-line-number="2">    my_rf &lt;-<span class="st"> </span><span class="kw">randomForest</span>(mpg<span class="op">~</span>.,</a>
<a class="sourceLine" id="cb439-3" data-line-number="3">                          <span class="dt">data =</span> new_train,</a>
<a class="sourceLine" id="cb439-4" data-line-number="4">                          <span class="dt">mtry =</span> mtry,</a>
<a class="sourceLine" id="cb439-5" data-line-number="5">                          <span class="dt">importance =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb439-6" data-line-number="6"></a>
<a class="sourceLine" id="cb439-7" data-line-number="7"><span class="co"># Check out the predictions</span></a>
<a class="sourceLine" id="cb439-8" data-line-number="8">    rmse_rf &lt;-<span class="st"> </span>Metrics<span class="op">::</span><span class="kw">rmse</span>(new_test<span class="op">$</span>mpg,<span class="kw">predict</span>(my_rf,new_test))</a>
<a class="sourceLine" id="cb439-9" data-line-number="9"></a>
<a class="sourceLine" id="cb439-10" data-line-number="10">    <span class="kw">return</span>(rmse_rf)</a>
<a class="sourceLine" id="cb439-11" data-line-number="11">}</a></code></pre></div>
<p>So now, the following example will call the <strong>randomForest</strong> package 5 times. Starting with the first iteration, the value of <strong>mtry</strong> will be 3, the next time it will be 4, and so on until the last iteration where it will be 8. This is just an experiment to see if varying <strong>mtry</strong> will help minimize the RMSE of our model.</p>
<p>We also have to be careful not to pick incorrect values for <strong>mtry</strong> so reading the help page for the <strong>randomForest</strong> package would be helpful. On the other hand, since <strong>mtry</strong> represents the number of features / variables to sample at each potential split, we know that it cannot practically exceed the total number of features in the data frame.</p>
<div class="sourceCode" id="cb440"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb440-1" data-line-number="1">total_rmse &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="dv">3</span><span class="op">:</span><span class="dv">9</span>,make_mtcars_rf) </a>
<a class="sourceLine" id="cb440-2" data-line-number="2"></a>
<a class="sourceLine" id="cb440-3" data-line-number="3"><span class="co"># Get the mean rmse</span></a>
<a class="sourceLine" id="cb440-4" data-line-number="4">total_rmse</a></code></pre></div>
<pre><code>## [1] 2.927925 2.872087 3.068901 3.146571 2.941079 3.130188 3.248836</code></pre>
<div class="sourceCode" id="cb442"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb442-1" data-line-number="1"><span class="kw">mean</span>(total_rmse)</a></code></pre></div>
<pre><code>## [1] 3.047941</code></pre>
<p>We could even create a plot of this information to simplify the selection of <strong>mtry</strong> corresponding to the lowest RMSE.</p>
<div class="sourceCode" id="cb444"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb444-1" data-line-number="1"><span class="kw">plot</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(total_rmse),total_rmse,<span class="dt">type=</span><span class="st">&quot;b&quot;</span>,</a>
<a class="sourceLine" id="cb444-2" data-line-number="2">     <span class="dt">xlab =</span> <span class="st">&quot;Value of mtry&quot;</span>, </a>
<a class="sourceLine" id="cb444-3" data-line-number="3">     <span class="dt">ylab =</span> <span class="st">&quot;RMSE&quot;</span>,</a>
<a class="sourceLine" id="cb444-4" data-line-number="4">     <span class="dt">main =</span> <span class="st">&quot;How RMSE varies with mtry&quot;</span>)</a>
<a class="sourceLine" id="cb444-5" data-line-number="5"><span class="kw">grid</span>()</a></code></pre></div>
<p><img src="biosml_files/figure-html/unnamed-chunk-58-1.png" width="672" /></p>
<p>While this is fine what would happen if there were more than one hyperparameter. If we look closer at the documentation for randomForest then we see that there is a hyperparameter called <strong>ntree</strong> which is set by default to a value of 500. We could build that into our above function to see how that might impact RMSE performance. For example, we’ll work through values of <strong>mtry</strong> from 2,3,..8. For each of those values of mtry we will use values of <strong>ntree</strong> ranging from 300, 400, to 500. Let’s go ahead and do that. So let’s build a “grid” of values.</p>
<div class="sourceCode" id="cb445"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb445-1" data-line-number="1">train_grid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">mtry=</span><span class="kw">seq</span>(<span class="dv">2</span>,<span class="dv">8</span>,<span class="dv">1</span>),</a>
<a class="sourceLine" id="cb445-2" data-line-number="2">                          <span class="dt">numoftrees=</span><span class="kw">c</span>(<span class="dv">300</span>,<span class="dv">400</span>,<span class="dv">500</span>))</a>
<a class="sourceLine" id="cb445-3" data-line-number="3">train_grid</a></code></pre></div>
<pre><code>##    mtry numoftrees
## 1     2        300
## 2     3        300
## 3     4        300
## 4     5        300
## 5     6        300
## 6     7        300
## 7     8        300
## 8     2        400
## 9     3        400
## 10    4        400
## 11    5        400
## 12    6        400
## 13    7        400
## 14    8        400
## 15    2        500
## 16    3        500
## 17    4        500
## 18    5        500
## 19    6        500
## 20    7        500
## 21    8        500</code></pre>
<p>We need to rewrite our function to handle this “training grid”. It’s not terribly difficult, just a bit tedious.</p>
<div class="sourceCode" id="cb447"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb447-1" data-line-number="1">make_mtcars_rf &lt;-<span class="st"> </span><span class="cf">function</span>(<span class="dt">grid=</span>train_grid) {</a>
<a class="sourceLine" id="cb447-2" data-line-number="2">    retlist &lt;-<span class="st"> </span><span class="kw">data.frame</span>()</a>
<a class="sourceLine" id="cb447-3" data-line-number="3">    <span class="cf">for</span> (ii <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(grid)) {</a>
<a class="sourceLine" id="cb447-4" data-line-number="4">      my_rf &lt;-<span class="st"> </span><span class="kw">randomForest</span>(mpg<span class="op">~</span>.,</a>
<a class="sourceLine" id="cb447-5" data-line-number="5">                          <span class="dt">data =</span> new_train,</a>
<a class="sourceLine" id="cb447-6" data-line-number="6">                          <span class="dt">mtry =</span> grid[ii,]<span class="op">$</span>mtry,</a>
<a class="sourceLine" id="cb447-7" data-line-number="7">                          <span class="dt">ntree =</span> grid[ii,]<span class="op">$</span>numoftrees,</a>
<a class="sourceLine" id="cb447-8" data-line-number="8">                          <span class="dt">importance =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb447-9" data-line-number="9">      rmse_rf &lt;-<span class="st"> </span>Metrics<span class="op">::</span><span class="kw">rmse</span>(new_test<span class="op">$</span>mpg,<span class="kw">predict</span>(my_rf,new_test))</a>
<a class="sourceLine" id="cb447-10" data-line-number="10">      retlist &lt;-<span class="st"> </span><span class="kw">rbind</span>(retlist,<span class="kw">c</span>(grid[ii,]<span class="op">$</span>mtry,grid[ii,]<span class="op">$</span>numoftrees,</a>
<a class="sourceLine" id="cb447-11" data-line-number="11">                          rmse_rf))</a>
<a class="sourceLine" id="cb447-12" data-line-number="12">      <span class="kw">names</span>(retlist) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;mtry&quot;</span>,<span class="st">&quot;ntree&quot;</span>,<span class="st">&quot;rmse&quot;</span>)</a>
<a class="sourceLine" id="cb447-13" data-line-number="13">     }</a>
<a class="sourceLine" id="cb447-14" data-line-number="14">    </a>
<a class="sourceLine" id="cb447-15" data-line-number="15"></a>
<a class="sourceLine" id="cb447-16" data-line-number="16"><span class="co"># Check out the predictions</span></a>
<a class="sourceLine" id="cb447-17" data-line-number="17">    </a>
<a class="sourceLine" id="cb447-18" data-line-number="18">    <span class="kw">return</span>(retlist)</a>
<a class="sourceLine" id="cb447-19" data-line-number="19">}</a></code></pre></div>
<p>Now we’ll call this function and plot the output. It appears that values of mtry correspinding to the lowest RMSE range from 4,5 to 6 depending on the number of trees. The good news is that we now have a function that we could use with other training grids.</p>
<div class="sourceCode" id="cb448"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb448-1" data-line-number="1"><span class="kw">make_mtcars_rf</span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb448-2" data-line-number="2"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>mtry,<span class="dt">y=</span>rmse)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb448-3" data-line-number="3"><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st"> </span><span class="kw">facet_wrap</span>(<span class="op">~</span>ntree) <span class="op">+</span></a>
<a class="sourceLine" id="cb448-4" data-line-number="4"><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Hyperparameter Experiment&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">theme_bw</span>()</a></code></pre></div>
<p><img src="biosml_files/figure-html/trnframerf-1.png" width="672" /></p>
<p>The above is an example of what we would have to do if we didn’t have something like caret to help us try out different values of <strong>mtry</strong> or other parameters. As we saw from our earlier work, the model <strong>caret_rf</strong> tried three different values of the <strong>mtry</strong> hyperparameter without us even asking for it. It event tells us that the best value for <strong>mtry</strong> was 6 which corresponds to an RMSE of 2.37.</p>
<div class="sourceCode" id="cb449"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb449-1" data-line-number="1">caret_rf</a></code></pre></div>
<pre><code>## Random Forest 
## 
## 28 samples
## 10 predictors
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 23, 22, 23, 21, 23 
## Resampling results across tuning parameters:
## 
##   mtry  RMSE      Rsquared   MAE     
##    2    2.507818  0.8721280  2.259318
##    6    2.453846  0.8953322  2.179824
##   10    2.465066  0.8936361  2.171276
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was mtry = 6.</code></pre>
<p>As a matter of fact we can plot this model with not a lot of effort and it will show us something interesting. In particular, we now see that using values less than 6 predictors / columns / features results in lower RMSE. If we use more, than the RMSE goes up. This is the power of methods that use hyperparamters. If we move through a number of values for the <strong>mtry</strong> then perhaps we can find the best value to get the best result.</p>
<div class="sourceCode" id="cb451"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb451-1" data-line-number="1"><span class="kw">plot</span>(caret_rf)</a></code></pre></div>
<p><img src="biosml_files/figure-html/unnamed-chunk-59-1.png" width="672" /></p>
<p>Telling <strong>caret</strong> to use more values of <strong>mtry</strong> is possible. This will explicitly try all values from 1 - 9 inclusive. Note that the model gets built for each value of <strong>mtry</strong>. This causes the model building process to take longer than it normally would.</p>
<div class="sourceCode" id="cb452"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb452-1" data-line-number="1">caret_rf &lt;-<span class="st"> </span><span class="kw">train</span>(mpg <span class="op">~</span><span class="st"> </span>.,</a>
<a class="sourceLine" id="cb452-2" data-line-number="2">                  <span class="dt">data =</span> new_train,</a>
<a class="sourceLine" id="cb452-3" data-line-number="3">                  <span class="dt">method =</span> <span class="st">&quot;rf&quot;</span>,</a>
<a class="sourceLine" id="cb452-4" data-line-number="4">                  <span class="dt">tuneLength =</span> <span class="dv">9</span>,</a>
<a class="sourceLine" id="cb452-5" data-line-number="5">                  <span class="dt">metric=</span><span class="st">&quot;RMSE&quot;</span>)</a>
<a class="sourceLine" id="cb452-6" data-line-number="6"></a>
<a class="sourceLine" id="cb452-7" data-line-number="7">caret_rf<span class="op">$</span>results</a></code></pre></div>
<pre><code>##   mtry     RMSE  Rsquared      MAE    RMSESD RsquaredSD     MAESD
## 1    2 2.571395 0.8703747 2.204286 0.7042295 0.07002818 0.5730363
## 2    3 2.531625 0.8753585 2.142666 0.6568683 0.06653321 0.5317836
## 3    4 2.499533 0.8785716 2.101291 0.6713803 0.06386455 0.5212127
## 4    5 2.467620 0.8832917 2.073318 0.6676407 0.05764912 0.5327038
## 5    6 2.512371 0.8776798 2.104877 0.6479920 0.06005013 0.5174176
## 6    7 2.514193 0.8790404 2.103327 0.6418979 0.05593957 0.5005799
## 7    8 2.529156 0.8774904 2.124679 0.6454195 0.05929541 0.5011914
## 8    9 2.555506 0.8738942 2.133072 0.6461009 0.06351165 0.5015938
## 9   10 2.557101 0.8739764 2.139092 0.6361090 0.06062165 0.4926305</code></pre>
<div class="sourceCode" id="cb454"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb454-1" data-line-number="1">Metrics<span class="op">::</span><span class="kw">rmse</span>(new_test<span class="op">$</span>mpg, <span class="kw">predict</span>(caret_rf,new_test))</a></code></pre></div>
<pre><code>## [1] 2.853987</code></pre>
<p>It looks like a value of 4 will produce the lowest value for RMSE.</p>
<div class="sourceCode" id="cb456"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb456-1" data-line-number="1"><span class="kw">plot</span>(caret_rf)</a></code></pre></div>
<p><img src="biosml_files/figure-html/plotcaretrf-1.png" width="672" /></p>
</div>
<div id="hyperparameter-tuning" class="section level2">
<h2><span class="header-section-number">9.2</span> Hyperparameter Tuning</h2>
<p>The process of finding the “right” values for these parameters is generally referred to as “hyperparameter tuning”. Different values are supplied for each invocation of a method (as we did in the above example) to see the effect on the model. We might do this many times to arrive at the optimal parameter set to produce a model that offers the “best” explanatory and predictive power.</p>
<p>Just to review - things like coefficients and residuals are parameters that are generated by a call to the <strong>lm</strong> function. They don’t actually exist until the function does some work. The <strong>hyperparameters</strong> are specific to whatever algorithm (and supporting R function) you are using. Concepts such as coefficients and intercept, however, are parameters that would be generated in this case by lm.</p>
<p>More generally, what if we wanted to use other functions to do some predicting ? You can check <a href="https://topepo.github.io/caret/available-models.html">this page</a> for a list of caret supported methods along with any hyperparamters available for tuning. Obviously, if you know the underlying method you can refer directly to the help page for it.</p>
<div id="multiple-hyperparameters" class="section level3">
<h3><span class="header-section-number">9.2.1</span> Multiple Hyperparameters ?</h3>
<p>Let’s look at another method for random forests such as the <strong>ranger</strong> function which builds trees very rapidly. We can easily call it via the <strong>train</strong> function. If you consult the reference manual for the caret implementation of <strong>ranger</strong> you will see that it supports three hyperparameters: <strong>mtry, splitrule</strong>, and <strong>min.node.size</strong>.</p>
<div class="sourceCode" id="cb457"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb457-1" data-line-number="1">control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;cv&quot;</span>,<span class="dt">number=</span><span class="dv">5</span>)</a>
<a class="sourceLine" id="cb457-2" data-line-number="2">caret_ranger &lt;-<span class="st"> </span><span class="kw">train</span>(mpg <span class="op">~</span><span class="st"> </span>.,</a>
<a class="sourceLine" id="cb457-3" data-line-number="3">                      <span class="dt">data =</span> new_train,</a>
<a class="sourceLine" id="cb457-4" data-line-number="4">                      <span class="dt">method =</span> <span class="st">&quot;ranger&quot;</span>,</a>
<a class="sourceLine" id="cb457-5" data-line-number="5">                      <span class="dt">trControl=</span>control)</a>
<a class="sourceLine" id="cb457-6" data-line-number="6"></a>
<a class="sourceLine" id="cb457-7" data-line-number="7">caret_ranger<span class="op">$</span>results</a></code></pre></div>
<pre><code>##   mtry min.node.size  splitrule     RMSE  Rsquared      MAE    RMSESD RsquaredSD
## 1    2             5   variance 2.476795 0.8666066 2.232269 1.0330289 0.08817255
## 2    2             5 extratrees 2.738357 0.8310435 2.385122 0.9860349 0.12116529
## 3    6             5   variance 2.375792 0.8820243 2.039655 1.0913421 0.06370665
## 4    6             5 extratrees 2.522912 0.8607126 2.186808 1.0611383 0.09795920
## 5   10             5   variance 2.428831 0.8747796 2.108597 1.0504830 0.05793241
## 6   10             5 extratrees 2.515091 0.8590272 2.182746 0.9780506 0.10072867
##       MAESD
## 1 0.8299998
## 2 0.7817984
## 3 0.9349471
## 4 0.8475972
## 5 0.8758160
## 6 0.8138169</code></pre>
<div class="sourceCode" id="cb459"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb459-1" data-line-number="1">Metrics<span class="op">::</span><span class="kw">rmse</span>(new_test<span class="op">$</span>mpg, <span class="kw">predict</span>(caret_ranger,new_test))</a></code></pre></div>
<pre><code>## [1] 2.955875</code></pre>
<p>And then we can plot these results to get a more intuitive view of the output. What we see is that it sweeps through three values of mtry. for each of those it tries out a split rule of “variance” or “extratrees” and min.node.size of 5.</p>
<div class="sourceCode" id="cb461"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb461-1" data-line-number="1"><span class="kw">plot</span>(caret_ranger)</a></code></pre></div>
<p><img src="biosml_files/figure-html/plotrangerinvoke-1.png" width="672" /></p>
<div class="sourceCode" id="cb462"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb462-1" data-line-number="1">caret_ranger &lt;-<span class="st"> </span><span class="kw">train</span>(mpg <span class="op">~</span><span class="st"> </span>.,</a>
<a class="sourceLine" id="cb462-2" data-line-number="2">                      <span class="dt">data =</span> new_train,</a>
<a class="sourceLine" id="cb462-3" data-line-number="3">                      <span class="dt">method =</span> <span class="st">&quot;ranger&quot;</span>,</a>
<a class="sourceLine" id="cb462-4" data-line-number="4">                      <span class="dt">tuneLength =</span> <span class="dv">5</span>)</a>
<a class="sourceLine" id="cb462-5" data-line-number="5"></a>
<a class="sourceLine" id="cb462-6" data-line-number="6">caret_ranger<span class="op">$</span>results</a></code></pre></div>
<pre><code>##    mtry min.node.size  splitrule     RMSE  Rsquared      MAE    RMSESD RsquaredSD
## 1     2             5   variance 2.690291 0.8700621 2.296038 0.8947813 0.06032597
## 2     2             5 extratrees 2.840479 0.8572420 2.427985 0.9382551 0.06464241
## 3     4             5   variance 2.611438 0.8712824 2.179740 0.8526205 0.06405016
## 4     4             5 extratrees 2.754806 0.8633208 2.337019 0.9067548 0.06216364
## 5     6             5   variance 2.607622 0.8698538 2.129738 0.8658528 0.06591231
## 6     6             5 extratrees 2.716449 0.8640799 2.288598 0.8989636 0.05934964
## 7     8             5   variance 2.627119 0.8647110 2.159415 0.8492106 0.06799667
## 8     8             5 extratrees 2.703783 0.8640564 2.268496 0.8973516 0.05913226
## 9    10             5   variance 2.633906 0.8607785 2.149732 0.8447265 0.07251033
## 10   10             5 extratrees 2.683358 0.8658499 2.238418 0.8936584 0.05963211
##        MAESD
## 1  0.6791852
## 2  0.7045386
## 3  0.6637256
## 4  0.6862209
## 5  0.6840275
## 6  0.6901260
## 7  0.6635008
## 8  0.7001131
## 9  0.6610340
## 10 0.6984022</code></pre>
<div class="sourceCode" id="cb464"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb464-1" data-line-number="1">Metrics<span class="op">::</span><span class="kw">rmse</span>(new_test<span class="op">$</span>mpg, <span class="kw">predict</span>(caret_ranger,new_test))</a></code></pre></div>
<pre><code>## [1] 3.061364</code></pre>
<div class="sourceCode" id="cb466"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb466-1" data-line-number="1"><span class="kw">plot</span>(caret_ranger)</a></code></pre></div>
<p><img src="biosml_files/figure-html/plotnewranger-1.png" width="672" /></p>
</div>
<div id="custom-tuning-grid" class="section level3">
<h3><span class="header-section-number">9.2.2</span> Custom Tuning Grid</h3>
<p>So it’s possible to go even deeper when tuning hyperparameters. We can create a custom tuning grid instead of letting caret pick the values. Of course, using <strong>tuneLength</strong> is fine but the custom tuning grid allows for finer grained control. It’s easy, all we need to do is to specify our own “tuneGrid” and then pass it to <strong>train</strong>.</p>
<div class="sourceCode" id="cb467"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb467-1" data-line-number="1">MyGrid &lt;-<span class="st"> </span><span class="kw">data.frame</span>(</a>
<a class="sourceLine" id="cb467-2" data-line-number="2">   <span class="dt">mtry =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">6</span>),</a>
<a class="sourceLine" id="cb467-3" data-line-number="3">   <span class="dt">splitrule =</span> <span class="st">&quot;variance&quot;</span>,</a>
<a class="sourceLine" id="cb467-4" data-line-number="4">   <span class="dt">min.node.size =</span> <span class="dv">4</span></a>
<a class="sourceLine" id="cb467-5" data-line-number="5">)</a>
<a class="sourceLine" id="cb467-6" data-line-number="6"></a>
<a class="sourceLine" id="cb467-7" data-line-number="7"><span class="co"># Now we supply it when calling train</span></a>
<a class="sourceLine" id="cb467-8" data-line-number="8"></a>
<a class="sourceLine" id="cb467-9" data-line-number="9">caret_ranger &lt;-<span class="st"> </span><span class="kw">train</span>(mpg <span class="op">~</span><span class="st"> </span>.,</a>
<a class="sourceLine" id="cb467-10" data-line-number="10">                  <span class="dt">data =</span> new_train,</a>
<a class="sourceLine" id="cb467-11" data-line-number="11">                  <span class="dt">method =</span> <span class="st">&quot;ranger&quot;</span>,</a>
<a class="sourceLine" id="cb467-12" data-line-number="12">                  <span class="dt">tuneGrid =</span> MyGrid)</a>
<a class="sourceLine" id="cb467-13" data-line-number="13"></a>
<a class="sourceLine" id="cb467-14" data-line-number="14">caret_ranger<span class="op">$</span>results</a></code></pre></div>
<pre><code>##   mtry splitrule min.node.size     RMSE  Rsquared      MAE    RMSESD RsquaredSD
## 1    2  variance             4 2.387988 0.8554081 2.024971 0.6323651 0.06570000
## 2    4  variance             4 2.362513 0.8566676 1.951680 0.5688922 0.06498350
## 3    6  variance             4 2.377784 0.8548658 1.962013 0.5643678 0.06844904
##       MAESD
## 1 0.5078494
## 2 0.4632975
## 3 0.4634406</code></pre>
<div class="sourceCode" id="cb469"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb469-1" data-line-number="1">Metrics<span class="op">::</span><span class="kw">rmse</span>(new_test<span class="op">$</span>mpg, <span class="kw">predict</span>(caret_ranger,new_test))</a></code></pre></div>
<pre><code>## [1] 2.775617</code></pre>
</div>
</div>
<div id="applied-to-classification" class="section level2">
<h2><span class="header-section-number">9.3</span> Applied To Classification</h2>
<p>We could extend this it deal with classification problems. We can adapt our randomForest function for use in predicting the Pima Indians diabetes outcome.</p>
<div class="sourceCode" id="cb471"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb471-1" data-line-number="1"><span class="kw">library</span>(randomForest)</a>
<a class="sourceLine" id="cb471-2" data-line-number="2"><span class="kw">library</span>(tidyverse)</a></code></pre></div>
<div class="sourceCode" id="cb472"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb472-1" data-line-number="1">url &lt;-<span class="st"> &quot;https://raw.githubusercontent.com/steviep42/bios534_spring_2020/master/data/pima.csv&quot;</span></a>
<a class="sourceLine" id="cb472-2" data-line-number="2">pm &lt;-<span class="st"> </span><span class="kw">read.csv</span>(url)</a></code></pre></div>
<p>We’ll implement the train / test pair</p>
<div class="sourceCode" id="cb473"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb473-1" data-line-number="1">idx &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(pm<span class="op">$</span>diabetes,<span class="dt">p=</span>.<span class="dv">8</span>,<span class="dt">list=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb473-2" data-line-number="2"></a>
<a class="sourceLine" id="cb473-3" data-line-number="3">rf_train &lt;-<span class="st"> </span>pm[ idx,]</a>
<a class="sourceLine" id="cb473-4" data-line-number="4">rf_test  &lt;-<span class="st"> </span>pm[<span class="op">-</span>idx,]</a></code></pre></div>
<p>Next, we’ll build the model using the randomForest method</p>
<div class="sourceCode" id="cb474"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb474-1" data-line-number="1">rf_model &lt;-<span class="st"> </span><span class="kw">randomForest</span>(diabetes <span class="op">~</span><span class="st"> </span>.,</a>
<a class="sourceLine" id="cb474-2" data-line-number="2">                         <span class="dt">data =</span> rf_train)</a></code></pre></div>
<p>Then we’ll do the usual things of making some predictions and then look at an Accuracy and an AUC.</p>
<div class="sourceCode" id="cb475"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb475-1" data-line-number="1"><span class="co"># Get the raw probabilities</span></a>
<a class="sourceLine" id="cb475-2" data-line-number="2">rf_probabilities &lt;-<span class="st"> </span><span class="kw">predict</span>(rf_model,rf_test,<span class="dt">type=</span><span class="st">&quot;prob&quot;</span>)</a></code></pre></div>
<p>Compute the AUC:</p>
<div class="sourceCode" id="cb476"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb476-1" data-line-number="1">caTools<span class="op">::</span><span class="kw">colAUC</span>(rf_probabilities[,<span class="dv">1</span>],rf_test<span class="op">$</span>diabetes)</a></code></pre></div>
<pre><code>##                  [,1]
## neg vs. pos 0.8383019</code></pre>
<p>Well, that was nice but how would we work in the search grid approach to see how this might be impacted by varying the hyper parameters ? We could start by picking parameters at random - as long as they are valid. mtry needs to be from 2 up to some number of features in the data set (this varies with the method) The number of trees should be somewhere between 100 and 1600.</p>
<div class="sourceCode" id="cb478"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb478-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">12</span>)</a>
<a class="sourceLine" id="cb478-2" data-line-number="2">mtry =<span class="st"> </span><span class="kw">sample</span>(<span class="dv">3</span><span class="op">:</span><span class="dv">8</span>,<span class="dv">5</span>)</a>
<a class="sourceLine" id="cb478-3" data-line-number="3">ntrees =<span class="st"> </span><span class="kw">sample</span>(<span class="dv">200</span><span class="op">:</span><span class="dv">1200</span>,<span class="dv">5</span>)</a>
<a class="sourceLine" id="cb478-4" data-line-number="4"></a>
<a class="sourceLine" id="cb478-5" data-line-number="5">my_grid &lt;-<span class="st"> </span><span class="kw">data.frame</span>()</a>
<a class="sourceLine" id="cb478-6" data-line-number="6"><span class="cf">for</span> (ii <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(mtry)) {</a>
<a class="sourceLine" id="cb478-7" data-line-number="7">  <span class="cf">for</span> (jj <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(ntrees)) {</a>
<a class="sourceLine" id="cb478-8" data-line-number="8">    my_grid &lt;-<span class="st"> </span><span class="kw">rbind</span>(my_grid,<span class="kw">c</span>(mtry[ii],ntrees[jj]))</a>
<a class="sourceLine" id="cb478-9" data-line-number="9">  }</a>
<a class="sourceLine" id="cb478-10" data-line-number="10">}</a>
<a class="sourceLine" id="cb478-11" data-line-number="11"><span class="kw">names</span>(my_grid) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;mtry&quot;</span>,<span class="st">&quot;numoftrees&quot;</span>)</a>
<a class="sourceLine" id="cb478-12" data-line-number="12">my_grid</a></code></pre></div>
<pre><code>##    mtry numoftrees
## 1     4        373
## 2     4        652
## 3     4        268
## 4     4        675
## 5     4        745
## 6     8        373
## 7     8        652
## 8     8        268
## 9     8        675
## 10    8        745
## 11    6        373
## 12    6        652
## 13    6        268
## 14    6        675
## 15    6        745
## 16    5        373
## 17    5        652
## 18    5        268
## 19    5        675
## 20    5        745
## 21    3        373
## 22    3        652
## 23    3        268
## 24    3        675
## 25    3        745</code></pre>
<p>So we could write a function to accept the combined set of randmoized parameters to see how it impacts performance:</p>
<div class="sourceCode" id="cb480"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb480-1" data-line-number="1">make_diabetes_rf &lt;-<span class="st"> </span><span class="cf">function</span>(<span class="dt">grid=</span>train_grid) {</a>
<a class="sourceLine" id="cb480-2" data-line-number="2">    retlist &lt;-<span class="st"> </span><span class="kw">data.frame</span>()</a>
<a class="sourceLine" id="cb480-3" data-line-number="3">    <span class="cf">for</span> (ii <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(grid)) {</a>
<a class="sourceLine" id="cb480-4" data-line-number="4">      my_rf &lt;-<span class="st"> </span><span class="kw">randomForest</span>(diabetes <span class="op">~</span><span class="st"> </span>. ,</a>
<a class="sourceLine" id="cb480-5" data-line-number="5">                           <span class="dt">data =</span> rf_train,</a>
<a class="sourceLine" id="cb480-6" data-line-number="6">                           <span class="dt">mtry =</span> grid[ii,]<span class="op">$</span>mtry,</a>
<a class="sourceLine" id="cb480-7" data-line-number="7">                           <span class="dt">ntree =</span> grid[ii,]<span class="op">$</span>numoftrees,</a>
<a class="sourceLine" id="cb480-8" data-line-number="8">                           <span class="dt">importance =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb480-9" data-line-number="9">      </a>
<a class="sourceLine" id="cb480-10" data-line-number="10">      <span class="co">#  Get the AUC</span></a>
<a class="sourceLine" id="cb480-11" data-line-number="11">      rf_probabilities &lt;-<span class="st"> </span><span class="kw">predict</span>(my_rf,rf_test,<span class="dt">type=</span><span class="st">&quot;prob&quot;</span>)</a>
<a class="sourceLine" id="cb480-12" data-line-number="12">      auc &lt;-<span class="st"> </span>caTools<span class="op">::</span><span class="kw">colAUC</span>(rf_probabilities[,<span class="dv">1</span>],rf_test<span class="op">$</span>diabetes)</a>
<a class="sourceLine" id="cb480-13" data-line-number="13">    </a>
<a class="sourceLine" id="cb480-14" data-line-number="14">      <span class="co"># Get the Accuracy</span></a>
<a class="sourceLine" id="cb480-15" data-line-number="15">      rf_labels &lt;-<span class="st"> </span><span class="kw">predict</span>(my_rf,rf_test)</a>
<a class="sourceLine" id="cb480-16" data-line-number="16">      myt &lt;-<span class="st"> </span><span class="kw">table</span>(rf_labels,rf_test<span class="op">$</span>diabetes)</a>
<a class="sourceLine" id="cb480-17" data-line-number="17">      acc &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="kw">diag</span>(myt))<span class="op">/</span><span class="kw">sum</span>(myt)</a>
<a class="sourceLine" id="cb480-18" data-line-number="18">      </a>
<a class="sourceLine" id="cb480-19" data-line-number="19">      <span class="co"># Return a data frame with the information</span></a>
<a class="sourceLine" id="cb480-20" data-line-number="20">      retlist &lt;-<span class="st"> </span><span class="kw">rbind</span>(retlist,</a>
<a class="sourceLine" id="cb480-21" data-line-number="21">                       <span class="kw">c</span>(grid[ii,]<span class="op">$</span>mtry,grid[ii,]<span class="op">$</span>numoftrees,auc,acc))</a>
<a class="sourceLine" id="cb480-22" data-line-number="22">      <span class="kw">names</span>(retlist) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;mtry&quot;</span>,<span class="st">&quot;ntree&quot;</span>,<span class="st">&quot;auc&quot;</span>,<span class="st">&quot;acc&quot;</span>)</a>
<a class="sourceLine" id="cb480-23" data-line-number="23">    }</a>
<a class="sourceLine" id="cb480-24" data-line-number="24">    </a>
<a class="sourceLine" id="cb480-25" data-line-number="25"><span class="co"># </span></a>
<a class="sourceLine" id="cb480-26" data-line-number="26">    <span class="kw">return</span>(retlist)</a>
<a class="sourceLine" id="cb480-27" data-line-number="27">}</a></code></pre></div>
<p>Let’s see what we get back:</p>
<div class="sourceCode" id="cb481"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb481-1" data-line-number="1"><span class="kw">make_diabetes_rf</span>(my_grid)</a></code></pre></div>
<pre><code>##    mtry ntree       auc       acc
## 1     4   373 0.8415094 0.7712418
## 2     4   652 0.8428302 0.7712418
## 3     4   268 0.8441509 0.7712418
## 4     4   675 0.8436792 0.7712418
## 5     4   745 0.8415094 0.7647059
## 6     8   373 0.8488679 0.7777778
## 7     8   652 0.8427358 0.7647059
## 8     8   268 0.8450000 0.7843137
## 9     8   675 0.8410377 0.7647059
## 10    8   745 0.8541509 0.7647059
## 11    6   373 0.8433019 0.7712418
## 12    6   652 0.8439623 0.7712418
## 13    6   268 0.8427358 0.7843137
## 14    6   675 0.8437736 0.7581699
## 15    6   745 0.8452830 0.7581699
## 16    5   373 0.8391509 0.7647059
## 17    5   652 0.8454717 0.7712418
## 18    5   268 0.8401887 0.7712418
## 19    5   675 0.8399057 0.7581699
## 20    5   745 0.8403774 0.7777778
## 21    3   373 0.8370755 0.7843137
## 22    3   652 0.8383962 0.7843137
## 23    3   268 0.8400000 0.7647059
## 24    3   675 0.8412264 0.7777778
## 25    3   745 0.8410377 0.7843137</code></pre>
<p>So there isn’t a lot of variance in the outcomes although we didn’t know that previously which is why we performed the random search in the first place. If we wanted to, we could do something less random and more systematic by creating a predictable seqeucne of values for the hyperparameters which would result in a more granular search. In this example, we’ll also take advantage of a function called <strong>expand.grid</strong> which will help make the training grid. We could still use the above approach which uses for loops if we wanted to.</p>
<div class="sourceCode" id="cb483"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb483-1" data-line-number="1">train_grid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">mtry=</span><span class="kw">seq</span>(<span class="dv">2</span>,<span class="dv">8</span>,<span class="dv">1</span>),</a>
<a class="sourceLine" id="cb483-2" data-line-number="2">                          <span class="dt">numoftrees=</span><span class="kw">seq</span>(<span class="dv">200</span>,<span class="dv">1400</span>,<span class="dv">200</span>))</a>
<a class="sourceLine" id="cb483-3" data-line-number="3">train_grid</a></code></pre></div>
<pre><code>##    mtry numoftrees
## 1     2        200
## 2     3        200
## 3     4        200
## 4     5        200
## 5     6        200
## 6     7        200
## 7     8        200
## 8     2        400
## 9     3        400
## 10    4        400
## 11    5        400
## 12    6        400
## 13    7        400
## 14    8        400
## 15    2        600
## 16    3        600
## 17    4        600
## 18    5        600
## 19    6        600
## 20    7        600
## 21    8        600
## 22    2        800
## 23    3        800
## 24    4        800
## 25    5        800
## 26    6        800
## 27    7        800
## 28    8        800
## 29    2       1000
## 30    3       1000
## 31    4       1000
## 32    5       1000
## 33    6       1000
## 34    7       1000
## 35    8       1000
## 36    2       1200
## 37    3       1200
## 38    4       1200
## 39    5       1200
## 40    6       1200
## 41    7       1200
## 42    8       1200
## 43    2       1400
## 44    3       1400
## 45    4       1400
## 46    5       1400
## 47    6       1400
## 48    7       1400
## 49    8       1400</code></pre>
<div class="sourceCode" id="cb485"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb485-1" data-line-number="1">make_diabetes_rf &lt;-<span class="st"> </span><span class="cf">function</span>(<span class="dt">grid=</span>train_grid) {</a>
<a class="sourceLine" id="cb485-2" data-line-number="2">    retlist &lt;-<span class="st"> </span><span class="kw">data.frame</span>()</a>
<a class="sourceLine" id="cb485-3" data-line-number="3">    <span class="cf">for</span> (ii <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(grid)) {</a>
<a class="sourceLine" id="cb485-4" data-line-number="4">      my_rf &lt;-<span class="st"> </span><span class="kw">randomForest</span>(diabetes <span class="op">~</span><span class="st"> </span>. ,</a>
<a class="sourceLine" id="cb485-5" data-line-number="5">                           <span class="dt">data =</span> rf_train,</a>
<a class="sourceLine" id="cb485-6" data-line-number="6">                           <span class="dt">mtry =</span> grid[ii,]<span class="op">$</span>mtry,</a>
<a class="sourceLine" id="cb485-7" data-line-number="7">                           <span class="dt">ntree =</span> grid[ii,]<span class="op">$</span>numoftrees,</a>
<a class="sourceLine" id="cb485-8" data-line-number="8">                           <span class="dt">importance =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb485-9" data-line-number="9">      </a>
<a class="sourceLine" id="cb485-10" data-line-number="10">      <span class="co">#  Get the AUC</span></a>
<a class="sourceLine" id="cb485-11" data-line-number="11">      rf_probabilities &lt;-<span class="st"> </span><span class="kw">predict</span>(my_rf,rf_test,<span class="dt">type=</span><span class="st">&quot;prob&quot;</span>)</a>
<a class="sourceLine" id="cb485-12" data-line-number="12">      auc &lt;-<span class="st"> </span>caTools<span class="op">::</span><span class="kw">colAUC</span>(rf_probabilities[,<span class="dv">1</span>],rf_test<span class="op">$</span>diabetes)</a>
<a class="sourceLine" id="cb485-13" data-line-number="13">    </a>
<a class="sourceLine" id="cb485-14" data-line-number="14">      <span class="co"># Get the Accuracy</span></a>
<a class="sourceLine" id="cb485-15" data-line-number="15">      rf_labels &lt;-<span class="st"> </span><span class="kw">predict</span>(my_rf,rf_test)</a>
<a class="sourceLine" id="cb485-16" data-line-number="16">      myt &lt;-<span class="st"> </span><span class="kw">table</span>(rf_labels,rf_test<span class="op">$</span>diabetes)</a>
<a class="sourceLine" id="cb485-17" data-line-number="17">      acc &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="kw">diag</span>(myt))<span class="op">/</span><span class="kw">sum</span>(myt)</a>
<a class="sourceLine" id="cb485-18" data-line-number="18">      </a>
<a class="sourceLine" id="cb485-19" data-line-number="19">      <span class="co"># Return a data frame with the information</span></a>
<a class="sourceLine" id="cb485-20" data-line-number="20">      retlist &lt;-<span class="st"> </span><span class="kw">rbind</span>(retlist,</a>
<a class="sourceLine" id="cb485-21" data-line-number="21">                       <span class="kw">c</span>(grid[ii,]<span class="op">$</span>mtry,grid[ii,]<span class="op">$</span>numoftrees,auc,acc))</a>
<a class="sourceLine" id="cb485-22" data-line-number="22">      <span class="kw">names</span>(retlist) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;mtry&quot;</span>,<span class="st">&quot;ntree&quot;</span>,<span class="st">&quot;auc&quot;</span>,<span class="st">&quot;acc&quot;</span>)</a>
<a class="sourceLine" id="cb485-23" data-line-number="23">    }</a>
<a class="sourceLine" id="cb485-24" data-line-number="24">    </a>
<a class="sourceLine" id="cb485-25" data-line-number="25"><span class="co"># </span></a>
<a class="sourceLine" id="cb485-26" data-line-number="26">    <span class="kw">return</span>(retlist)</a>
<a class="sourceLine" id="cb485-27" data-line-number="27">}</a></code></pre></div>
<p>So we could look at how the AUC varies for the values of mtry and ntrees. What we will experience here is longer run times because the grid has more values than the one we made when using a random approach.</p>
<div class="sourceCode" id="cb486"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb486-1" data-line-number="1"><span class="kw">nrow</span>(my_grid)</a></code></pre></div>
<pre><code>## [1] 25</code></pre>
<div class="sourceCode" id="cb488"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb488-1" data-line-number="1"><span class="kw">nrow</span>(train_grid)</a></code></pre></div>
<pre><code>## [1] 49</code></pre>
<div class="sourceCode" id="cb490"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb490-1" data-line-number="1"><span class="co">#</span></a>
<a class="sourceLine" id="cb490-2" data-line-number="2">stats &lt;-<span class="st"> </span><span class="kw">make_diabetes_rf</span>()</a></code></pre></div>
<p>This plot will show us what is happening.</p>
<div class="sourceCode" id="cb491"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb491-1" data-line-number="1">stats &lt;-<span class="st"> </span>stats <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">ntree=</span><span class="kw">factor</span>(ntree))</a>
<a class="sourceLine" id="cb491-2" data-line-number="2"><span class="kw">ggplot</span>(stats,<span class="kw">aes</span>(<span class="dt">x=</span>mtry,<span class="dt">y=</span>auc)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb491-3" data-line-number="3"><span class="st">  </span><span class="kw">facet_grid</span>(.<span class="op">~</span>ntree) <span class="op">+</span><span class="st"> </span><span class="kw">theme_bw</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb491-4" data-line-number="4"><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;AUC Across Random Forest Hyperparameters&quot;</span>)</a></code></pre></div>
<p><img src="biosml_files/figure-html/statsm-1.png" width="672" /></p>
<p>So what about the Accuracy ? It would be nice to see that on the graph also. There aren’t major variations in these metrics across these parameters. It depends on the method</p>
<div class="sourceCode" id="cb492"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb492-1" data-line-number="1"><span class="co"># Reformat the stats data frame into long format</span></a>
<a class="sourceLine" id="cb492-2" data-line-number="2">long_stats &lt;-<span class="st"> </span>stats <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">gather</span>(<span class="dt">key=</span><span class="st">&quot;variable&quot;</span>,<span class="dt">value=</span><span class="st">&quot;value&quot;</span>,<span class="op">-</span>mtry,<span class="op">-</span>ntree)</a>
<a class="sourceLine" id="cb492-3" data-line-number="3"></a>
<a class="sourceLine" id="cb492-4" data-line-number="4"><span class="co"># Next, make the plot</span></a>
<a class="sourceLine" id="cb492-5" data-line-number="5"><span class="kw">ggplot</span>(<span class="dt">data=</span>long_stats,</a>
<a class="sourceLine" id="cb492-6" data-line-number="6">       <span class="kw">aes</span>(<span class="dt">x=</span>mtry, <span class="dt">y=</span>value, <span class="dt">colour=</span>variable)) <span class="op">+</span></a>
<a class="sourceLine" id="cb492-7" data-line-number="7"><span class="st">       </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st"> </span><span class="kw">facet_wrap</span>(<span class="op">~</span>ntree) <span class="op">+</span><span class="st"> </span><span class="kw">theme_minimal</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb492-8" data-line-number="8"><span class="st">       </span><span class="kw">ggtitle</span>(<span class="st">&quot;AUC and Accuracy Across Random Forest Hyperparameters&quot;</span>)</a></code></pre></div>
<p><img src="biosml_files/figure-html/reformv-1.png" width="672" /></p>
<p>There are other ways to do this as some functions have tuning methods associated with them to help address this issue. The <strong>randomForest</strong> function is one such function. There is the <strong>tuneRF</strong> function available. Let’s go back to a basic example:</p>
<div class="sourceCode" id="cb493"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb493-1" data-line-number="1">my_rf &lt;-<span class="st"> </span><span class="kw">randomForest</span>(diabetes <span class="op">~</span><span class="st"> </span>. ,</a>
<a class="sourceLine" id="cb493-2" data-line-number="2">                           <span class="dt">data =</span> rf_train)</a></code></pre></div>
<p>So we could use the <strong>tuneRF</strong> function and pass it our X and Y variables along with some arguments to do a grid search with hyperparameters. The function itself will call the <strong>randomForest</strong> package:</p>
<div class="sourceCode" id="cb494"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb494-1" data-line-number="1">X =<span class="st"> </span>rf_train[,<span class="dv">1</span><span class="op">:</span><span class="dv">8</span>]</a>
<a class="sourceLine" id="cb494-2" data-line-number="2">Y =<span class="st"> </span>rf_train[,<span class="dv">9</span>]</a>
<a class="sourceLine" id="cb494-3" data-line-number="3"></a>
<a class="sourceLine" id="cb494-4" data-line-number="4">my_tune &lt;-<span class="st"> </span><span class="kw">tuneRF</span>(X,Y,<span class="dt">mtryStart=</span><span class="dv">2</span>,<span class="dt">ntreeTry=</span><span class="dv">50</span>,<span class="dt">stepFactor=</span><span class="dv">2</span>,<span class="dt">doBest=</span>T)</a></code></pre></div>
<pre><code>## mtry = 2  OOB error = 25.37% 
## Searching left ...
## mtry = 1     OOB error = 26.02% 
## -0.02564103 0.05 
## Searching right ...
## mtry = 4     OOB error = 25.53% 
## -0.006410256 0.05</code></pre>
<p><img src="biosml_files/figure-html/runtunef-1.png" width="672" /></p>
<div class="sourceCode" id="cb496"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb496-1" data-line-number="1">my_tune</a></code></pre></div>
<pre><code>## 
## Call:
##  randomForest(x = x, y = y, mtry = res[which.min(res[, 2]), 1]) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 2
## 
##         OOB estimate of  error rate: 23.25%
## Confusion matrix:
##     neg pos class.error
## neg 338  62   0.1550000
## pos  81 134   0.3767442</code></pre>
<div class="sourceCode" id="cb498"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb498-1" data-line-number="1"><span class="co"># Define the tuning grid: tuneGrid</span></a>
<a class="sourceLine" id="cb498-2" data-line-number="2">tuneGrid &lt;-<span class="st"> </span><span class="kw">data.frame</span>(</a>
<a class="sourceLine" id="cb498-3" data-line-number="3">  <span class="dt">.mtry =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">7</span>),</a>
<a class="sourceLine" id="cb498-4" data-line-number="4">  <span class="dt">.min.node.size =</span> <span class="dv">5</span></a>
<a class="sourceLine" id="cb498-5" data-line-number="5">)</a>
<a class="sourceLine" id="cb498-6" data-line-number="6"></a>
<a class="sourceLine" id="cb498-7" data-line-number="7">control &lt;-<span class="st"> </span><span class="kw">trainControl</span>( <span class="dt">classProbs=</span>T, <span class="dt">summaryFunction =</span> twoClassSummary)</a>
<a class="sourceLine" id="cb498-8" data-line-number="8"></a>
<a class="sourceLine" id="cb498-9" data-line-number="9">my_rf &lt;-<span class="st"> </span><span class="kw">train</span>(diabetes<span class="op">~</span>.,</a>
<a class="sourceLine" id="cb498-10" data-line-number="10">                     rf_train,</a>
<a class="sourceLine" id="cb498-11" data-line-number="11">                     <span class="dt">method=</span><span class="st">&quot;rf&quot;</span>,</a>
<a class="sourceLine" id="cb498-12" data-line-number="12">                     <span class="dt">metric=</span><span class="st">&quot;ROC&quot;</span>,</a>
<a class="sourceLine" id="cb498-13" data-line-number="13">                     <span class="dt">trControl =</span> control,</a>
<a class="sourceLine" id="cb498-14" data-line-number="14">               <span class="dt">tuneLength=</span><span class="dv">7</span>)</a></code></pre></div>
</div>
<div id="using-validation-data-sets" class="section level2">
<h2><span class="header-section-number">9.4</span> Using Validation Data Sets</h2>
<p>When building a model, we generate a training and test data set. We use the former to build a model and, if we are using something like the caret package, that process involves cross fold validation or bootstrap sampling to generate a good estimate for out-of-sample error. We then apply the model to the test data frame.</p>
<p>If we are using a method that has hyperparamters then maybe we want an intermediate data set to help validate our ultimate choice of hyperparameters. By taking this approach we can still keep our test data set off to the side for later use with the trained and validated model. Using this idea doesn’t require us to do much beyond generating a third data set.</p>
<pre><code>The training set is used to build the classifier
The validation set is used to tune the algorithm hyperparameters repeatedly. 
So there will be some overfitting here, but that is why there is another stage:

The test set must not be touched until the classifier is final to prevent 
overfitting. It serves to estimate the true accuracy, if you would put the model 
into production.</code></pre>
<div class="sourceCode" id="cb500"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb500-1" data-line-number="1"><span class="co"># Simple into 3 sets.</span></a>
<a class="sourceLine" id="cb500-2" data-line-number="2">idx &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">3</span>), <span class="dt">size =</span> <span class="kw">nrow</span>(pm), <span class="dt">replace =</span> <span class="ot">TRUE</span>, <span class="dt">prob =</span> <span class="kw">c</span>(.<span class="dv">8</span>, <span class="fl">.2</span>, <span class="fl">.2</span>))</a>
<a class="sourceLine" id="cb500-3" data-line-number="3">train &lt;-<span class="st"> </span>pm[idx <span class="op">==</span><span class="st"> </span><span class="dv">1</span>,]</a>
<a class="sourceLine" id="cb500-4" data-line-number="4">test &lt;-<span class="st"> </span>pm[idx <span class="op">==</span><span class="st"> </span><span class="dv">2</span>,]</a>
<a class="sourceLine" id="cb500-5" data-line-number="5">valid &lt;-<span class="st"> </span>pm[idx <span class="op">==</span><span class="st"> </span><span class="dv">3</span>,]</a>
<a class="sourceLine" id="cb500-6" data-line-number="6"></a>
<a class="sourceLine" id="cb500-7" data-line-number="7"><span class="co"># </span></a>
<a class="sourceLine" id="cb500-8" data-line-number="8"></a>
<a class="sourceLine" id="cb500-9" data-line-number="9"><span class="kw">nrow</span>(train)</a></code></pre></div>
<pre><code>## [1] 518</code></pre>
<div class="sourceCode" id="cb502"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb502-1" data-line-number="1"><span class="kw">nrow</span>(test)</a></code></pre></div>
<pre><code>## [1] 122</code></pre>
<div class="sourceCode" id="cb504"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb504-1" data-line-number="1"><span class="kw">nrow</span>(valid)</a></code></pre></div>
<pre><code>## [1] 128</code></pre>
<p>Then we might build a model using the training data. We’ll use Accuracy as a metric.</p>
<div class="sourceCode" id="cb506"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb506-1" data-line-number="1">control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;cv&quot;</span>,<span class="dt">number=</span><span class="dv">5</span>)</a>
<a class="sourceLine" id="cb506-2" data-line-number="2"></a>
<a class="sourceLine" id="cb506-3" data-line-number="3"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb506-4" data-line-number="4">train_rf &lt;-<span class="st"> </span><span class="kw">train</span>(diabetes<span class="op">~</span>.,</a>
<a class="sourceLine" id="cb506-5" data-line-number="5">                  <span class="dt">data =</span> train,</a>
<a class="sourceLine" id="cb506-6" data-line-number="6">                  <span class="dt">metric =</span> <span class="st">&quot;Accuracy&quot;</span>,</a>
<a class="sourceLine" id="cb506-7" data-line-number="7">                  <span class="dt">method =</span> <span class="st">&quot;rf&quot;</span>, </a>
<a class="sourceLine" id="cb506-8" data-line-number="8">                  <span class="dt">trControl =</span> control)</a>
<a class="sourceLine" id="cb506-9" data-line-number="9"></a>
<a class="sourceLine" id="cb506-10" data-line-number="10">train_rf</a></code></pre></div>
<pre><code>## Random Forest 
## 
## 518 samples
##   8 predictor
##   2 classes: &#39;neg&#39;, &#39;pos&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 414, 415, 414, 415, 414 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##   2     0.7624160  0.4513385
##   5     0.7604556  0.4537527
##   8     0.7585325  0.4454327
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 2.</code></pre>
<div class="sourceCode" id="cb508"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb508-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb508-2" data-line-number="2">valid_rf &lt;-<span class="st"> </span><span class="kw">train</span>(diabetes<span class="op">~</span>.,</a>
<a class="sourceLine" id="cb508-3" data-line-number="3">                  <span class="dt">data=</span>valid,</a>
<a class="sourceLine" id="cb508-4" data-line-number="4">                  <span class="dt">metric=</span><span class="st">&quot;Accuracy&quot;</span>,</a>
<a class="sourceLine" id="cb508-5" data-line-number="5">                  <span class="dt">method =</span> <span class="st">&quot;rf&quot;</span>,</a>
<a class="sourceLine" id="cb508-6" data-line-number="6">                  <span class="dt">trControl =</span> control)</a>
<a class="sourceLine" id="cb508-7" data-line-number="7">valid_rf<span class="op">$</span>bestTune</a></code></pre></div>
<pre><code>##   mtry
## 3    8</code></pre>
<div class="sourceCode" id="cb510"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb510-1" data-line-number="1"><span class="co"># Now we would use the value of mtry to find an appropriate value</span></a>
<a class="sourceLine" id="cb510-2" data-line-number="2"><span class="co"># of mtyry to use in the test data</span></a>
<a class="sourceLine" id="cb510-3" data-line-number="3"></a>
<a class="sourceLine" id="cb510-4" data-line-number="4"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb510-5" data-line-number="5">test_rf  &lt;-<span class="st"> </span><span class="kw">train</span>(diabetes<span class="op">~</span>.,</a>
<a class="sourceLine" id="cb510-6" data-line-number="6">                 <span class="dt">data=</span>test,</a>
<a class="sourceLine" id="cb510-7" data-line-number="7">                 <span class="dt">metric=</span><span class="st">&quot;Accuracy&quot;</span>,</a>
<a class="sourceLine" id="cb510-8" data-line-number="8">                 <span class="dt">method =</span> <span class="st">&quot;rf&quot;</span>,</a>
<a class="sourceLine" id="cb510-9" data-line-number="9">                 <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;none&quot;</span>),</a>
<a class="sourceLine" id="cb510-10" data-line-number="10">                 <span class="dt">tuneGrid =</span> <span class="kw">data.frame</span>(<span class="dt">mtry =</span> <span class="dv">5</span>))</a>
<a class="sourceLine" id="cb510-11" data-line-number="11"></a>
<a class="sourceLine" id="cb510-12" data-line-number="12"></a>
<a class="sourceLine" id="cb510-13" data-line-number="13">some_preds &lt;-<span class="st"> </span><span class="kw">predict</span>(test_rf,test)</a>
<a class="sourceLine" id="cb510-14" data-line-number="14"><span class="kw">confusionMatrix</span>(<span class="kw">table</span>(<span class="dt">preds=</span>some_preds,<span class="dt">actual=</span>test<span class="op">$</span>diabetes),<span class="dt">positive=</span><span class="st">&quot;pos&quot;</span>)</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##      actual
## preds neg pos
##   neg  81   0
##   pos   0  41
##                                      
##                Accuracy : 1          
##                  95% CI : (0.9702, 1)
##     No Information Rate : 0.6639     
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16  
##                                      
##                   Kappa : 1          
##                                      
##  Mcnemar&#39;s Test P-Value : NA         
##                                      
##             Sensitivity : 1.0000     
##             Specificity : 1.0000     
##          Pos Pred Value : 1.0000     
##          Neg Pred Value : 1.0000     
##              Prevalence : 0.3361     
##          Detection Rate : 0.3361     
##    Detection Prevalence : 0.3361     
##       Balanced Accuracy : 1.0000     
##                                      
##        &#39;Positive&#39; Class : pos        
## </code></pre>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="decision-trees.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="selecting-the-best-model.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
