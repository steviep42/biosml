<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Classification Example | Predictive Learning in R</title>
  <meta name="description" content="Chapter 7 Classification Example | Predictive Learning in R" />
  <meta name="generator" content="bookdown 0.14 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Classification Example | Predictive Learning in R" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Classification Example | Predictive Learning in R" />
  
  
  

<meta name="author" content="Steve Pittard" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="classification-problems.html"/>
<link rel="next" href="decision-trees.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#machine-learning"><i class="fa fa-check"></i><b>1.1</b> Machine Learning</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#predictive-modeling"><i class="fa fa-check"></i><b>1.2</b> Predictive Modeling</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#in-sample-vs-out-of-sample-data"><i class="fa fa-check"></i><b>1.3</b> In-Sample vs Out-Of-Sample Data</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#performance-metrics"><i class="fa fa-check"></i><b>1.4</b> Performance Metrics</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#black-box"><i class="fa fa-check"></i><b>1.5</b> Black Box</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html"><i class="fa fa-check"></i><b>2</b> Predictive / Supervised Learning</a><ul>
<li class="chapter" data-level="2.1" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#explanation-vs-prediction"><i class="fa fa-check"></i><b>2.1</b> Explanation vs Prediction</a></li>
<li class="chapter" data-level="2.2" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#two-types-of-predictive-models"><i class="fa fa-check"></i><b>2.2</b> Two Types of Predictive Models:</a></li>
<li class="chapter" data-level="2.3" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#bias-vs-variance"><i class="fa fa-check"></i><b>2.3</b> Bias vs Variance</a><ul>
<li class="chapter" data-level="2.3.1" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#bias"><i class="fa fa-check"></i><b>2.3.1</b> Bias</a></li>
<li class="chapter" data-level="2.3.2" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#variance"><i class="fa fa-check"></i><b>2.3.2</b> Variance</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#overfitting-and-underfitting"><i class="fa fa-check"></i><b>2.4</b> Overfitting and Underfitting</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="a-motivating-example.html"><a href="a-motivating-example.html"><i class="fa fa-check"></i><b>3</b> A Motivating Example</a><ul>
<li class="chapter" data-level="3.1" data-path="a-motivating-example.html"><a href="a-motivating-example.html#suggested-workflow"><i class="fa fa-check"></i><b>3.1</b> Suggested Workflow</a></li>
<li class="chapter" data-level="3.2" data-path="a-motivating-example.html"><a href="a-motivating-example.html#scatterplot"><i class="fa fa-check"></i><b>3.2</b> Scatterplot</a></li>
<li class="chapter" data-level="3.3" data-path="a-motivating-example.html"><a href="a-motivating-example.html#correlations"><i class="fa fa-check"></i><b>3.3</b> Correlations</a></li>
<li class="chapter" data-level="3.4" data-path="a-motivating-example.html"><a href="a-motivating-example.html#building-a-model---in-sample-error"><i class="fa fa-check"></i><b>3.4</b> Building A Model - In Sample Error</a></li>
<li class="chapter" data-level="3.5" data-path="a-motivating-example.html"><a href="a-motivating-example.html#out-of-sample-data"><i class="fa fa-check"></i><b>3.5</b> Out Of Sample Data</a></li>
<li class="chapter" data-level="3.6" data-path="a-motivating-example.html"><a href="a-motivating-example.html#other-methods"><i class="fa fa-check"></i><b>3.6</b> Other Methods ?</a></li>
<li class="chapter" data-level="3.7" data-path="a-motivating-example.html"><a href="a-motivating-example.html#summary"><i class="fa fa-check"></i><b>3.7</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="training-test-data.html"><a href="training-test-data.html"><i class="fa fa-check"></i><b>4</b> Training / Test Data</a><ul>
<li class="chapter" data-level="4.1" data-path="training-test-data.html"><a href="training-test-data.html#cross-fold-validation"><i class="fa fa-check"></i><b>4.1</b> Cross Fold Validation</a></li>
<li class="chapter" data-level="4.2" data-path="training-test-data.html"><a href="training-test-data.html#create-a-function-to-automate-things"><i class="fa fa-check"></i><b>4.2</b> Create A Function To Automate Things</a></li>
<li class="chapter" data-level="4.3" data-path="training-test-data.html"><a href="training-test-data.html#repeated-cross-validation"><i class="fa fa-check"></i><b>4.3</b> Repeated Cross Validation</a></li>
<li class="chapter" data-level="4.4" data-path="training-test-data.html"><a href="training-test-data.html#bootstrap"><i class="fa fa-check"></i><b>4.4</b> Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="caret-package.html"><a href="caret-package.html"><i class="fa fa-check"></i><b>5</b> Caret Package</a><ul>
<li class="chapter" data-level="5.1" data-path="caret-package.html"><a href="caret-package.html#putting-caret-to-work"><i class="fa fa-check"></i><b>5.1</b> Putting caret To Work</a></li>
<li class="chapter" data-level="5.2" data-path="caret-package.html"><a href="caret-package.html#back-to-the-beginning"><i class="fa fa-check"></i><b>5.2</b> Back To The Beginning</a></li>
<li class="chapter" data-level="5.3" data-path="caret-package.html"><a href="caret-package.html#splitting"><i class="fa fa-check"></i><b>5.3</b> Splitting</a></li>
<li class="chapter" data-level="5.4" data-path="caret-package.html"><a href="caret-package.html#calling-the-train-function"><i class="fa fa-check"></i><b>5.4</b> Calling The train() Function</a></li>
<li class="chapter" data-level="5.5" data-path="caret-package.html"><a href="caret-package.html#one-size-fits-all"><i class="fa fa-check"></i><b>5.5</b> One Size Fits All</a></li>
<li class="chapter" data-level="5.6" data-path="caret-package.html"><a href="caret-package.html#hyperparameters"><i class="fa fa-check"></i><b>5.6</b> Hyperparameters</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="classification-problems.html"><a href="classification-problems.html"><i class="fa fa-check"></i><b>6</b> Classification Problems</a><ul>
<li class="chapter" data-level="6.1" data-path="classification-problems.html"><a href="classification-problems.html#performance-measures"><i class="fa fa-check"></i><b>6.1</b> Performance Measures</a></li>
<li class="chapter" data-level="6.2" data-path="classification-problems.html"><a href="classification-problems.html#important-terminology"><i class="fa fa-check"></i><b>6.2</b> Important Terminology</a></li>
<li class="chapter" data-level="6.3" data-path="classification-problems.html"><a href="classification-problems.html#a-basic-model"><i class="fa fa-check"></i><b>6.3</b> A Basic Model</a></li>
<li class="chapter" data-level="6.4" data-path="classification-problems.html"><a href="classification-problems.html#selecting-the-correct-alpha"><i class="fa fa-check"></i><b>6.4</b> Selecting The Correct Alpha</a></li>
<li class="chapter" data-level="6.5" data-path="classification-problems.html"><a href="classification-problems.html#hypothesis-testing"><i class="fa fa-check"></i><b>6.5</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="6.6" data-path="classification-problems.html"><a href="classification-problems.html#confusion-matrix"><i class="fa fa-check"></i><b>6.6</b> Confusion Matrix</a><ul>
<li class="chapter" data-level="6.6.1" data-path="classification-problems.html"><a href="classification-problems.html#computing-performance-metrics"><i class="fa fa-check"></i><b>6.6.1</b> Computing Performance Metrics</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="classification-problems.html"><a href="classification-problems.html#picking-the-right-metric"><i class="fa fa-check"></i><b>6.7</b> Picking the Right Metric</a></li>
<li class="chapter" data-level="6.8" data-path="classification-problems.html"><a href="classification-problems.html#wait.-where-are-we"><i class="fa fa-check"></i><b>6.8</b> Wait. Where Are We ?</a></li>
<li class="chapter" data-level="6.9" data-path="classification-problems.html"><a href="classification-problems.html#better-ways-to-compute-the-roc-curve"><i class="fa fa-check"></i><b>6.9</b> Better Ways To Compute The ROC Curve</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="classification-example.html"><a href="classification-example.html"><i class="fa fa-check"></i><b>7</b> Classification Example</a><ul>
<li class="chapter" data-level="7.1" data-path="classification-example.html"><a href="classification-example.html#exploratory-plots"><i class="fa fa-check"></i><b>7.1</b> Exploratory Plots</a></li>
<li class="chapter" data-level="7.2" data-path="classification-example.html"><a href="classification-example.html#generalized-linear-models"><i class="fa fa-check"></i><b>7.2</b> Generalized Linear Models</a></li>
<li class="chapter" data-level="7.3" data-path="classification-example.html"><a href="classification-example.html#random-forests"><i class="fa fa-check"></i><b>7.3</b> Random Forests</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>8</b> Decision Trees</a><ul>
<li class="chapter" data-level="8.1" data-path="decision-trees.html"><a href="decision-trees.html#advantages"><i class="fa fa-check"></i><b>8.1</b> Advantages</a></li>
<li class="chapter" data-level="8.2" data-path="decision-trees.html"><a href="decision-trees.html#a-classification-example"><i class="fa fa-check"></i><b>8.2</b> A Classification Example</a><ul>
<li class="chapter" data-level="8.2.1" data-path="decision-trees.html"><a href="decision-trees.html#evaluating-performance"><i class="fa fa-check"></i><b>8.2.1</b> Evaluating performance</a></li>
<li class="chapter" data-level="8.2.2" data-path="decision-trees.html"><a href="decision-trees.html#tree-splitting"><i class="fa fa-check"></i><b>8.2.2</b> Tree Splitting</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="decision-trees.html"><a href="decision-trees.html#gini-index"><i class="fa fa-check"></i><b>8.3</b> Gini Index</a></li>
<li class="chapter" data-level="8.4" data-path="decision-trees.html"><a href="decision-trees.html#regression-trees"><i class="fa fa-check"></i><b>8.4</b> Regression Trees</a><ul>
<li class="chapter" data-level="8.4.1" data-path="decision-trees.html"><a href="decision-trees.html#performance-measure"><i class="fa fa-check"></i><b>8.4.1</b> Performance Measure</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="decision-trees.html"><a href="decision-trees.html#parameters-vs-hyperparameters"><i class="fa fa-check"></i><b>8.5</b> Parameters vs Hyperparameters</a></li>
<li class="chapter" data-level="8.6" data-path="decision-trees.html"><a href="decision-trees.html#grid-searching"><i class="fa fa-check"></i><b>8.6</b> Grid Searching</a></li>
<li class="chapter" data-level="8.7" data-path="decision-trees.html"><a href="decision-trees.html#bagged-trees"><i class="fa fa-check"></i><b>8.7</b> Bagged Trees</a></li>
<li class="chapter" data-level="8.8" data-path="decision-trees.html"><a href="decision-trees.html#random-forests-1"><i class="fa fa-check"></i><b>8.8</b> Random Forests</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="using-methods-other-than-lm.html"><a href="using-methods-other-than-lm.html"><i class="fa fa-check"></i><b>9</b> Using Methods Other Than lm</a><ul>
<li class="chapter" data-level="9.1" data-path="using-methods-other-than-lm.html"><a href="using-methods-other-than-lm.html#parameters-vs-hyperparameters-1"><i class="fa fa-check"></i><b>9.1</b> Parameters vs Hyperparameters</a></li>
<li class="chapter" data-level="9.2" data-path="using-methods-other-than-lm.html"><a href="using-methods-other-than-lm.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>9.2</b> Hyperparameter Tuning</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html"><i class="fa fa-check"></i><b>10</b> Picking The Best Model</a><ul>
<li class="chapter" data-level="10.1" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#an-example"><i class="fa fa-check"></i><b>10.1</b> An Example</a></li>
<li class="chapter" data-level="10.2" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#more-comparisons"><i class="fa fa-check"></i><b>10.2</b> More Comparisons</a></li>
<li class="chapter" data-level="10.3" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#using-the-resamples-function"><i class="fa fa-check"></i><b>10.3</b> Using the resamples() function</a></li>
<li class="chapter" data-level="10.4" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#model-performance"><i class="fa fa-check"></i><b>10.4</b> Model Performance</a></li>
<li class="chapter" data-level="10.5" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#feature-selection"><i class="fa fa-check"></i><b>10.5</b> Feature Selection</a><ul>
<li class="chapter" data-level="10.5.1" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#recursive-feature-elimination"><i class="fa fa-check"></i><b>10.5.1</b> Recursive Feature Elimination</a></li>
<li class="chapter" data-level="10.5.2" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#redundant-feature-removal"><i class="fa fa-check"></i><b>10.5.2</b> Redundant Feature Removal</a></li>
<li class="chapter" data-level="10.5.3" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#feature-importance"><i class="fa fa-check"></i><b>10.5.3</b> Feature Importance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="data-pre-processing.html"><a href="data-pre-processing.html"><i class="fa fa-check"></i><b>11</b> Data Pre Processing</a><ul>
<li class="chapter" data-level="11.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#types-of-pre-processing"><i class="fa fa-check"></i><b>11.1</b> Types of Pre Processing</a></li>
<li class="chapter" data-level="11.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#missing-values"><i class="fa fa-check"></i><b>11.2</b> Missing Values</a><ul>
<li class="chapter" data-level="11.2.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#finding-rows-with-missing-data"><i class="fa fa-check"></i><b>11.2.1</b> Finding Rows with Missing Data</a></li>
<li class="chapter" data-level="11.2.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#finding-columns-with-missing-data"><i class="fa fa-check"></i><b>11.2.2</b> Finding Columns With Missing Data</a></li>
<li class="chapter" data-level="11.2.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#use-the-median-approach"><i class="fa fa-check"></i><b>11.2.3</b> Use the Median Approach</a></li>
<li class="chapter" data-level="11.2.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#package-based-approach"><i class="fa fa-check"></i><b>11.2.4</b> Package-based Approach</a></li>
<li class="chapter" data-level="11.2.5" data-path="data-pre-processing.html"><a href="data-pre-processing.html#using-caret"><i class="fa fa-check"></i><b>11.2.5</b> Using caret</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#scaling"><i class="fa fa-check"></i><b>11.3</b> Scaling</a><ul>
<li class="chapter" data-level="11.3.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#methods-that-benefit-from-scaling"><i class="fa fa-check"></i><b>11.3.1</b> Methods That Benefit From Scaling</a></li>
<li class="chapter" data-level="11.3.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#methods-that-do-not-require-scaling"><i class="fa fa-check"></i><b>11.3.2</b> Methods That Do Not Require Scaling</a></li>
<li class="chapter" data-level="11.3.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#how-to-scale"><i class="fa fa-check"></i><b>11.3.3</b> How To Scale</a></li>
<li class="chapter" data-level="11.3.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#order-of-processing"><i class="fa fa-check"></i><b>11.3.4</b> Order of Processing</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#low-variance-variables"><i class="fa fa-check"></i><b>11.4</b> Low Variance Variables</a></li>
<li class="chapter" data-level="11.5" data-path="data-pre-processing.html"><a href="data-pre-processing.html#order-of-pre-processing"><i class="fa fa-check"></i><b>11.5</b> Order of Pre-Processing</a></li>
<li class="chapter" data-level="11.6" data-path="data-pre-processing.html"><a href="data-pre-processing.html#identifying-redundant-features"><i class="fa fa-check"></i><b>11.6</b> Identifying Redundant Features</a><ul>
<li class="chapter" data-level="11.6.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#highly-correlated-variables"><i class="fa fa-check"></i><b>11.6.1</b> Highly Correlated Variables</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="data-pre-processing.html"><a href="data-pre-processing.html#ranking-features"><i class="fa fa-check"></i><b>11.7</b> Ranking Features</a></li>
<li class="chapter" data-level="11.8" data-path="data-pre-processing.html"><a href="data-pre-processing.html#feature-selection-1"><i class="fa fa-check"></i><b>11.8</b> Feature Selection</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html"><i class="fa fa-check"></i><b>12</b> Using External ML Frameworks</a><ul>
<li class="chapter" data-level="12.1" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#using-h2o"><i class="fa fa-check"></i><b>12.1</b> Using h2o</a></li>
<li class="chapter" data-level="12.2" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#create-some-h20-models"><i class="fa fa-check"></i><b>12.2</b> Create Some h20 Models</a></li>
<li class="chapter" data-level="12.3" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#saving-a-model"><i class="fa fa-check"></i><b>12.3</b> Saving A Model</a></li>
<li class="chapter" data-level="12.4" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#using-the-h2o-auto-ml-feature"><i class="fa fa-check"></i><b>12.4</b> Using The h2o Auto ML Feature</a></li>
<li class="chapter" data-level="12.5" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#launching-a-job"><i class="fa fa-check"></i><b>12.5</b> Launching a Job</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Predictive Learning in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="classification-example" class="section level1">
<h1><span class="header-section-number">Chapter 7</span> Classification Example</h1>
<p>Now that we’ve got an idea about how we might judge the performance quality of classification problem let’s look at the mechanics of implementing a classificsastion model using the caret package. We’ve already seen it in action on a regression problem where we were predicting the MPG for the mtcars data frame. We’ll be sticking with the Pima Indians dataset that we used previously This data is provided by the <strong>mlbench</strong> package. The source of the information is the National Institute of Diabetes and Digestive and Kidney Diseases which in turn was hosted on the UCI Repository of Machine Learning. The variables are:</p>
<pre><code>pregnant - Number of times pregnant
glucose  - Plasma glucose concentration (glucose tolerance test)
pressure - Diastolic blood pressure (mm Hg)
triceps  - Triceps skin fold thickness (mm)
insulin  - 2-Hour serum insulin (mu U/ml)
mass       - Body mass index (weight in kg/(height in m)\^2)
pedigree - Diabetes pedigree function
age      - Age (years)
diabetes - Class variable (test for diabetes)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(mlbench)
<span class="kw">data</span>(<span class="st">&quot;PimaIndiansDiabetes&quot;</span>)

<span class="co"># The nqme is a little long so let&#39;s shorten it up</span>
pm &lt;-<span class="st"> </span>PimaIndiansDiabetes</code></pre></div>
<p>So let’s look at some exploratory plots to see if there is anything interesting happening. We’ll use the Data Explorer pacakge to help us with this although both R and Python have various packages to help with this kind of thing. In fact, there are probably too many packages and more are being developed every 6 months or so.</p>
<div id="exploratory-plots" class="section level2">
<h2><span class="header-section-number">7.1</span> Exploratory Plots</h2>
<p>We’ll look use some stock plots from the <a href="https://github.com/elastacloud/automatic-data-explorer"><strong>DataExplorer</strong></a> package to get a feel for the data. Look at correlations between the variables to see if any are strongly correlated with the variable we wish to predict or any other variables. Let’s start out with the <strong>plot_intro</strong> function which can provide an overview of our data. It turns out that our data is pretty clean. There are no rows with missing values and we have only one categorical feature.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot_intro</span>(pm)</code></pre></div>
<p><img src="biosml_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<p>Let’s see if there are any string correltations we need to be aware of.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot_correlation</span>(pm, <span class="dt">type=</span><span class="st">&quot;continuous&quot;</span>)</code></pre></div>
<p><img src="biosml_files/figure-html/decorr-1.png" width="672" /></p>
<p>There are more diabetes “negative” people than “positive”.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot_bar</span>(pm)</code></pre></div>
<p><img src="biosml_files/figure-html/debar-1.png" width="672" /></p>
<p>The histograms help us see what variables might be normally distributed although most of our features are skewed which makes sense in this case. For example, as people age, they tend to die so it’s not surprising that we have by far more young people. It looks to me that the insulin data is a little odd and might warrant greater consideration.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot_histogram</span>(pm)</code></pre></div>
<p><img src="biosml_files/figure-html/phist-1.png" width="672" /></p>
<p>This plot will show us side by side boxplots of the features as a function of “pos” or “neg”. This is helpful to determine if, for example, there might be significant differences between glucose levels across the positive and negative groups. It makes sense that there might be. Insulin might be also although it’s not totally apparent from the following graph. This is the kind of thing you would do to zone in on important variables.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot_boxplot</span>(pm,<span class="dt">by=</span><span class="st">&quot;diabetes&quot;</span>)</code></pre></div>
<p><img src="biosml_files/figure-html/pbox-1.png" width="672" /></p>
<p>This plot will help us see if any of our features are normally distibuted:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"> <span class="kw">plot_qq</span>(pm,<span class="dt">by=</span><span class="st">&quot;diabetes&quot;</span>)</code></pre></div>
<p><img src="biosml_files/figure-html/pqq-1.png" width="672" /></p>
<p>It turns out that Data Explorer will help us create a detailed report involving all of these plot tops.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">create_report</span>(pm, <span class="dt">y =</span> <span class="st">&quot;diabetes&quot;</span>)</code></pre></div>
<p>At this point we know that we want to predict “diabetes” and that perhaps glucose is an important variables in the data. We also don’t observe many strong correlations in the data so mulitcollinrarity isn’t a concern. We also don’t see strong evidence in the PCA plot that the data would benefit from a PCA transformation. One thing that we might consider doing is scaling the data since the features do not share the same measurement scale. We’ll take this into consideration.</p>
</div>
<div id="generalized-linear-models" class="section level2">
<h2><span class="header-section-number">7.2</span> Generalized Linear Models</h2>
<p>Let’s pick a technique to model the data with the ultimate goal of being able to predict whether someone has diabetes or not. We’ll start with the <strong>glm</strong> function in R. We’ll take a kitchen sink approach where we predict the diabetes variable (“yes” or “no”) based on the rest of the information in the data frame.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">123</span>)
idx &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(pm<span class="op">$</span>diabetes, <span class="dt">p =</span> .<span class="dv">8</span>, 
                                  <span class="dt">list =</span> <span class="ot">FALSE</span>, 
                                  <span class="dt">times =</span> <span class="dv">1</span>)
<span class="kw">head</span>(idx)</code></pre></div>
<pre><code>##      Resample1
## [1,]         1
## [2,]         2
## [3,]         3
## [4,]         4
## [5,]         5
## [6,]         6</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train &lt;-<span class="st"> </span>pm[ idx,]
test  &lt;-<span class="st"> </span>pm[<span class="op">-</span>idx,]

<span class="co">#</span>
<span class="kw">nrow</span>(train)</code></pre></div>
<pre><code>## [1] 615</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">nrow</span>(test)</code></pre></div>
<pre><code>## [1] 153</code></pre>
<p>If we used the non caret approach we might do something like the following:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pm_model_glm &lt;-<span class="st"> </span><span class="kw">glm</span>(diabetes <span class="op">~</span><span class="st"> </span>.,
                        <span class="dt">data =</span> train, <span class="dt">family=</span><span class="st">&quot;binomial&quot;</span>)

pm_model_fitpreds &lt;-<span class="st"> </span><span class="kw">predict</span>(pm_model_glm,test,<span class="dt">type=</span><span class="st">&quot;response&quot;</span>)

fitpredt &lt;-<span class="st"> </span><span class="cf">function</span>(t) <span class="kw">ifelse</span>(pm_model_fitpreds <span class="op">&gt;</span><span class="st"> </span>t , <span class="st">&quot;pos&quot;</span>,<span class="st">&quot;neg&quot;</span>)

fitpreds &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">fitpredt</span>(.<span class="dv">4</span>),<span class="dt">level=</span><span class="kw">levels</span>(test<span class="op">$</span>diabetes))

caret<span class="op">::</span><span class="kw">confusionMatrix</span>(fitpreds,
                       test<span class="op">$</span>diabetes,
                       <span class="dt">positive=</span><span class="st">&quot;pos&quot;</span>)</code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction neg pos
##        neg  88  20
##        pos  12  33
##                                           
##                Accuracy : 0.7908          
##                  95% CI : (0.7178, 0.8523)
##     No Information Rate : 0.6536          
##     P-Value [Acc &gt; NIR] : 0.0001499       
##                                           
##                   Kappa : 0.5211          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.2159249       
##                                           
##             Sensitivity : 0.6226          
##             Specificity : 0.8800          
##          Pos Pred Value : 0.7333          
##          Neg Pred Value : 0.8148          
##              Prevalence : 0.3464          
##          Detection Rate : 0.2157          
##    Detection Prevalence : 0.2941          
##       Balanced Accuracy : 0.7513          
##                                           
##        &#39;Positive&#39; Class : pos             
## </code></pre>
<p>And if you haven’t yet enough of ROC curves just yet, we could put up one of those.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caTools)
<span class="kw">colAUC</span>(pm_model_fitpreds,test<span class="op">$</span>diabetes,<span class="dt">plotROC=</span><span class="ot">TRUE</span>)</code></pre></div>
<p><img src="biosml_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<pre><code>##                  [,1]
## neg vs. pos 0.8296226</code></pre>
<p>But wait, we’ve already been through the whole ROC curve, AUC, confusion matrix route so why would we take a manual approach if we have the <strong>caret</strong> package readily available. We can explore any number methods, implement K Fold Cross Validation,and get feedback on the performance measures at the same time. Let’s reframe our above work using the caret package conveniences.</p>
<p>We’ve seen this before in the regression section so we’ll dive right in with a realistic example. We want to use Cross Fold validation here. We’ll select a metric of “Accuracy” and process the data by centering and scaling it since we have data on different measure scales.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ctrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, 
                     <span class="dt">number =</span> <span class="dv">5</span>)

pm_glm_mod &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">form =</span> diabetes <span class="op">~</span><span class="st"> </span>.,
                    <span class="dt">data =</span> train,
                    <span class="dt">trControl =</span> ctrl,
                    <span class="dt">metric =</span> <span class="st">&quot;Accuracy&quot;</span>,
                    <span class="dt">method =</span> <span class="st">&quot;glm&quot;</span>,
                    <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>,
                    <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>))
pm_glm_mod</code></pre></div>
<pre><code>## Generalized Linear Model 
## 
## 615 samples
##   8 predictor
##   2 classes: &#39;neg&#39;, &#39;pos&#39; 
## 
## Pre-processing: centered (8), scaled (8) 
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 492, 492, 492, 492, 492 
## Resampling results:
## 
##   Accuracy   Kappa    
##   0.7756098  0.4849025</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pm_glm_mod<span class="op">$</span>results</code></pre></div>
<pre><code>##   parameter  Accuracy     Kappa AccuracySD   KappaSD
## 1      none 0.7756098 0.4849025 0.04726648 0.1046417</code></pre>
<p>So we get an estiamte of a 77% accuracy rate when the model is applied to out of sample data. This isn’t so impressive but we aren’t here to solve that problem (at least not just yet). So let’s make some predictions use thing test data to see what the Accuracy rate is.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pm_glm_pred_labels &lt;-<span class="st"> </span><span class="kw">predict</span>(pm_glm_mod,test)
<span class="kw">confusionMatrix</span>(pm_glm_pred_labels,test<span class="op">$</span>diabetes)</code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction neg pos
##        neg  91  26
##        pos   9  27
##                                           
##                Accuracy : 0.7712          
##                  95% CI : (0.6965, 0.8352)
##     No Information Rate : 0.6536          
##     P-Value [Acc &gt; NIR] : 0.001098        
##                                           
##                   Kappa : 0.4536          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.006841        
##                                           
##             Sensitivity : 0.9100          
##             Specificity : 0.5094          
##          Pos Pred Value : 0.7778          
##          Neg Pred Value : 0.7500          
##              Prevalence : 0.6536          
##          Detection Rate : 0.5948          
##    Detection Prevalence : 0.7647          
##       Balanced Accuracy : 0.7097          
##                                           
##        &#39;Positive&#39; Class : neg             
## </code></pre>
<p>The <strong>train</strong> function provides is with an object that contains lots of informstion but in no way interferes with the results of the glm model. It’s as if you had built it using the standalone glm function which means that you can easily examine the model diagnostics:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(pm_glm_mod<span class="op">$</span>finalModel)</code></pre></div>
<pre><code>## 
## Call:
## NULL
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.5516  -0.7296  -0.4037   0.7642   2.7704  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -0.87828    0.10862  -8.086 6.16e-16 ***
## pregnant     0.34964    0.11914   2.935  0.00334 ** 
## glucose      1.03922    0.13048   7.965 1.66e-15 ***
## pressure    -0.25398    0.11127  -2.283  0.02246 *  
## triceps      0.01623    0.12153   0.134  0.89379    
## insulin     -0.17091    0.11809  -1.447  0.14781    
## mass         0.81801    0.13393   6.108 1.01e-09 ***
## pedigree     0.32550    0.10906   2.985  0.00284 ** 
## age          0.23529    0.12291   1.914  0.05558 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 796.05  on 614  degrees of freedom
## Residual deviance: 581.37  on 606  degrees of freedom
## AIC: 599.37
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>This includes the ability to see the various diagnostic plots:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(pm_glm_mod<span class="op">$</span>finalModel)</code></pre></div>
<p><img src="biosml_files/figure-html/glmplot-1.png" width="672" /><img src="biosml_files/figure-html/glmplot-2.png" width="672" /><img src="biosml_files/figure-html/glmplot-3.png" width="672" /><img src="biosml_files/figure-html/glmplot-4.png" width="672" /></p>
<p>We can certainly change the scoring metric to prioritize, for example, the area under the associated ROC curve. We just need to make some adjustments to the trainControl argument list and the train argument list. But these changes are minor.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ctrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, 
                     <span class="dt">number =</span> <span class="dv">5</span>,
                     <span class="dt">classProbs =</span> T,
                     <span class="dt">savePredictions =</span> T,  <span class="co"># Useful for Diagnostics</span>
                     <span class="dt">summaryFunction =</span> twoClassSummary)

pm_glm_mod &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">form =</span> diabetes <span class="op">~</span><span class="st"> </span>.,
                    <span class="dt">data =</span> train,
                    <span class="dt">trControl =</span> ctrl,
                    <span class="dt">metric =</span> <span class="st">&quot;ROC&quot;</span>,
                    <span class="dt">method =</span> <span class="st">&quot;glm&quot;</span>,
                    <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>,
                    <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>))

pm_glm_mod<span class="op">$</span>results</code></pre></div>
<pre><code>##   parameter  ROC   Sens      Spec      ROCSD     SensSD     SpecSD
## 1      none 0.83 0.8775 0.5767442 0.03093329 0.01629801 0.08607793</code></pre>
<p>Notice that we get a different result back than before. Here we get the Area Under the ROC curve as well as the Sensitivity and Specifity. In many ways, this is all we need but if we wanted more we could use the MLeval package to help us.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">MLeval<span class="op">::</span><span class="kw">evalm</span>(pm_glm_mod)</code></pre></div>
<p><img src="biosml_files/figure-html/classmeval-1.png" width="672" /><img src="biosml_files/figure-html/classmeval-2.png" width="672" /><img src="biosml_files/figure-html/classmeval-3.png" width="672" /><img src="biosml_files/figure-html/classmeval-4.png" width="672" /></p>
<pre><code>## $roc</code></pre>
<p><img src="biosml_files/figure-html/classmeval-5.png" width="672" /></p>
<pre><code>## 
## $proc</code></pre>
<p><img src="biosml_files/figure-html/classmeval-6.png" width="672" /></p>
<pre><code>## 
## $prg</code></pre>
<p><img src="biosml_files/figure-html/classmeval-7.png" width="672" /></p>
<pre><code>## 
## $cc</code></pre>
<p><img src="biosml_files/figure-html/classmeval-8.png" width="672" /></p>
<pre><code>## 
## $probs
## $probs$`Group 1`
##            neg       pos obs   Group TP  TN FP  FN        SENS   SPEC
## 32  0.02217280 0.9778272 pos Group 1  1 400  0 214 0.004651163 1.0000
## 162 0.02653983 0.9734602 neg Group 1  1 399  1 214 0.004651163 0.9975
## 133 0.02927972 0.9707203 pos Group 1  2 399  1 213 0.009302326 0.9975
## 91  0.03753611 0.9624639 pos Group 1  3 399  1 212 0.013953488 0.9975
## 273 0.03843262 0.9615674 pos Group 1  4 399  1 211 0.018604651 0.9975
## 271 0.03875394 0.9612461 pos Group 1  5 399  1 210 0.023255814 0.9975
## 152 0.05006443 0.9499356 pos Group 1  6 399  1 209 0.027906977 0.9975
## 599 0.05343322 0.9465668 neg Group 1  6 398  2 209 0.027906977 0.9950
## 132 0.05395120 0.9460488 pos Group 1  7 398  2 208 0.032558140 0.9950
## 243 0.05592280 0.9440772 neg Group 1  7 397  3 208 0.032558140 0.9925
## 576 0.05665742 0.9433426 neg Group 1  7 396  4 208 0.032558140 0.9900
## 428 0.06152255 0.9384775 pos Group 1  8 396  4 207 0.037209302 0.9900
## 491 0.06592492 0.9340751 pos Group 1  9 396  4 206 0.041860465 0.9900
## 407 0.06838412 0.9316159 pos Group 1 10 396  4 205 0.046511628 0.9900
## 71  0.07126228 0.9287377 pos Group 1 11 396  4 204 0.051162791 0.9900
## 81  0.07656184 0.9234382 neg Group 1 11 395  5 204 0.051162791 0.9875
## 99  0.07860194 0.9213981 pos Group 1 12 395  5 203 0.055813953 0.9875
## 497 0.08375937 0.9162406 pos Group 1 13 395  5 202 0.060465116 0.9875
## 167 0.08654437 0.9134556 pos Group 1 14 395  5 201 0.065116279 0.9875
## 158 0.09218622 0.9078138 pos Group 1 15 395  5 200 0.069767442 0.9875
## 376 0.09390555 0.9060945 neg Group 1 15 394  6 200 0.069767442 0.9850
## 398 0.10118226 0.8988177 pos Group 1 16 394  6 199 0.074418605 0.9850
## 512 0.10209732 0.8979027 pos Group 1 17 394  6 198 0.079069767 0.9850
## 2   0.10414621 0.8958538 pos Group 1 18 394  6 197 0.083720930 0.9850
## 59  0.10540637 0.8945936 neg Group 1 18 393  7 197 0.083720930 0.9825
## 126 0.10622243 0.8937776 pos Group 1 19 393  7 196 0.088372093 0.9825
## 530 0.10900417 0.8909958 pos Group 1 20 393  7 195 0.093023256 0.9825
## 163 0.11778919 0.8822108 pos Group 1 21 393  7 194 0.097674419 0.9825
## 490 0.12492220 0.8750778 pos Group 1 22 393  7 193 0.102325581 0.9825
## 431 0.12575488 0.8742451 pos Group 1 23 393  7 192 0.106976744 0.9825
## 201 0.12820510 0.8717949 pos Group 1 24 393  7 191 0.111627907 0.9825
## 386 0.12981904 0.8701810 pos Group 1 25 393  7 190 0.116279070 0.9825
## 151 0.13085837 0.8691416 pos Group 1 26 393  7 189 0.120930233 0.9825
## 255 0.13214019 0.8678598 pos Group 1 27 393  7 188 0.125581395 0.9825
## 484 0.13418598 0.8658140 pos Group 1 28 393  7 187 0.130232558 0.9825
## 518 0.13443483 0.8655652 pos Group 1 29 393  7 186 0.134883721 0.9825
## 301 0.13723213 0.8627679 pos Group 1 30 393  7 185 0.139534884 0.9825
## 575 0.13782587 0.8621741 pos Group 1 31 393  7 184 0.144186047 0.9825
## 230 0.13919771 0.8608023 pos Group 1 32 393  7 183 0.148837209 0.9825
## 67  0.14016631 0.8598337 pos Group 1 33 393  7 182 0.153488372 0.9825
## 413 0.14441435 0.8555857 pos Group 1 34 393  7 181 0.158139535 0.9825
## 377 0.14555044 0.8544496 pos Group 1 35 393  7 180 0.162790698 0.9825
## 300 0.15186416 0.8481358 pos Group 1 36 393  7 179 0.167441860 0.9825
## 606 0.15499952 0.8450005 pos Group 1 37 393  7 178 0.172093023 0.9825
## 317 0.15674045 0.8432595 neg Group 1 37 392  8 178 0.172093023 0.9800
## 558 0.15834015 0.8416599 pos Group 1 38 392  8 177 0.176744186 0.9800
## 344 0.15874539 0.8412546 pos Group 1 39 392  8 176 0.181395349 0.9800
## 408 0.16338788 0.8366121 pos Group 1 40 392  8 175 0.186046512 0.9800
## 157 0.17179225 0.8282077 neg Group 1 40 391  9 175 0.186046512 0.9775
## 554 0.17215232 0.8278477 pos Group 1 41 391  9 174 0.190697674 0.9775
## 244 0.17486927 0.8251307 pos Group 1 42 391  9 173 0.195348837 0.9775
## 190 0.18502185 0.8149782 pos Group 1 43 391  9 172 0.200000000 0.9775
##     Informedness      PREC       NPV      MARK          F1        MCC
## 32   0.004651163 1.0000000 0.6514658 0.6514658 0.009259259 0.05504610
## 162  0.002151163 0.5000000 0.6508972 0.1508972 0.009216590 0.01801678
## 133  0.006802326 0.6666667 0.6519608 0.3186275 0.018348624 0.04655543
## 91   0.011453488 0.7500000 0.6530278 0.4030278 0.027397260 0.06794170
## 273  0.016104651 0.8000000 0.6540984 0.4540984 0.036363636 0.08551664
## 271  0.020755814 0.8333333 0.6551724 0.4885057 0.045248869 0.10069426
## 152  0.025406977 0.8571429 0.6562500 0.5133929 0.054054054 0.11420928
## 599  0.022906977 0.7500000 0.6556837 0.4056837 0.053811659 0.09640014
## 132  0.027558140 0.7777778 0.6567657 0.4345435 0.062500000 0.10943130
## 243  0.025058140 0.7000000 0.6561983 0.3561983 0.062222222 0.09447575
## 576  0.022558140 0.6363636 0.6556291 0.2919928 0.061946903 0.08115919
## 428  0.027209302 0.6666667 0.6567164 0.3233831 0.070484581 0.09380313
## 491  0.031860465 0.6923077 0.6578073 0.3501150 0.078947368 0.10561641
## 407  0.036511628 0.7142857 0.6589018 0.3731875 0.087336245 0.11672911
## 71   0.041162791 0.7333333 0.6600000 0.3933333 0.095652174 0.12724267
## 81   0.038662791 0.6875000 0.6594324 0.3469324 0.095238095 0.11581612
## 99   0.043313953 0.7058824 0.6605351 0.3664175 0.103448276 0.12598011
## 497  0.047965116 0.7222222 0.6616415 0.3838638 0.111587983 0.13569108
## 167  0.052616279 0.7368421 0.6627517 0.3995938 0.119658120 0.14500048
## 158  0.057267442 0.7500000 0.6638655 0.4138655 0.127659574 0.15395136
## 376  0.054767442 0.7142857 0.6632997 0.3775854 0.127118644 0.14380329
## 398  0.059418605 0.7272727 0.6644182 0.3916909 0.135021097 0.15255730
## 512  0.064069767 0.7391304 0.6655405 0.4046710 0.142857143 0.16101918
## 2    0.068720930 0.7500000 0.6666667 0.4166667 0.150627615 0.16921501
## 59   0.066220930 0.7200000 0.6661017 0.3861017 0.150000000 0.15990001
## 126  0.070872093 0.7307692 0.6672326 0.3980018 0.157676349 0.16795006
## 530  0.075523256 0.7407407 0.6683673 0.4091081 0.165289256 0.17577592
## 163  0.080174419 0.7500000 0.6695060 0.4195060 0.172839506 0.18339478
## 490  0.084825581 0.7586207 0.6706485 0.4292692 0.180327869 0.19082192
## 431  0.089476744 0.7666667 0.6717949 0.4384615 0.187755102 0.19807097
## 201  0.094127907 0.7741935 0.6729452 0.4471388 0.195121951 0.20515417
## 386  0.098779070 0.7812500 0.6740995 0.4553495 0.202429150 0.21208253
## 151  0.103430233 0.7878788 0.6752577 0.4631365 0.209677419 0.21886598
## 255  0.108081395 0.7941176 0.6764200 0.4705376 0.216867470 0.22551355
## 484  0.112732558 0.8000000 0.6775862 0.4775862 0.224000000 0.23203343
## 518  0.117383721 0.8055556 0.6787565 0.4843120 0.231075697 0.23843311
## 301  0.122034884 0.8108108 0.6799308 0.4907416 0.238095238 0.24471942
## 575  0.126686047 0.8157895 0.6811092 0.4968987 0.245059289 0.25089864
## 230  0.131337209 0.8205128 0.6822917 0.5028045 0.251968504 0.25697653
## 67   0.135988372 0.8250000 0.6834783 0.5084783 0.258823529 0.26295842
## 413  0.140639535 0.8292683 0.6846690 0.5139373 0.265625000 0.26884921
## 377  0.145290698 0.8333333 0.6858639 0.5191972 0.272373541 0.27465346
## 300  0.149941860 0.8372093 0.6870629 0.5242722 0.279069767 0.28037538
## 606  0.154593023 0.8409091 0.6882662 0.5291753 0.285714286 0.28601889
## 317  0.152093023 0.8222222 0.6877193 0.5099415 0.284615385 0.27849335
## 558  0.156744186 0.8260870 0.6889279 0.5150149 0.291187739 0.28412249
## 344  0.161395349 0.8297872 0.6901408 0.5199281 0.297709924 0.28967909
## 408  0.166046512 0.8333333 0.6913580 0.5246914 0.304182510 0.29516634
## 157  0.163546512 0.8163265 0.6908127 0.5071393 0.303030303 0.28799454
## 554  0.168197674 0.8200000 0.6920354 0.5120354 0.309433962 0.29346748
## 244  0.172848837 0.8235294 0.6932624 0.5167918 0.315789474 0.29887600
## 190  0.177500000 0.8269231 0.6944938 0.5214169 0.322097378 0.30422277
##        FPR        PG RG
## 32  0.0000 1.0000000  0
## 162 0.0025 0.1156250  0
## 133 0.0025 0.3250000  0
## 91  0.0025 0.4617187  0
## 273 0.0025 0.5540000  0
## 271 0.0025 0.6197917  0
## 152 0.0025 0.6688776  0
## 599 0.0050 0.4617187  0
## 132 0.0050 0.5120370  0
## 243 0.0075 0.3771250  0
## 576 0.0100 0.2805785  0
## 428 0.0100 0.3250000  0
## 491 0.0100 0.3647929  0
## 407 0.0100 0.4005102  0
## 71  0.0100 0.4326667  0
## 81  0.0125 0.3571777  0
## 99  0.0125 0.3866782  0
## 497 0.0125 0.4137731  0
## 167 0.0125 0.4387119  0
## 158 0.0125 0.4617187  0
## 376 0.0150 0.4005102  0
## 398 0.0150 0.4223140  0
## 512 0.0150 0.4426749  0
## 2   0.0150 0.4617187  0
## 59  0.0175 0.4100400  0
## 126 0.0175 0.4282729  0
## 530 0.0175 0.4454733  0
## 163 0.0175 0.4617187  0
## 490 0.0175 0.4770809  0
## 431 0.0175 0.4916250  0
## 201 0.0175 0.5054110  0
## 386 0.0175 0.5184937  0
## 151 0.0175 0.5309229  0
## 255 0.0175 0.5427444  0
## 484 0.0175 0.5540000  0
## 518 0.0175 0.5647280  0
## 301 0.0175 0.5749635  0
## 575 0.0175 0.5847386  0
## 230 0.0175 0.5940828  0
## 67  0.0175 0.6030234  0
## 413 0.0175 0.6115854  0
## 377 0.0175 0.6197917  0
## 300 0.0175 0.6276636  0
## 606 0.0175 0.6352208  0
## 317 0.0200 0.5974815  0
## 558 0.0200 0.6051985  0
## 344 0.0200 0.6126301  0
## 408 0.0200 0.6197917  0
## 157 0.0225 0.5857976  0
## 554 0.0225 0.5930650  0
## 244 0.0225 0.6000865  0
## 190 0.0225 0.6068741  0
##  [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 563 rows ]
## 
## 
## $optres
## $optres$`Group 1`
##                Score        CI
## SENS           0.786 0.73-0.84
## SPEC           0.730 0.68-0.77
## MCC            0.495      &lt;NA&gt;
## Informedness   0.516      &lt;NA&gt;
## PREC           0.610 0.55-0.67
## NPV            0.864  0.82-0.9
## FPR            0.270      &lt;NA&gt;
## F1             0.687      &lt;NA&gt;
## TP           169.000      &lt;NA&gt;
## FP           108.000      &lt;NA&gt;
## TN           292.000      &lt;NA&gt;
## FN            46.000      &lt;NA&gt;
## AUC-ROC        0.830 0.79-0.87
## AUC-PR         0.690      &lt;NA&gt;
## AUC-PRG        0.250      &lt;NA&gt;
## 
## 
## $stdres
## $stdres$`Group 1`
##                Score        CI
## SENS           0.577 0.51-0.64
## SPEC           0.878 0.84-0.91
## MCC            0.482      &lt;NA&gt;
## Informedness   0.454      &lt;NA&gt;
## PREC           0.717 0.65-0.78
## NPV            0.794 0.75-0.83
## FPR            0.122      &lt;NA&gt;
## F1             0.639      &lt;NA&gt;
## TP           124.000      &lt;NA&gt;
## FP            49.000      &lt;NA&gt;
## TN           351.000      &lt;NA&gt;
## FN            91.000      &lt;NA&gt;
## AUC-ROC        0.830 0.79-0.87
## AUC-PR         0.690      &lt;NA&gt;
## AUC-PRG        0.250      &lt;NA&gt;</code></pre>
</div>
<div id="random-forests" class="section level2">
<h2><span class="header-section-number">7.3</span> Random Forests</h2>
<p>Let’s use random forests to see what results we get. Random forests are robust to over fitting and are fairly easy to implement. They can improve accuracy by fitting many trees. Each tree is fit to a resampled version of the input data (usually a bootstrap). This is known as bootstrap aggregation or “bagged” trees. At each split, the function takes a random sample of columns (the mtry argument).</p>
<p>The function we will use here, <strong>ranger</strong>, has three hyper parameters which could be set to a range of values which, in turn, could influence the resulting model. With glm, we didn’t really have a hyper parameter. Here is how to tell if a caret-supported model has one or more hyper parameters available for tuning:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">modelLookup</span>(<span class="st">&quot;ranger&quot;</span>)</code></pre></div>
<pre><code>##    model     parameter                         label forReg forClass
## 1 ranger          mtry #Randomly Selected Predictors   TRUE     TRUE
## 2 ranger     splitrule                Splitting Rule   TRUE     TRUE
## 3 ranger min.node.size             Minimal Node Size   TRUE     TRUE
##   probModel
## 1      TRUE
## 2      TRUE
## 3      TRUE</code></pre>
<p>We’ll switch out metric back to Accuracy</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ctrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, 
                     <span class="dt">number =</span> <span class="dv">5</span>
                     )

pm_ranger_mod &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">form =</span> diabetes <span class="op">~</span><span class="st"> </span>.,
                    <span class="dt">data =</span> train,
                    <span class="dt">trControl =</span> ctrl,
                    <span class="dt">metric =</span> <span class="st">&quot;Accuracy&quot;</span>,
                    <span class="dt">method =</span> <span class="st">&quot;ranger&quot;</span>,
                    <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>)
                    )</code></pre></div>
<p>By default the training process will move through three different values of mtry though we could either set this explicitly in the train function or as part of the hyper parameter tuning processed mentioned previously. If we choose the latter, then we can take advantage of the fact that <strong>caret</strong> knows what hyper parameters the method supports and can cycle through possible valid values of these hyper parameters. This is accomplished via the <strong>tuneLength</strong> argument to the <strong>train</strong> function. We could use the <strong>tuneGrid</strong> argument along with a manually specified tuning grid but it’s easier to use <strong>tuneLength</strong> for now.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pm_ranger_mod</code></pre></div>
<pre><code>## Random Forest 
## 
## 615 samples
##   8 predictor
##   2 classes: &#39;neg&#39;, &#39;pos&#39; 
## 
## Pre-processing: centered (8), scaled (8) 
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 492, 492, 492, 492, 492 
## Resampling results across tuning parameters:
## 
##   mtry  splitrule   Accuracy   Kappa    
##   2     gini        0.7430894  0.4171190
##   2     extratrees  0.7495935  0.4200788
##   5     gini        0.7512195  0.4438470
##   5     extratrees  0.7609756  0.4514632
##   8     gini        0.7495935  0.4402630
##   8     extratrees  0.7609756  0.4578970
## 
## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 1
## Accuracy was used to select the optimal model using the largest value.
## The final values used for the model were mtry = 5, splitrule =
##  extratrees and min.node.size = 1.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ctrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, 
                     <span class="dt">number =</span> <span class="dv">5</span>,
                     <span class="dt">classProbs =</span> <span class="ot">TRUE</span>,
                     <span class="dt">summaryFunction =</span> twoClassSummary
                     )

pm_ranger_mod &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">form =</span> diabetes <span class="op">~</span><span class="st"> </span>.,
                    <span class="dt">data =</span> train,
                    <span class="dt">trControl =</span> ctrl,
                    <span class="dt">metric =</span> <span class="st">&quot;ROC&quot;</span>,
                    <span class="dt">method =</span> <span class="st">&quot;ranger&quot;</span>,
                    <span class="dt">tuneLength =</span> <span class="dv">7</span>,
                    <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>)
                    )</code></pre></div>
<p>The object can be plotted. Here we see that the max AUC of .825 occurs when mtry is 3 and the Gini criterion is used to evaluate a tree.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(pm_ranger_mod)</code></pre></div>
<p><img src="biosml_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">max</span>(pm_ranger_mod[[<span class="st">&quot;results&quot;</span>]]<span class="op">$</span>ROC)</code></pre></div>
<pre><code>## [1] 0.8190116</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">preds &lt;-<span class="st"> </span><span class="kw">predict</span>(pm_ranger_mod,test)
<span class="kw">confusionMatrix</span>(preds,test<span class="op">$</span>diabetes)</code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction neg pos
##        neg  88  21
##        pos  12  32
##                                           
##                Accuracy : 0.7843          
##                  95% CI : (0.7106, 0.8466)
##     No Information Rate : 0.6536          
##     P-Value [Acc &gt; NIR] : 0.0003018       
##                                           
##                   Kappa : 0.5039          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.1637344       
##                                           
##             Sensitivity : 0.8800          
##             Specificity : 0.6038          
##          Pos Pred Value : 0.8073          
##          Neg Pred Value : 0.7273          
##              Prevalence : 0.6536          
##          Detection Rate : 0.5752          
##    Detection Prevalence : 0.7124          
##       Balanced Accuracy : 0.7419          
##                                           
##        &#39;Positive&#39; Class : neg             
## </code></pre>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="classification-problems.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="decision-trees.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
