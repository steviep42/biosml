[
["index.html", "Predictive Learning in R Chapter 1 Preface 1.1 Machine Learning 1.2 Predictive Modeling 1.3 In-Sample vs Out-Of-Sample Error 1.4 Performance Metrics 1.5 Black Box", " Predictive Learning in R Steve Pittard - wsp@emory.edu Chapter 1 Preface Predictive Modeling is a type of Machine Learning which itself is a sub branch of Artificial Intelligence. The following graphic provides us with some history of these domains. This is helpful if you are trying to orient yourself in the world of analytics and machine learning. Note that AI has been around for quite some time. The Wikipedia definition of AI is: The study of “intelligent agents”: any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals 1.1 Machine Learning Machine Learning relies upon “patterns and inference”&quot; to “perform a specific task without using explicit instructions”. It is a form of Applied AI that attempts to automatically learn from experience without being explicitly programmed. Think of Predictive Modeling as a subset of this which falls into two categories: Supervised Algorithms that build a model on a set of data containing both the inputs and the desired outputs (“labels” or known numeric values). When you want to map input to known output labels. Build a model that, when applied to “new” data, will hopefully predict the correct label. Some common techniques for Supervised learning include: Generalized Linear Models (GLM), Logistic Regression, Random Forests, Decision Trees, Neural Networks, Multivariate Adaptive Regression Splines (MARS), and K Nearest Neighbors. Unsupervised Algorithms that take a set of data that contains only inputs, and find structure in the data (e.g. clustering of data points) Some common techniques for unsupervised learning include: hierarchical clustering, k-means clustering, mixture models, DBSCAN, Association Rules, Neural Networks 1.2 Predictive Modeling This lecture is concerned primarily with Predictive Modeling. Some examples of Predictive Modeling include: Predict current CD4 cell count of an HIV-positive patient using genome sequences Predict Success of Grant Applications Use attributes of chemical compounds to predict likelihood of hepatic injury How many copies of a new book will sell ? Will a customer change Internet Service Providers ? In this domain there are generally two types of predictive models. 1.3 In-Sample vs Out-Of-Sample Error The goal of predictive model is to generate models that can generalize to new data. It would be good if any model we generate could provide a good estimate of out of sample error. It’s easy to generate a model on an entire data set (in sample data) and then turn around and use that data for prediction. But how will it perform on new data ? Haven’t we just over trained our model ? 1.4 Performance Metrics For either case (regression vs classification) we need some type of metric or measure to let us know how well a given model will work on new or unseen data - also known as “out of sample” data. for Classification problems we look at things like “sensitivity”, “specificity”, “accuracy”, and “Area Under Curve”. For Quantitative outcomes, we look at things like Root Mean Square Error (RMSE) or Mean Absolute Error (MAE). Here is the formula for Root Mean Square Error (RMSE). P represents a vector of predictions and O represents a vector of the observed (true) values. \\[ RMSE = \\sqrt\\frac{\\sum_i^n(P_i-O_i)^2}{n} \\] 1.5 Black Box The good news is that you can treat building predictive models as a “Black Box”. The bad news is that you can treat building predictive models as a “Black Box”. It’s better to get up and running with building some models than it is to sit back and read how to do it. The process of assessing models and diagnosing them can be an in depth process thus I don’t wish to understate what it takes to do that. However, moving forward with some of the basic mechanics and concepts is good. One can always work with the individual methods or go to Section 6 to start with caret. The larger message is to get busy getting your hands dirty. "],
["predictive-supervised-learning.html", "Chapter 2 Predictive / Supervised Learning 2.1 Explanation vs Prediction 2.2 Bias vs Variance 2.3 Overfitting and Underfitting 2.4 Some Important Terminology 2.5 Levels of Measurement", " Chapter 2 Predictive / Supervised Learning In Predictive Learning it is customary to apply methods to build models on existing data to help you Understand the data at hand and Build a model to predict outcomes on related information Of course, number 2 assumes that what you already know in terms of the data and model is suitable for application to a more general situation when this might not be the case. 2.1 Explanation vs Prediction Sometimes you just want to build a model (apply a method) to explain the data at hand as opposed to using it to predict outcomes for incoming data of a related nature. Not all models need to be predictive and in fact it might be useful to first get a basic understanding of the data before attempting to extend it to new information. “A common misconception in various scientific fields is that predictive power can be inferred from explanatory power. However, the two are different and should be assessed separately.” — Gamit Shmueli - “To Explain or Predict” Given a data set, there might be some variable therein that you would like to predict in terms of other variables. Ideally you would like to automate this process but not at the expense of understanding the underlying statistical considerations. The process should also be done in a way that allows one to define a specific performance measure / metric that can then be used to compare performance across a number of “models”. Relative to explanatory work you wish to drill down into the data to better understand it and are less concerned about how any resulting model might apply to unseen or new data. In this case you might not look at other data sets or permutated versions of the data in which case you just build different models against the same data while trying out different features to arrive a the “best” model. In predictive work, we typically use techniques such as cross fold validation and bootstrapping to provide us with different versions of the data to “train” a model after which we validate it on some “test” data that has been held out. So in this context, you will probably be looking at multiple versions of the original data set which adds to the overhead somewhat. 2.1.1 Titanic Data A good example of explanatory work is the often referred to “Titanic” data set that has lots of information on the passengers who traveled on the ill-fated Titanic ship. This data set is used in many Data Science type educational contexts to talk about building models. url &lt;- &quot;https://raw.githubusercontent.com/steviep42/bios534_spring_2020/master/data/etitanic.csv&quot; titan &lt;- read.csv(url) head(titan) ## pclass survived sex age sibsp parch ## 1 1st 1 female 29.0000 0 0 ## 2 1st 1 male 0.9167 1 2 ## 3 1st 0 female 2.0000 1 2 ## 4 1st 0 male 30.0000 1 2 ## 5 1st 0 female 25.0000 1 2 ## 6 1st 1 male 48.0000 0 0 This data set provides information on the fate of passengers on the fatal maiden voyage of the ocean liner ‘Titanic’, summarized according to economic status (class), sex, age and survival. pclass Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd) survival Survival (0 = No; 1 = Yes) sex Sex age Age sibsp Number of Siblings/Spouses Aboard parch Number of Parents/Children Aboard So then we look at different plots to become acquainted with the data. Here let’s see the survival counts by Gender: titan %&gt;% mutate(survived=factor(survived,labels=c(&quot;Died&quot;,&quot;Survived&quot;))) %&gt;% group_by(survived,sex) %&gt;% count() %&gt;% ungroup() %&gt;% mutate(sex = fct_reorder(sex,n)) %&gt;% ggplot(aes(x=sex,y=n,fill=survived)) + geom_col() + theme_light() + xlab(&quot;Gender&quot;) + ylab(&quot;Number of Persons&quot;) + ggtitle(&quot;Titanic Survival By Gender&quot;) Next, we’ll see the survival rates by Passenger Class. If you saw the movie Titanic then you know what this will look like: titan %&gt;% mutate(survived=factor(survived,labels=c(&quot;Died&quot;,&quot;Survived&quot;))) %&gt;% group_by(survived,pclass) %&gt;% count() %&gt;% ungroup() %&gt;% mutate(pclass = fct_reorder(pclass,n)) %&gt;% ggplot(aes(x=pclass,y=n,fill=survived)) + geom_col() + theme_light() + xlab(&quot;Passenger Class&quot;) + ylab(&quot;Number of Persons&quot;) + ggtitle(&quot;Titanic Survival By Passenger Class&quot;) Next, let’s see the above plot broken out by gender: titan %&gt;% mutate(survived=factor(survived,labels=c(&quot;Died&quot;,&quot;Survived&quot;))) %&gt;% group_by(survived,pclass,sex) %&gt;% count() %&gt;% ungroup() %&gt;% mutate(pclass = fct_reorder(pclass,n)) %&gt;% ggplot(aes(x=pclass,y=n,fill=survived)) + geom_col() + theme_light() + facet_wrap(~sex) + xlab(&quot;Passenger Class&quot;) + ylab(&quot;Number of Persons&quot;) + ggtitle(&quot;Titanic Survival By Passenger Class&quot;) We could even look at age distributions here. What patterns do you see here ? Did young children fare better than say someone who was 25-35 ? ptitanic %&gt;% drop_na() %&gt;% ggplot(aes(x=age)) + geom_histogram(aes(fill=survived),bins=20) + theme_light() + facet_wrap(~pclass) 2.2 Bias vs Variance See this blog for more information. 2.2.1 Bias Think of the term bias as meaning deviation from the truth or reality. If a model fits some data very well then deviations from the actual values will be small which is what you might expect when applying the model to the very data used to produce it. You are able to explain the data but not then be able to use it to predict outcomes without experiencing large variance. Any performance metric that is computed on the data “as is” will typically be somewhat optimistic in the context of predictive utility. The model you build on some data might fit the data very well (low bias) but then does not do very well when applied to a new data set wherein the model exhibits large variance. A model might not, on average, fit some existing data very well (high bias) but results in more predictable results (lower variance) when applied to new data. 2.2.2 Variance The concept of variance generally refers to a given algorithm’s sensitivity to unfamiliar or new data. An algorithm that has been well-trained, or even over trained, on a data set might exhibit low bias in that it “knows” or “hugs” the data very well. However, when given new data that might not be from the same distribution as the “training” data then large and/or unpredictable variance might result. If you flip this around a bit, an algorithm whose bias is high generally means that it deviates from the “truth” perhaps to a significant degree. Correspondingly, when the resulting model is applied to new or unseen data it will behave in a more predictable manner, generally resulting in lower variance. This isn’t always a bad thing. Maybe a more biased model is quick and easy to compute and it’s estimated performance (as determined by some metric such as RMSE) might be acceptable when applied to new data. The general conclusion is that there is a trade off between Bias and Variance. The higher the bias, the lower the variance and the higher the variance, the lower the bias. 2.3 Overfitting and Underfitting If we overfit some data we then undermine our ability to apply it to new data in a way that results in good performance. This is the case where we have high model bias and resulting high variability when it is applied to new data. If you do “too good of a job” of learning some data then you might actually be unknowingly modeling inherent sources of error. If we underfit some data then consider that we haven’t “learned enough” from it to ensure low bias (deviations from reality). On the other hand if the model does a “good enough job” of describing the data then maybe it’s not a big deal especially since, when applied to new data, it doesn’t exhibit a high degree of variance. Simpler models, such as linear regression, can be like this - easy to understand but somewhat biased in their assessment of data. Depending on the size of the data it might be computationally less expensive to build more biased models. Which one of the models is more biased ? plot(mpg~wt,data=example_Train,main=&quot;Two Models For The Same Data&quot;) abline(lm_fit$finalModel) grid() svm_preds &lt;- predict(my_svm,example_Train) pdf &lt;- data.frame(cbind(Train$wt,svm_preds)) points(svm_preds~V1,data=pdf[order(pdf$V1),],col=&quot;red&quot;,pch=4,type=&quot;l&quot;) legend(&quot;topright&quot;,c(&quot;lm&quot;,&quot;svm&quot;),col=c(&quot;black&quot;,&quot;red&quot;),lty=1,cex=1) Models that are biased generally have less variability when applied to new data whereas less biased models generally have higher variability when applied to new data. Obviously, striking a balance between the two is desirable. There are techniques to help with this. 2.4 Some Important Terminology While we could start building a lot of models at this point it would be wise to review some jargon in the world of predictive modeling so we can understand how to fully exploit the variables (also known as “features”) in our data. This is very important. Let’s start out with a basic example involving the most (over)used data set in R Education - the “mtcars” data frame. If you are using R you can simply type data(mtcars) or read it in as follows: url &lt;- &quot;https://raw.githubusercontent.com/steviep42/bios534_spring_2020/master/data/mtcars.csv&quot; mtcars &lt;- read.csv(url, stringsAsFactors = FALSE) If you are using Python then reading it in looks something like: import pandas as pd url = &quot;https://raw.githubusercontent.com/steviep42/bios534_spring_2020/master/data/mtcars.csv&quot; dfmtcars = pd.read_csv(url) dfmtcars.head() mpg cyl disp hp drat wt qsec vs am gear carb 0 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 1 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 2 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 3 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 4 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 The documentation for the data frame is as follows: Motor Trend Car Road Tests The data was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models). A data frame with 32 observations on 11 (numeric) variables. [, 1] mpg Miles/(US) gallon [, 2] cyl Number of cylinders [, 3] disp Displacement (cu.in.) [, 4] hp Gross horsepower [, 5] drat Rear axle ratio [, 6] wt Weight (1000 lbs) [, 7] qsec 1/4 mile time [, 8] vs Engine (0 = V-shaped, 1 = straight) [, 9] am Transmission (0 = automatic, 1 = manual) [,10] gear Number of forward gears [,11] carb Number of carburetors We need to understand the variables in this data frame beyond the superficial. Let’s say that we wanted to predict the mpg for a given car using other variables in the data frame. We might also want to predict whether a car has an automatic or manual transmission based on some other variables. Some jargon would be useful here. For example, if we wanted to predict the mpg, consider the following graphic: Or if we wanted to predict the transmission type - the jargon is the same but the variable type is different: In the first case we are attempting to predict a numeric quantity whereas in the second case we are predicting a binary outcome (a 0 or 1) which corresponds to, respectively, an automatic or manual transmission. The fact that it appears to be numeric is a bit deceptive since it’s just a label - we wouldn’t compute on this number. So how do we get to know the data ? Plotting it might be one way but there are a couple of ways to get some clues up front using some code: mtcars %&gt;% summarize_all(n_distinct) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 25 3 27 22 22 29 30 2 2 3 6 Be on the lookout for columns that don’t have very many unique values such as vs, am, and cyl. These are more likely categories by which we might summarize the continuous quantities. As an example - what is the average mpg for each transmission type ? What is the average mpg for all cylinder types ? Even though all the variables appear to be numeric doesn’t mean that they can be “computed” on. Does it make sense to take the average of the transmission type ? It’s either 0 or 1 (auto or manual). There is no such thing as a car with a 0.40625 transmission. mean(mtcars$am) ## [1] 0.40625 Perhaps it’s better to count the number of times each transmission type occurs. table(mtcars$am) ## ## 0 1 ## 19 13 Or summarize MPG in terms of the am mtcars %&gt;% mutate(am=factor(am,labels=c(&quot;Auto&quot;,&quot;Manual&quot;))) %&gt;% group_by(am) %&gt;% dplyr::summarize(Average_mpg=mean(mpg)) ## # A tibble: 2 x 2 ## am Average_mpg ## &lt;fct&gt; &lt;dbl&gt; ## 1 Auto 17.1 ## 2 Manual 24.4 2.5 Levels of Measurement We should review the concepts behind the “levels of measurement” which are helpful in classifying the variables we have in this, or other, data frames. Here is a graphic which summarizes them. (Note: see this video from which the graphics were taken.) 2.5.1 Nominal The lowest level of measurement Discrete Categories No Order Binary Qualitative or Categorical Examples include gender (assuming binary M/F), colors (“red”,“blue”,“pink”), marital status, religion, political affiliation (“republican”,“democrat”), smoker (“Y/N”), names Possible measures for this type of information include Mode, Range, Percentage 2.5.2 Ordinal Ordered Categories Relative Rankings Unknown Distance Between Rankings Zero Is Arbitrary Examples include “first, second, third”, “low, middle, high”, Likert scales, size (small, medium,large), movie ratings, satisfaction ratings, pain ratings (1-10) Possible measures include all those for nominal data with the addition of Median, Percentile, and Rank order or correlations. Can be counted but not measured granulalrly. 2.5.3 Interval Ordered categories Equal distance between values Known and accepted unit of measurement Zero is arbitrary The difference between two variables is on an interval. Fahrenheit or Celsius temp (0 does not represent and absolute lowest value). Interval scales tell us about order and differences. Elevation, time of day. Measures include all those for Ordinal with the addition of mean, standard deviation, addition, and subtraction. However, multiplication and division wouldn’t make sense given that there is no absolute zero. Would a cylinder value of 0 make sense ? 2.5.4 Ratio Equal Intervals Ordered Natural Zero Examples include Weight, Height, Time (stopwatch) degrees Kelvin, blood pressure, Can use all Interval tests, descriptive and inferential statistics, can make comparisons, add, subtract, multiply, divide. Decision Tree for Levels of Measurement: "],
["a-motivating-example.html", "Chapter 3 A Motivating Example 3.1 A More Detailed Workflow 3.2 Visualizations 3.3 Correlations 3.4 Building A Model - In Sample Error 3.5 Out Of Sample Data 3.6 Some Additional Considerations 3.7 Other Methods ? 3.8 Summary", " Chapter 3 A Motivating Example Here is a high level over view of an ML work flow. Note that: It is a cycle, quite likely to be repeated multiple times before arriving at some actionable result The driving questions / hypotheses are subject to change, redefinition, or abandonment Multiple people might be involved 3.1 A More Detailed Workflow Thus far we haven’t gotten our hands dirty but we’ll need to do that if we want an experiential approach to any of this. It’s useful to have a schematic that outlines the general process. Consider the following workflow. In general this is a solid representation of what one might do as part of building a predictive model. Let’s keep working with the mtcars data frame: Motor Trend Car Road Tests The data was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models). A data frame with 32 observations on 11 (numeric) variables. [, 1] mpg Miles/(US) gallon [, 2] cyl Number of cylinders [, 3] disp Displacement (cu.in.) [, 4] hp Gross horsepower [, 5] drat Rear axle ratio [, 6] wt Weight (1000 lbs) [, 7] qsec 1/4 mile time [, 8] vs Engine (0 = V-shaped, 1 = straight) [, 9] am Transmission (0 = automatic, 1 = manual) [,10] gear Number of forward gears [,11] carb Number of carburetors 3.2 Visualizations This is a big topic on its own but winds up being the gateway into forming hypotheses to help drive the aims of any machine learning project. On the other hand, some people just start building models with variables they believe to be of interest and only later worry about how the data might look. With respect to the above data frame, the idea of Fuel Economy is usually a popular concept which then leads one to look at the mpg feature in the data set. Or, when we look at the Pima Indians dataset, we’ll see that the “diabetes” column is sure to be of interest to someone wanting to use other variables from that dataset to predict whether someone has that disease or not. Not all data sets have a clear cut variable that you are trying to predict. Sometimes you go on a “fishing expedition” to find interesting variables and it’s only after working with the data, building models, and evaluating them that one comes to an understanding of what variables / features are actually relevant. In the mean time though, doing some plots can be helpful 3.2.1 Scatterplots For continuous quantities, things like histograms, scatterplots, strip charts, and dot plots can be useful. Let’s look at a pairs() plot which plots each variable in the data frame against all others to see if there are any obvious linear relationships between any of the variables. Some of these variables can be considered as factors or categories (such as cyl, vs, am, gear, and carb) so for now we will exclude them to focus only on the continuous / measured variables. pairs(mtcars, main=&quot;Pairs Plot for All Variables&quot;) This plot is rather busy but instructive nonetheless as it confirms the idea that some variables, such as am, vs, cyl, gear are more ordinal or categorical than truly numeric in nature. We could narrow down our plot by looking at just the continuous quantities. This is also interesting in that we see a number of apparent linear or polynomial style relationships that might benefit from some form on linear modeling. If we are already thinking about how to deal with the mpg variable we can see that it is correlated with a number of variables. pairs(mtcars[,c(1,3:7)], main=&quot;Pairs plot for numeric features&quot;) We might also look at how MPG values are distributed across cylinder groups (4,6, or 8). The boxplot is an ideal choice for summarizing continuous data across groups. We check to see if there appear to be significant differences between the median MPG. 3.2.2 Boxplots mtcars %&gt;% mutate(cyl=factor(cyl)) %&gt;% ggplot(aes(x=cyl,y=mpg)) + geom_boxplot() + theme_bw() + ggtitle(&quot;MPG Across Cylinder Groups&quot;) The same could be done for transmission types where we can definitely see a difference emerging between MPG for Automatic (0) vs Manual (1) Transmissions. mtcars %&gt;% mutate(cyl=factor(cyl),am=factor(am,labels=c(&quot;Auto&quot;,&quot;Manual&quot;))) %&gt;% ggplot(aes(x=am,y=mpg)) + geom_boxplot() + theme_bw() + ggtitle(&quot;MPG Across Transmission Types&quot;) + xlab(&quot;Transmission Type&quot;) If we wanted to look at above relationships simultaneously we could use something like ggplot to help us. mtcars %&gt;% mutate(cyl=factor(cyl),am=factor(am,labels=c(&quot;Auto&quot;,&quot;Manual&quot;))) %&gt;% ggplot(aes(x=am,y=mpg)) + geom_boxplot() + facet_wrap(.~cyl) + theme_bw() + ggtitle(&quot;MPG Across Transmission Types And Cylinder Group&quot;) + xlab(&quot;Transmission Type&quot;) 3.2.3 Histograms Another favorite plot type for numeric data is the histogram although we don’t have much data here. Still, we can make a histogram of the MPG to get a sense of a possible distribution. This could look better but you get the overall message that most cars exhibit MPG in the range of 12-25 MPG. mtcars %&gt;% ggplot(aes(x=mpg)) + geom_histogram(bins=15,fill=&quot;aquamarine&quot;,color=&quot;black&quot;) + theme_bw() + ggtitle(&quot;Distribution of MPG&quot;) mtcars %&gt;% ggplot(aes(x=mpg)) + geom_histogram(bins=8,fill=&quot;aquamarine&quot;,color=&quot;black&quot;) + theme_classic() + facet_wrap(~factor(am,labels=c(&quot;Auto&quot;,&quot;Manual&quot;)),ncol=1) + ggtitle(&quot;Distribution of MPG By Transmission Type&quot;) The larger point of this graph is that we clearly see differences between MPG across the transmission types. 3.2.4 Tables Tables a are a good way to visualize relationships between categories. This might be helpful to identify what categories are more (or less) frequently occurring. One need not use graphics to see these relationships. Text Tables can easily be made although they can become hard to interpret. mtcars %&gt;% mutate(am=factor(am,labels=c(&quot;Automatic&quot;,&quot;Manual&quot;)), cyl=factor(cyl)) %&gt;% count(am,cyl) ## # A tibble: 6 x 3 ## am cyl n ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 Automatic 4 3 ## 2 Automatic 6 4 ## 3 Automatic 8 12 ## 4 Manual 4 8 ## 5 Manual 6 3 ## 6 Manual 8 2 This is why we might choose a bar plot since now we can easily see that Manual Transmissions are under represented in the 8 cylinder class. mtcars %&gt;% mutate(am=factor(am,labels=c(&quot;Automatic&quot;,&quot;Manual&quot;)), cyl=factor(cyl)) %&gt;% count(am,cyl) %&gt;% ggplot(aes(x=cyl,y=n)) + geom_col(aes(fill=am)) + theme_classic() While knowing how to make plots and graphs are useful. 3.3 Correlations Let’s look at some correlations to see how we might predict MPG as a function of other variables in the data set. Note that this isn’t an in-depth modeling lecture so we will fast track over deep discussions on how to fully evaluate and diagnose a model emerging from a specific method - although that is important. But, in the interest of motivating a work flow, we’ll simplify some of those discussions for now. library(DataExplorer) plot_correlation(mtcars) There are some strong correlations here and perhaps a case could be made for collinearity but we aren’t going to get into that right now. We also have variables on different measurement scales but, again, we’ll hold off dealing with that for the moment. As a preview of the caret package, we’ll use the findCorrelation function to help identify variables that are correlated above a certain threshold. Here, we’ll identify variables that are correlated at a level of 0.7 or above. This function takes a correlation matrix as input. We have a number of variables we might consider removing but, again, we’ll hold off on that for now. data(mtcars) caret::findCorrelation(cor(mtcars), cutoff = 0.7, names = TRUE) ## [1] &quot;cyl&quot; &quot;disp&quot; &quot;mpg&quot; &quot;wt&quot; &quot;hp&quot; &quot;vs&quot; &quot;drat&quot; &quot;am&quot; 3.4 Building A Model - In Sample Error So now we will use the above information to build a linear model using the mtcars data frame. We’ll turn around and use the same exact data frame to test our model - Any resulting error we see will be in sample error and will not generalize well to new data. However, the model will not help us anticipate any out of sample error. data(mtcars) # Let&#39;s evaluate a basic formula myform &lt;- formula(mpg~wt) # Use the built in &quot;lm&quot; function lm_model &lt;- lm(myform,data=mtcars) summary(lm_model) ## ## Call: ## lm(formula = myform, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.5432 -2.3647 -0.1252 1.4096 6.8727 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 37.2851 1.8776 19.858 &lt; 2e-16 *** ## wt -5.3445 0.5591 -9.559 1.29e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.046 on 30 degrees of freedom ## Multiple R-squared: 0.7528, Adjusted R-squared: 0.7446 ## F-statistic: 91.38 on 1 and 30 DF, p-value: 1.294e-10 # Do the predictions on the data set used to train the # model. This isn&#39;t what you want to do in the real world training_preds &lt;- predict( lm_model, newdata=mtcars, type=&quot;response&quot; ) So let’s see what this looks like graphically. Remember that we want to be able to compute the Root Mean Square Error for this model: title &lt;- &quot;lm - mpg~wt&quot; plot(mpg~wt,mtcars,pch=19,main=title, ylim=c(min(mtcars$mpg)-5,max(mtcars$mpg))) abline(lm_model) grid() segments(mtcars$wt,training_preds, mtcars$wt,mtcars$mpg, col=&quot;red&quot;,lwd=1.2) Let’s compute the RMSE for this model. The formula for this is: \\[ RMSE = \\sqrt\\frac{\\sum_i^n(P_i-O_i)^2}{n} \\] errors &lt;- training_preds-mtcars$mpg training_rmse &lt;- sqrt(mean(errors^2)) print(training_rmse) ## [1] 2.949163 There are built in R functions to compute this: Metrics::rmse(mtcars$mpg,training_preds) ## [1] 2.949163 Is this good ? Bad ? Just average ? Well we don’t really know. One good thing is that the metric is in terms of the predicted variable, mpg, so it can easily be interpreted. However, unless someone has specified a tolerance level for the RMSE we don’t know if we have something that can be extended to other car types. We also could experiment with other regression formula to see if the RMSE goes down (or up). 3.5 Out Of Sample Data Now let’s repeat this exercise by generating a linear model on a subset of the mtcars data frame and then apply that model to the remaining data. In modeling parlance this is known as having a “training” and “test” data set. The idea here is to build a model using say the first 21 rows of mtcars (a training set that is roughly 65% of the data) and then use a test set, rows 22 - 32 of mtcars, as input to the model to determine how well the model performs. Remember - we want to minimize the RMSE. The first 21 rows are outlined in green and rows 22-32 are outlined in red. This means we are training on a subset of the data and we hope that any model we build thereon will be extensible to the holdout or test data frame lm_model_train &lt;- lm(myform,data=mtcars[1:21,]) # Do the prediction on the test set test_preds &lt;- predict( lm_model_train, newdata=mtcars[22:32,], type=&quot;response&quot; ) (test_rmse &lt;- Metrics::rmse(mtcars[22:32,]$mpg,test_preds)) ## [1] 3.286759 We trained the model on the first 21 rows of the data frame which might contain some outliers (or not). The RMSE got larger ! Does this mean the model is “bad” ? Maybe, maybe not. One thing we could do is to experiment with another split of the data, perhaps in a different proportion (e.g. 80/20) or maybe even a series of splits to see if we can get an idea of how widely the RMSE varies. Here we create a sample of 80% of mtcars to create a training set. set.seed(123) (train_index &lt;- sample(1:nrow(mtcars),nrow(mtcars)*.80)) ## [1] 31 15 19 14 3 10 18 22 11 5 20 29 23 30 9 28 8 27 7 32 26 17 4 1 24 # Get 80% of the records from the data frame train_df &lt;- mtcars[train_index,] # We have approx 80% of the data in train_df nrow(train_df) ## [1] 25 # Get the other 20% that we wish to test on test_df &lt;- mtcars[-train_index,] nrow(test_df) ## [1] 7 Now do the modeling train_model_lm &lt;- lm(myform, data=train_df) # Test the model on the test / holdout data frame test_pred &lt;- predict( train_model_lm, newdata=test_df, type=&quot;response&quot; ) (test_rmse &lt;- Metrics::rmse(test_df$mpg,test_pred)) ## [1] 2.021501 3.6 Some Additional Considerations So in smaller data sets it can be useful to choose a different split that allocates more data to the testing dataset such as a 60/40 or 70/30 split. It might also be useful to first shuffle the data frame in case there is some type of intrinsic ordering of which we were unaware. Both of these are easy to do. set.seed(123) shuffled &lt;- sample(1:nrow(mtcars)) shuff_mtcars &lt;- mtcars[shuffled,] (train_index &lt;- sample(1:nrow(shuff_mtcars),nrow(shuff_mtcars)*.70)) ## [1] 9 32 10 23 27 21 7 28 6 2 5 8 12 13 18 1 31 11 16 24 22 14 # Get 80% of the records from the data frame train_df &lt;- shuff_mtcars[train_index,] # We have approx 80% of the data in train_df nrow(train_df) ## [1] 22 # Get the other 20% that we wish to test on test_df &lt;- shuff_mtcars[-train_index,] train_model_lm &lt;- lm(myform, data=train_df) # Test the model on the test / holdout data frame test_pred &lt;- predict( train_model_lm, newdata=test_df, type=&quot;response&quot; ) (test_rmse &lt;- Metrics::rmse(test_df$mpg,test_pred)) ## [1] 2.668003 set.seed(123) split_train_test &lt;- function(prop=.70) { shuffled &lt;- sample(1:nrow(mtcars)) shuff_mtcars &lt;- mtcars[shuffled,] (train_index &lt;- sample(1:nrow(shuff_mtcars),nrow(shuff_mtcars)*prop)) # Get prop of the records from the data frame train_df &lt;- shuff_mtcars[train_index,] # Get the other 20% that we wish to test on test_df &lt;- shuff_mtcars[-train_index,] train_model_lm &lt;- lm(myform, data=train_df) test_pred &lt;- predict(train_model_lm, newdata=test_df,type=&quot;response&quot;) return(Metrics::rmse(test_df$mpg,test_pred)) } The first thing to notice is that multiple calls to the function can yield different RMSE values even without changing the proportion of the train / test sets. Let’s call this function 100 times with the 70/30 split and check the resulting boxplot to see how the RMSE values vary replicate(100,split_train_test()) %&gt;% boxplot() Does this look much different than say a 60/40 split ? Or a 65/35 replicate(100,split_train_test(prop=.6)) %&gt;% boxplot() # replicate(100,split_train_test(prop=.65)) %&gt;% boxplot() Let’s generalize this comparison myseq &lt;- seq(.6,.8,.05) comp_mat &lt;- t(replicate(100,sapply(myseq,function(x) split_train_test(x)))) colnames(comp_mat) &lt;- as.character(myseq) boxplot(comp_mat,ylab=&quot;RMSE&quot;,xlab=&quot;Train / Test Proportion&quot;, main=&quot;Comparison of Different Train / Test Proportions&quot;) It looks to me like the .6 or .7 split provides the least variation for the RMSE which would be quite useful when estimating out of sample error. While other splits seem to give a lower median RMSE, they have more variation. 3.7 Other Methods ? Could we improve the situation by using another modeling method ? This would be something that we could try with out much effort as long as we know the name of the method and how to call it. This can sometimes be a challenge as you have to find the documentation for it and figure out how to prepare the data before calling the command. In the above case we just used the formula interface which is favored in R but not all functions support that. Just to say that you will generally have to study each new function to see how it works. Let’s check out the XGBoost function which is an algorithm that has enjoyed recent popularity and celebrity in the world of Data Science. XGBoost is an implementation of gradient boosted decision trees designed for speed and performance. XGBoost requires us to specify the input as matrices. We also have to give it the input in the form of X, Y arguments where the first is a matrix of all the predictor variables and the latter is the thing being predicted. In this case the MPG which is the first column library(xgboost) xgb &lt;- xgboost(data = as.matrix(train_df[,-1]), label=train_df[,1],nrounds=25,verbose=0) How does this perform in the prediction phase ? It turns out that it’s pretty impressive when compared to the humble lm function. The question remains though which method is easier to defend if you were challenged ? The lm function implements a very well established and well known model whose parameters are well known. So even though XGBoost seems to be good at minimizing the RMSE it might be better to stick with an approach like lm even if it performs more poorly. xgboost_preds &lt;- predict(xgb,as.matrix(test_df[,-1])) Metrics::rmse(test_df$mpg,xgpreds) ## [1] 2.008207 split_train_test_xg &lt;- function(prop=.70) { shuffled &lt;- sample(1:nrow(mtcars)) shuff_mtcars &lt;- mtcars[shuffled,] (train_index &lt;- sample(1:nrow(shuff_mtcars),nrow(shuff_mtcars)*prop)) # Get prop of the records from the data frame train_df &lt;- shuff_mtcars[train_index,] # Get the other 20% that we wish to test on test_df &lt;- shuff_mtcars[-train_index,] xgb &lt;- xgboost(data = as.matrix(train_df[,-1]), label=train_df[,1], nrounds=25, verbose=0) xgboost_preds &lt;- predict(xgb,as.matrix(test_df[,-1])) return(Metrics::rmse(test_df$mpg,xgboost_preds)) } set.seed(123) myseq &lt;- seq(.6,.8,.05) comp_mat &lt;- t(replicate(100,sapply(myseq,function(x) split_train_test_xg(x)))) colnames(comp_mat) &lt;- as.character(myseq) boxplot(comp_mat,ylab=&quot;RMSE&quot;,xlab=&quot;Train / Test Proportion&quot;, main=&quot;XGBoost - Comparison of Different Train / Test Proportions&quot;) 3.8 Summary Aside from trying an alternative method to lm what we have done here is to sample some portion of the original mtcars data frame to use as a training set while holding out the rest of the data to use as a test data to see how well our model performed. We could repeat this (re)sampling activity multiple times to better train our data over different segments or “folds” of data so any model we ultimately generate will “learn” as much from the data as it can without modeling any “noise”. There are various methods for doing this including K-Fold Cross Validation and Bootstrap Resampling. Let’s dig in a little deeper into these methods because they help us build models that might offer more robust performance when applied to new data. "],
["training-test-data.html", "Chapter 4 Training / Test Data 4.1 Cross Fold Validation 4.2 Create A Function To Automate Things 4.3 Repeated Cross Validation 4.4 Bootstrap", " Chapter 4 Training / Test Data https://www.stat.berkeley.edu/~aldous/157/Papers/shmueli.pdf Predictive power is assessed using metrics computedfrom a holdout set or using cross-validation (Stone,1974; Geisser,1975) Testing the procedure on the data that gave it birth is almost certain to overestimate performance” (Mosteller and Tukey,1977). Let’s extend this idea of training and test splits. Remember, our goal is to generate a robust model that better estimates out-of-sample error. We can do this by resampling our data set in a way that allows us to learn from the data but not so much so that it follows the data set too closely. We can take a single data set and partition / split it into a number of train / test subsets. We just did that in the earlier section but we only did it once. If we do this a number of times we hope we are training our model more effectively. What would the RMSE look like if we created say K number of subsets of the data frame and selectively held out each of the K subsets, built a model on the combined remaining subsets, and then tested the model on the holdout ? We would then average the RMSE to get an idea of its variation. The series of sequential steps would be as follows: Subset the data frame into k groups For each subset: Consider the subset as a &quot;hold out&quot;&quot; or test data set Combine the remaining subsets as a training data set Fit a model on the combined training set Evaluate the model using the holdout test set Save the evaluation score (e.g. RNSE) Summarize evaluation score (e.g. mean of RMSE) This is called K-Fold Cross Validation. Here is the general idea in illustrated form relative to mtcars. Assume we want 4 folds. We would divide the data frame into 4 folds of 8 records each. The first model would be built using Fold 1 as the holdout / test data set after first combining Folds 2,3 and 4 into a training set set So the second iteration would then take the second fold as the holdout / test data frame and combine Folds 1,3, and 4 into a training data frame. # Generates Some Folds num_of_folds &lt;- 4 # This generates 8 groups of 4 indices such that each # group has unique observations. No observation is used # more than once - although we could use bootstrapping set.seed(321) folds &lt;- split(sample(1:nrow(mtcars)),1:num_of_folds) # We should have 32 indicies across the 8 groups sum(sapply(folds,length)) ## [1] 32 Check out the folds to get a better understanding of what is going on. We generated a list that has 8 elements each of which holds a 4 element vector corresponding to indices for records in the mtcars data frame. folds ## $`1` ## [1] 22 25 4 23 20 26 8 12 ## ## $`2` ## [1] 18 24 15 9 21 14 3 1 ## ## $`3` ## [1] 29 16 11 2 27 28 6 19 ## ## $`4` ## [1] 13 17 31 32 7 10 5 30 Again, each list element has the indices of eight unique observations from the data frame. We have four folds with eight elements each for a total of 32 numbers corresponding to row numbers from the mtcars data frame. mtcars[folds[[1]],] ## mpg cyl disp hp drat wt qsec vs am gear carb ## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 4.1 Cross Fold Validation To implement the cross validation, we will create a processing loop that will execute once for each of the 8 folds. During each execution of the loop we will create a model using data combined from all folds except the fold corresponding to the current loop number (e.g, 1, 2, .. 4). Once the model is built we then test it on the fold number corresponding to the current loop number. So now we can create some lists to contain the models that we make along withe the associated predictions, errors and computed RMSE. We we can inspect any of the intermediate results after the fact to validate our work or look more closely at any specific result. # Next we setup some blank lists to stash results folddf &lt;- list() # Contains folds modl &lt;- list() # Hold each of the K models predl &lt;- list() # Hold rach of the K predictions rmse &lt;- list() # Hold the computed rmse for a given model # Now, for each of the 8 subgroups of holdout data we will # create a lda model based on all the data *except* the # holdout group for (ii in 1:length(folds)) { # This list holds the actual model we create for each of the # 10 folds modl[[ii]] &lt;- lm(formula = myform, data = mtcars[-folds[[ii]],] ) # This list will contain / hold the models build on the fold predl[[ii]] &lt;- predict(modl[[ii]], newdata=mtcars[folds[[ii]],], type=&quot;response&quot;) # This list will hold the results of the confusion matrix # function. This obkect will contain info on the # accuracy, sensitivity/recall, specificity # and so on for each model per fold errors &lt;- predl[[ii]]-mtcars[folds[[ii]],]$mpg rmse[[ii]] &lt;- sqrt(mean(errors^2)) } The above list structures allow us to drill down into any aspect of the models and predictions we have made for each of the 4 folds. More importantly we can see how well the model works against each of the individual holdout / test data sets. In the end, we just want to be able to look at the average RMSE across the folds. This gives us clues as to how good the model might perform against new data. rmse &lt;- unlist(rmse) lattice::dotplot(rmse, main=&quot;RMSE Across Folds Using K-Fold CV&quot;) mean(rmse) ## [1] 3.046228 sd(rmse) ## [1] 0.7071975 4.2 Create A Function To Automate Things Since we have gone to the trouble of creating a loop structure to process the folds, we could easily turn this into a function to automate the splitting of the data frame across some arbitrary number of folds just to get an idea of how the RMSE looks for different numbers of folds. We could even have our function accommodate different formula if we wanted but we won’t focus on that right now. You will soon discover that the caret package does these kinds of things for you but we aren’t quite there yet. make_mtcars_model &lt;- function(formula=myform, num_of_folds=8) { folds &lt;- split(sample(1:nrow(mtcars)),1:num_of_folds) modl &lt;- list() predl &lt;- list() rmse &lt;- list() # Now, for each of the 10 subgroups of holdout data we will # create a lda model based on all the data *except* the # holdout group for (ii in 1:length(folds)) { # This list holds the actual model we create for each of the folds modl[[ii]] &lt;- lm(formula = myform, data = mtcars[-folds[[ii]],] ) # This list will contain / hold the models build on the fold predl[[ii]] &lt;- predict(modl[[ii]], newdata=mtcars[folds[[ii]],], type=&quot;response&quot;) # Let&#39;s compute the RMSE and save it errors &lt;- predl[[ii]]-mtcars[folds[[ii]],]$mpg rmse[[ii]] &lt;- sqrt(mean(errors^2)) } return(rmse=unlist(rmse)) } Let’s look at the average RMSE across 8 folds. num_of_folds &lt;- 8 set.seed(321) rmse &lt;- make_mtcars_model(num_of_folds) title &lt;- paste(&quot;RMSE Across&quot;,num_of_folds, &quot;folds - &quot;,as.character(deparse(myform)),sep=&quot; &quot;) print(mean(rmse)) ## [1] 3.047164 lattice::dotplot(rmse, main=&quot;RMSE Across Folds Using K-Fold CV&quot;) sd(rmse) ## [1] 0.9967001 boxplot(rmse,main=title) 4.3 Repeated Cross Validation Since we already have an existing function we can up the ante by repeating the cross validation. This will provide more data on how the RMSE might be distributed across multiple runs, each of which does Cross Fold validation. This example will repeat a 4 Fold Cross Validation , 20 times. num_of_folds &lt;- 4 # Just to be clear - here is what happens when we call the function # once. We get back 4 RMSE values - one for each fold set.seed(321) (rmse &lt;- make_mtcars_model(num_of_folds)) ## [1] 4.565091 3.895989 2.514073 2.757429 2.287045 2.680096 1.663681 4.013908 # Now we repeat this some number of times - like 10. So we get back # 80 RMSE values repeated_cv_rmse &lt;- sapply(1:20,make_mtcars_model) boxplot(repeated_cv_rmse, main=&quot;RMSE Across 20 Repeats of 4 CV Folds&quot;) title &lt;- paste(&quot;RMSE Across&quot;,num_of_folds, &quot;folds - &quot;,as.character(deparse(myform)),sep=&quot; &quot;) mean(as.vector(repeated_cv_rmse)) ## [1] 2.993835 boxplot(repeated_cv_rmse) summary(repeated_cv_rmse) ## V1 V2 V3 V4 V5 ## Min. :1.508 Min. :1.560 Min. :1.633 Min. :1.981 Min. :1.836 ## 1st Qu.:2.027 1st Qu.:2.535 1st Qu.:2.146 1st Qu.:2.360 1st Qu.:2.269 ## Median :2.428 Median :2.736 Median :2.498 Median :2.691 Median :2.492 ## Mean :3.046 Mean :3.015 Mean :2.939 Mean :3.061 Mean :2.973 ## 3rd Qu.:4.581 3rd Qu.:3.342 3rd Qu.:4.087 3rd Qu.:3.538 3rd Qu.:3.844 ## Max. :4.902 Max. :4.975 Max. :4.544 Max. :5.045 Max. :4.624 ## V6 V7 V8 V9 V10 ## Min. :1.735 Min. :1.269 Min. :1.254 Min. :1.133 Min. :0.6545 ## 1st Qu.:2.310 1st Qu.:2.311 1st Qu.:1.746 1st Qu.:2.213 1st Qu.:2.8133 ## Median :2.637 Median :2.785 Median :3.085 Median :2.811 Median :3.3927 ## Mean :2.929 Mean :2.999 Mean :2.958 Mean :2.917 Mean :3.0968 ## 3rd Qu.:3.148 3rd Qu.:3.550 3rd Qu.:3.915 3rd Qu.:3.617 3rd Qu.:3.6623 ## Max. :5.316 Max. :5.208 Max. :4.631 Max. :4.764 Max. :4.6753 ## V11 V12 V13 V14 V15 ## Min. :1.254 Min. :1.445 Min. :1.198 Min. :0.8574 Min. :1.766 ## 1st Qu.:2.121 1st Qu.:2.198 1st Qu.:2.518 1st Qu.:2.2471 1st Qu.:2.298 ## Median :2.824 Median :2.437 Median :2.932 Median :2.7995 Median :2.528 ## Mean :2.958 Mean :2.987 Mean :3.033 Mean :2.9685 Mean :2.984 ## 3rd Qu.:4.022 3rd Qu.:3.316 3rd Qu.:4.020 3rd Qu.:4.0352 3rd Qu.:3.225 ## Max. :4.712 Max. :6.101 Max. :4.177 Max. :4.6596 Max. :5.642 ## V16 V17 V18 V19 V20 ## Min. :1.114 Min. :1.681 Min. :0.7252 Min. :1.862 Min. :1.593 ## 1st Qu.:2.122 1st Qu.:2.465 1st Qu.:2.7033 1st Qu.:2.158 1st Qu.:1.967 ## Median :2.714 Median :3.103 Median :2.9494 Median :2.798 Median :2.923 ## Mean :2.923 Mean :3.054 Mean :2.9983 Mean :3.054 Mean :2.984 ## 3rd Qu.:3.968 3rd Qu.:3.904 3rd Qu.:3.8184 3rd Qu.:3.983 3rd Qu.:3.749 ## Max. :4.641 Max. :3.969 Max. :4.2868 Max. :4.645 Max. :5.075 4.4 Bootstrap An alternative to K-Fold Cross Validation is to use the bootstrap sampling approach which will produce training data sets the same size as the original data set although some observations might be repeated as the sampling process is done with replacement. The observations that do not appear in each of the training sets are then used as a test set. These observations are known as “out of bag samples”. We’ll make a function to do bootstrap sampling. make_mtcars_boot &lt;- function(formula=myform, num_of_folds=8) { modl &lt;- list() predl &lt;- list() rmse &lt;- list() # Now, for each of the 10 subgroups of holdout data we will # create a lda model based on all the data *except* the # holdout group for (ii in 1:length(folds)) { training_boot_idx &lt;- sample(1:nrow(mtcars),replace=TRUE) test_boot_idx &lt;- !(1:32 %in% training_boot_idx) # This list holds the actual model we create for each of the folds modl[[ii]] &lt;- lm(formula = myform, data = mtcars[training_boot_idx,] ) # This list will contain / hold the models build on the fold predl[[ii]] &lt;- predict(modl[[ii]], newdata=mtcars[test_boot_idx,], type=&quot;response&quot;) # Let&#39;s compute the RMSE and save it errors &lt;- predl[[ii]]-mtcars[test_boot_idx,]$mpg rmse[[ii]] &lt;- sqrt(mean(errors^2)) } return(rmse=unlist(rmse)) } num_of_folds &lt;- 8 # Just to be clear - here is what happens when we call the function # once. We get back 8 RMSE values - one for each fold set.seed(321) (rmse &lt;- make_mtcars_boot(num_of_folds)) ## [1] 2.441587 3.733133 3.216098 2.544607 # Now we repeat this some number of times - like 10. So we get back # 80 RMSE values repeated_rmse &lt;- sapply(1:20,make_mtcars_boot) boxplot(repeated_rmse,main=&quot;RMSE Across 20 Repeats of 4 Boostrap Folds&quot;) title &lt;- paste(&quot;RMSE Across&quot;,num_of_folds, &quot;folds - &quot;,as.character(deparse(myform)),sep=&quot; &quot;) boot_repeated_rmse &lt;- as.vector(repeated_rmse) boxplot(boot_repeated_rmse) # How does the RMSE from the boostrap approach compare to the # K-Fold CV approach ? print(&quot;Summary of Bootstrap RMSE&quot;) ## [1] &quot;Summary of Bootstrap RMSE&quot; summary(boot_repeated_rmse) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.046 2.799 3.197 3.242 3.615 4.749 print(&quot;Summary of CV&quot;) ## [1] &quot;Summary of CV&quot; summary(repeated_cv_rmse) ## V1 V2 V3 V4 V5 ## Min. :1.508 Min. :1.560 Min. :1.633 Min. :1.981 Min. :1.836 ## 1st Qu.:2.027 1st Qu.:2.535 1st Qu.:2.146 1st Qu.:2.360 1st Qu.:2.269 ## Median :2.428 Median :2.736 Median :2.498 Median :2.691 Median :2.492 ## Mean :3.046 Mean :3.015 Mean :2.939 Mean :3.061 Mean :2.973 ## 3rd Qu.:4.581 3rd Qu.:3.342 3rd Qu.:4.087 3rd Qu.:3.538 3rd Qu.:3.844 ## Max. :4.902 Max. :4.975 Max. :4.544 Max. :5.045 Max. :4.624 ## V6 V7 V8 V9 V10 ## Min. :1.735 Min. :1.269 Min. :1.254 Min. :1.133 Min. :0.6545 ## 1st Qu.:2.310 1st Qu.:2.311 1st Qu.:1.746 1st Qu.:2.213 1st Qu.:2.8133 ## Median :2.637 Median :2.785 Median :3.085 Median :2.811 Median :3.3927 ## Mean :2.929 Mean :2.999 Mean :2.958 Mean :2.917 Mean :3.0968 ## 3rd Qu.:3.148 3rd Qu.:3.550 3rd Qu.:3.915 3rd Qu.:3.617 3rd Qu.:3.6623 ## Max. :5.316 Max. :5.208 Max. :4.631 Max. :4.764 Max. :4.6753 ## V11 V12 V13 V14 V15 ## Min. :1.254 Min. :1.445 Min. :1.198 Min. :0.8574 Min. :1.766 ## 1st Qu.:2.121 1st Qu.:2.198 1st Qu.:2.518 1st Qu.:2.2471 1st Qu.:2.298 ## Median :2.824 Median :2.437 Median :2.932 Median :2.7995 Median :2.528 ## Mean :2.958 Mean :2.987 Mean :3.033 Mean :2.9685 Mean :2.984 ## 3rd Qu.:4.022 3rd Qu.:3.316 3rd Qu.:4.020 3rd Qu.:4.0352 3rd Qu.:3.225 ## Max. :4.712 Max. :6.101 Max. :4.177 Max. :4.6596 Max. :5.642 ## V16 V17 V18 V19 V20 ## Min. :1.114 Min. :1.681 Min. :0.7252 Min. :1.862 Min. :1.593 ## 1st Qu.:2.122 1st Qu.:2.465 1st Qu.:2.7033 1st Qu.:2.158 1st Qu.:1.967 ## Median :2.714 Median :3.103 Median :2.9494 Median :2.798 Median :2.923 ## Mean :2.923 Mean :3.054 Mean :2.9983 Mean :3.054 Mean :2.984 ## 3rd Qu.:3.968 3rd Qu.:3.904 3rd Qu.:3.8184 3rd Qu.:3.983 3rd Qu.:3.749 ## Max. :4.641 Max. :3.969 Max. :4.2868 Max. :4.645 Max. :5.075 "],
["caret-package.html", "Chapter 5 Caret Package 5.1 Putting caret To Work 5.2 Back To The Beginning 5.3 Splitting 5.4 Calling The train() Function 5.5 Reproducible Results 5.6 One Size Fits All 5.7 Alternative Calling Sequence 5.8 Hyperparameters", " Chapter 5 Caret Package By now you are probably fatigued with understanding the details of writing the code to split data, doing Cross Validation, storing the results, and looking at descriptive stats associated with the resulting RMSE. And this is all before considering the various parameters associated with whatever method we wish to implement. Each function has its own set of requirements which may not extend to other functions. What we need (well, what we would like) is a framework to streamline this process and automate it as much as possible but not at the expense of understanding the results. The caret (Classification And Regression Training) package provides a uniform interface for calling different algorithms while simplifying the data splitting and RMSE calculation. It supports many different model types and also provides the ability to tune hyper parameters. Here are some of the features: Streamlined and consistent syntax to implement any of the 238 different methods using a single function Easy data splitting to simplify the creation of train / test pairs Realistic model estimates through built-in resampling Convenient feature importance determination Easy selection of different performance metrics (e.g. “ROC”,“Accuracy”, “Sensitivity”) Automated and semi-automated parameter tuning Simplified comparison of different models Note that caret provides a nice wrapper around the various modeling functions. Since each underlying model is itself a standalone R package and associated set of functions you can always call them directly should you prefer that approach. That’s what we have been doing in the earlier part of this text. 5.1 Putting caret To Work It’s easy to get lost in all that we have been doing so let’s review what the typical predictive modeling workflow will look like: Data Import (.csv., extraction from a database, etc) Some Data Visualization Data Prep (We haven’t done any of this yet) - Missing, imputation - Scaling - Create dummy variables / one hot encoding - Dimensionality Reduction Data Splitting (training / test) - Determine split ration - K-Fold Cross Validation (repeated) Modeling / Prediction Evaluation To do step 5 requires some predefined idea of a performance metric. We have been using RMSE and will continue to do so as we rework some of the previous examples using the caret package. 5.2 Back To The Beginning It is implied that in predictive modeling the ultimate goal is to generate a model that could be reasonably applied to new data. As we have learned, it is best to train any model on a data set that has been (re)sampled in some way (e.g. K Fold CV) which should help provide a more realistic estimate of “out of sample” error. In our earliest example we tried to predict the MPG from mtcars using a basic linear modeling function. The caret package provides a uniform way to do this which allows us to easily substitute in alternative functions without having to majorly change our code. We can call the train function in such a way as to pass in any arguments that are specific to a given method though in a way we could do for other methods. We can also tell the train function that we want to evaluate RMSE as a performance measure. That is, it will “know” that our primary performance measure for a model is RMSE. Before we do that, however, we’ll make a test / train pair. The caret package provides ways to do that. 5.3 Splitting createDataPartition can be used to create test and train data splits according to some proportion. There is a function called createFolds can be used to generate balanced cross–validation groupings from a set of data. createResample can be used to make simple bootstrap samples. For now, we’ll just stick with createDataPartition for creating a test/train pair. set.seed(123) # Make this example reproducible idx &lt;- createDataPartition(mtcars$mpg, p = .8, list = FALSE, times = 1) head(idx) ## Resample1 ## [1,] 1 ## [2,] 3 ## [3,] 4 ## [4,] 5 ## [5,] 6 ## [6,] 8 Train &lt;- mtcars[ idx,] Test &lt;- mtcars[-idx,] # nrow(Train) ## [1] 28 nrow(Test) ## [1] 4 5.4 Calling The train() Function To actually create a model involves use of the train function which is the premier function in the caret package. It does what it name suggests - train models. Note that we tell it: What we are trying to predict (a formula) What our data set is (e.g. Train) The desired method (“lm”) Note that this method name MUST match an existing R modeling function A desired scoring metric. In this case we seek to minimize RMSE on future predictions set.seed(123) # Make this example reproducible lm_fit &lt;- train(mpg~wt, data=Train, method=&quot;lm&quot;, metric=&quot;RMSE&quot;) We get back a single object that contains a lot of information that could help us figure out if the model is worth anything. But first, just type the name of fit object to see what you can see. This shows us information that has been derived from some re sampling activity across a number of bootstrapped samples. lm_fit ## Linear Regression ## ## 28 samples ## 1 predictor ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 28, 28, 28, 28, 28, 28, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 2.676533 0.79883 2.138882 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE summary(lm_fit) ## ## Call: ## lm(formula = .outcome ~ ., data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.890 -2.163 -0.091 1.361 7.140 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 36.2505 1.7925 20.223 &lt; 2e-16 *** ## wt -4.9957 0.5249 -9.516 5.89e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.75 on 26 degrees of freedom ## Multiple R-squared: 0.7769, Adjusted R-squared: 0.7684 ## F-statistic: 90.56 on 1 and 26 DF, p-value: 5.889e-10 Note that the summary of the model “summary(lm_fit)” returns the same information that would be returned had we used the lm function directly as we did in the previous section. The point is that the train function doesn’t not seek to replace or obscure the resulting model in any way. We can always get whatever information we need from it. So let’s apply this model to the test data frame Metrics::rmse(Test$mpg,predict(lm_fit,Test)) ## [1] 4.622936 There is more here than meets the eye. names(lm_fit) ## [1] &quot;method&quot; &quot;modelInfo&quot; &quot;modelType&quot; &quot;results&quot; &quot;pred&quot; ## [6] &quot;bestTune&quot; &quot;call&quot; &quot;dots&quot; &quot;metric&quot; &quot;control&quot; ## [11] &quot;finalModel&quot; &quot;preProcess&quot; &quot;trainingData&quot; &quot;resample&quot; &quot;resampledCM&quot; ## [16] &quot;perfNames&quot; &quot;maximize&quot; &quot;yLimits&quot; &quot;times&quot; &quot;levels&quot; ## [21] &quot;terms&quot; &quot;coefnames&quot; &quot;xlevels&quot; str(lm_fit$control,1) ## List of 28 ## $ method : chr &quot;boot&quot; ## $ number : num 25 ## $ repeats : logi NA ## $ search : chr &quot;grid&quot; ## $ p : num 0.75 ## $ initialWindow : NULL ## $ horizon : num 1 ## $ fixedWindow : logi TRUE ## $ skip : num 0 ## $ verboseIter : logi FALSE ## $ returnData : logi TRUE ## $ returnResamp : chr &quot;final&quot; ## $ savePredictions : chr &quot;none&quot; ## $ classProbs : logi FALSE ## $ summaryFunction :function (data, lev = NULL, model = NULL) ## $ selectionFunction: chr &quot;best&quot; ## $ preProcOptions :List of 6 ## $ sampling : NULL ## $ index :List of 25 ## $ indexOut :List of 25 ## $ indexFinal : NULL ## $ timingSamps : num 0 ## $ predictionBounds : logi [1:2] FALSE FALSE ## $ seeds :List of 26 ## $ adaptive :List of 4 ## $ trim : logi FALSE ## $ allowParallel : logi TRUE ## $ yLimits : num [1:2] 9.3 33.5 Check out the some of the model characteristics summary(lm_fit) ## ## Call: ## lm(formula = .outcome ~ ., data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.890 -2.163 -0.091 1.361 7.140 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 36.2505 1.7925 20.223 &lt; 2e-16 *** ## wt -4.9957 0.5249 -9.516 5.89e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.75 on 26 degrees of freedom ## Multiple R-squared: 0.7769, Adjusted R-squared: 0.7684 ## F-statistic: 90.56 on 1 and 26 DF, p-value: 5.889e-10 summary(lm_fit$finalModel)$r.squared ## [1] 0.7769458 We can go right to the final Model which contains the information for the lm_fit$finalModel ## ## Call: ## lm(formula = .outcome ~ ., data = dat) ## ## Coefficients: ## (Intercept) wt ## 36.250 -4.996 So it looks like caret did some re sampling for us by default. Actually, it was bootstrap sampling that we discussed earlier. However, we can specify cross fold validation if we wanted to. This requires a slightly more involved form of the train function. You can influence the train function by passing a “special” list / object to it via the trControl argument. This gets a bit confusing because the primary function to train models is called train and the command used to create the special is called trainControl and the argument in the train function is called trControl. With use, it becomes easier to remember the difference though at first it’s confusing. Here we train the model as before but specifically requesting a Cross Fold Validation method. We are requesting verbose output. control &lt;- trainControl(method = &quot;cv&quot;, # Cross Fold number = 5, # 5 Folds verboseIter = TRUE) # Verbose # Train the model set.seed(123) # Make this example reproducible my_lm &lt;- train( mpg ~ ., Train, method = &quot;lm&quot;, trControl = control ) ## + Fold1: intercept=TRUE ## - Fold1: intercept=TRUE ## + Fold2: intercept=TRUE ## - Fold2: intercept=TRUE ## + Fold3: intercept=TRUE ## - Fold3: intercept=TRUE ## + Fold4: intercept=TRUE ## - Fold4: intercept=TRUE ## + Fold5: intercept=TRUE ## - Fold5: intercept=TRUE ## Aggregating results ## Fitting final model on full training set So the object returned from caret gives us an estimate of how well the model will perform (based on RMSE) for out of sample data. my_lm ## Linear Regression ## ## 28 samples ## 10 predictors ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 22, 22, 23, 22, 23 ## Resampling results: ## ## RMSE Rsquared MAE ## 2.955532 0.7487594 2.528231 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE Metrics::rmse(Test$mpg,predict(my_lm,Test)) ## [1] 4.808981 We could also repeat the 5 times CV validation an arbitrary number of times to generate greater confidence in the RMSE estimates returned by the model. Remember, a major reason for using K Fold validation is to better estimate the out of sample error by holding out a portion of the data frame being trained upon. control &lt;- trainControl(method = &quot;repeatedcv&quot;, # Repeated Cross Fold number = 3, # 3 Folds repeats = 3, # Repeats verboseIter = FALSE) # Verbose set.seed(123) # Make this example reproducible my_lm &lt;- train( mpg ~ ., Train, method = &quot;lm&quot;, trControl = control ) Metrics::rmse(Test$mpg,predict(my_lm,Test)) ## [1] 4.808981 5.5 Reproducible Results So this is where using a package like caret really pays off in that not only does it provide a nice, consistent front end for a large number of methods, it gives you back an object containing all of the results plus all thing used to provide the final estimate on out of band error. While digging in deeper into the object can be scary, it does pay off to spend time looking at it. Think of it this way, if you don’t use something like caret then you will have to write your own loops to keep track of, for example, the RMSE resulting from each train/test pair in a cross fold validation. This of course is fine but caret makes it easier. Let’s study the previous example in greater detail. To get the best result (e.g. RMSE, R-Squared, MAE) directly: my_lm$result ## intercept RMSE Rsquared MAE RMSESD RsquaredSD MAESD ## 1 TRUE 4.574962 0.6092634 3.584692 2.811159 0.2692945 1.881702 To see the results coming from each train / test pair using to build the final Out of Sample Error estimate. Why are there 9 rows here ? my_lm$resample ## RMSE Rsquared MAE Resample ## 1 3.217212 0.68744503 2.744463 Fold1.Rep1 ## 2 5.348479 0.27930338 3.923923 Fold2.Rep1 ## 3 5.670200 0.48061619 4.860719 Fold3.Rep1 ## 4 2.346548 0.83748766 1.989082 Fold1.Rep2 ## 5 3.220792 0.76252308 2.511865 Fold2.Rep2 ## 6 4.150818 0.74640561 3.858077 Fold3.Rep2 ## 7 2.386493 0.86674237 1.915139 Fold1.Rep3 ## 8 3.438640 0.73519906 2.592498 Fold2.Rep3 ## 9 11.395476 0.08764857 7.866464 Fold3.Rep3 Given that this is the underlying information that was used to build the ultimate out of sample error estimate, we should be able to take the average of the RMSE column which should match what we get in the final result. # Access the RMSE column my_lm$resample[,&#39;RMSE&#39;] ## [1] 3.217212 5.348479 5.670200 2.346548 3.220792 4.150818 2.386493 3.438640 ## [9] 11.395476 # What is the average my_lm$resample[,&#39;RMSE&#39;] %&gt;% mean ## [1] 4.574962 # Does it match what we get in the final result ? my_lm$results[,&#39;RMSE&#39;] ## [1] 4.574962 The object is even more complete in that it provides you with access to the underlying sampled data used to generate the intermediate train / test and final results. The following will give us the row numbers of the training data used per iteration. my_lm$control$index ## $Fold1.Rep1 ## [1] 1 3 7 8 9 10 11 12 13 14 16 17 18 20 21 22 24 25 28 ## ## $Fold2.Rep1 ## [1] 2 4 5 6 7 8 10 13 14 15 19 22 23 24 25 26 27 28 ## ## $Fold3.Rep1 ## [1] 1 2 3 4 5 6 9 11 12 15 16 17 18 19 20 21 23 26 27 ## ## $Fold1.Rep2 ## [1] 1 2 4 6 7 8 9 10 14 15 16 17 18 19 20 22 24 27 ## ## $Fold2.Rep2 ## [1] 1 3 5 6 7 9 10 11 12 13 15 17 20 21 22 23 25 26 28 ## ## $Fold3.Rep2 ## [1] 2 3 4 5 8 11 12 13 14 16 18 19 21 23 24 25 26 27 28 ## ## $Fold1.Rep3 ## [1] 1 4 6 7 8 11 13 15 16 19 20 21 22 24 25 26 27 28 ## ## $Fold2.Rep3 ## [1] 2 3 5 6 9 10 11 12 14 17 18 19 20 21 23 25 26 27 28 ## ## $Fold3.Rep3 ## [1] 1 2 3 4 5 7 8 9 10 12 13 14 15 16 17 18 22 23 24 The following will give us the row numbers of the corresponding test data used per iteration. The general idea here is that you could recreate the final result by writing your own code if you were asked to do so. my_lm$control$indexOut ## $Resample1 ## [1] 2 4 5 6 15 19 23 26 27 ## ## $Resample2 ## [1] 1 3 9 11 12 16 17 18 20 21 ## ## $Resample3 ## [1] 7 8 10 13 14 22 24 25 28 ## ## $Resample4 ## [1] 3 5 11 12 13 21 23 25 26 28 ## ## $Resample5 ## [1] 2 4 8 14 16 18 19 24 27 ## ## $Resample6 ## [1] 1 6 7 9 10 15 17 20 22 ## ## $Resample7 ## [1] 2 3 5 9 10 12 14 17 18 23 ## ## $Resample8 ## [1] 1 4 7 8 13 15 16 22 24 ## ## $Resample9 ## [1] 6 11 19 20 21 25 26 27 28 5.6 One Size Fits All So this is where things get interesting. If we wanted to use another method such as Random Forests, we do NOT have to change much at all. We just provide the name of the desired method which in this case, is ranger which is a function to create Random Forests. control &lt;- trainControl(method = &quot;cv&quot;, number = 5) set.seed(123) # Make this example reproducible my_ranger &lt;- train( mpg ~ ., Train, method = &quot;ranger&quot;, trControl = control ) my_ranger ## Random Forest ## ## 28 samples ## 10 predictors ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 22, 22, 23, 22, 23 ## Resampling results across tuning parameters: ## ## mtry splitrule RMSE Rsquared MAE ## 2 variance 2.587753 0.9017899 2.180380 ## 2 extratrees 2.694316 0.8805852 2.297226 ## 6 variance 2.490290 0.9091406 2.147651 ## 6 extratrees 2.608913 0.8938029 2.240797 ## 10 variance 2.535569 0.9107767 2.186020 ## 10 extratrees 2.523521 0.8992038 2.170427 ## ## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 5 ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were mtry = 6, splitrule = variance ## and min.node.size = 5. Metrics::rmse(Test$mpg,predict(my_ranger,Test)) ## [1] 2.288412 5.7 Alternative Calling Sequence It is native to R to want to use a formula when specifying the goal of the modeling process. We’ve been using something along the lines of “mpg ~ .” because R uses this format for many different statistical functions. However, it is also possible to specify the predictor variables as X and the predicted variable as Y. In the above example, this would look like: my_ctrl &lt;- trainControl( method = &quot;cv&quot;, number = 3, verboseIter = FALSE ) my_ranger &lt;- train( x = Train[,-1], # Everything BUT the MPG column y = Train[,1], # The MPG column method = &quot;ranger&quot;, tuneLength = 6, trControl = my_ctrl ) my_ranger ## Random Forest ## ## 28 samples ## 10 predictors ## ## No pre-processing ## Resampling: Cross-Validated (3 fold) ## Summary of sample sizes: 19, 18, 19 ## Resampling results across tuning parameters: ## ## mtry splitrule RMSE Rsquared MAE ## 2 variance 2.890146 0.8056424 2.266813 ## 2 extratrees 2.926584 0.7983939 2.326078 ## 3 variance 2.843515 0.8124454 2.219626 ## 3 extratrees 2.884787 0.8097881 2.310120 ## 5 variance 2.830262 0.8086326 2.217911 ## 5 extratrees 2.887976 0.8122192 2.317901 ## 6 variance 2.819049 0.8125409 2.190555 ## 6 extratrees 2.972158 0.7987946 2.378725 ## 8 variance 2.822495 0.8085079 2.214570 ## 8 extratrees 2.944229 0.7993095 2.365574 ## 10 variance 2.834211 0.8025443 2.247054 ## 10 extratrees 2.951646 0.8008366 2.357687 ## ## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 5 ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were mtry = 6, splitrule = variance ## and min.node.size = 5. Metrics::rmse(Test$mpg,predict(my_ranger,Test)) ## [1] 2.338702 5.8 Hyperparameters This model returns more information than say the lm function because this method uses something called “hyperparameters” which are arguments to a given method that gets set before you call the method. In this case there are two hyperparameters called mtry and splitrule that assume default variables if we don’t supply values. We can get a plot of how the RMSE and R squared value varied with different values of mtry as well the splitrule. Here we see that an mtry value of 6 randomly selected columns / variables provides the lowest RMSE. ggplot(my_ranger) + theme(legend.position = &quot;top&quot;) We can use the tuneLength argument to tell the train function to use N different values of mtry which is a hyperparameter to the randomForest package. The value relates to the number of columns in the data frame. We have 11 total and we are trying to predict one of them (mpg). So we can tell the train function to randomly select N variables (up to 10) when a tree is split. my_ctrl &lt;- trainControl( method = &quot;cv&quot;, number = 5, verboseIter = TRUE ) set.seed(123) # Make this example reproducible my_rf &lt;- train( mpg ~ ., Train, method = &quot;rf&quot;, tuneLength = 9, # We&#39;ll use 9 different values trControl = my_ctrl ) ## + Fold1: mtry= 2 ## - Fold1: mtry= 2 ## + Fold1: mtry= 3 ## - Fold1: mtry= 3 ## + Fold1: mtry= 4 ## - Fold1: mtry= 4 ## + Fold1: mtry= 5 ## - Fold1: mtry= 5 ## + Fold1: mtry= 6 ## - Fold1: mtry= 6 ## + Fold1: mtry= 7 ## - Fold1: mtry= 7 ## + Fold1: mtry= 8 ## - Fold1: mtry= 8 ## + Fold1: mtry= 9 ## - Fold1: mtry= 9 ## + Fold1: mtry=10 ## - Fold1: mtry=10 ## + Fold2: mtry= 2 ## - Fold2: mtry= 2 ## + Fold2: mtry= 3 ## - Fold2: mtry= 3 ## + Fold2: mtry= 4 ## - Fold2: mtry= 4 ## + Fold2: mtry= 5 ## - Fold2: mtry= 5 ## + Fold2: mtry= 6 ## - Fold2: mtry= 6 ## + Fold2: mtry= 7 ## - Fold2: mtry= 7 ## + Fold2: mtry= 8 ## - Fold2: mtry= 8 ## + Fold2: mtry= 9 ## - Fold2: mtry= 9 ## + Fold2: mtry=10 ## - Fold2: mtry=10 ## + Fold3: mtry= 2 ## - Fold3: mtry= 2 ## + Fold3: mtry= 3 ## - Fold3: mtry= 3 ## + Fold3: mtry= 4 ## - Fold3: mtry= 4 ## + Fold3: mtry= 5 ## - Fold3: mtry= 5 ## + Fold3: mtry= 6 ## - Fold3: mtry= 6 ## + Fold3: mtry= 7 ## - Fold3: mtry= 7 ## + Fold3: mtry= 8 ## - Fold3: mtry= 8 ## + Fold3: mtry= 9 ## - Fold3: mtry= 9 ## + Fold3: mtry=10 ## - Fold3: mtry=10 ## + Fold4: mtry= 2 ## - Fold4: mtry= 2 ## + Fold4: mtry= 3 ## - Fold4: mtry= 3 ## + Fold4: mtry= 4 ## - Fold4: mtry= 4 ## + Fold4: mtry= 5 ## - Fold4: mtry= 5 ## + Fold4: mtry= 6 ## - Fold4: mtry= 6 ## + Fold4: mtry= 7 ## - Fold4: mtry= 7 ## + Fold4: mtry= 8 ## - Fold4: mtry= 8 ## + Fold4: mtry= 9 ## - Fold4: mtry= 9 ## + Fold4: mtry=10 ## - Fold4: mtry=10 ## + Fold5: mtry= 2 ## - Fold5: mtry= 2 ## + Fold5: mtry= 3 ## - Fold5: mtry= 3 ## + Fold5: mtry= 4 ## - Fold5: mtry= 4 ## + Fold5: mtry= 5 ## - Fold5: mtry= 5 ## + Fold5: mtry= 6 ## - Fold5: mtry= 6 ## + Fold5: mtry= 7 ## - Fold5: mtry= 7 ## + Fold5: mtry= 8 ## - Fold5: mtry= 8 ## + Fold5: mtry= 9 ## - Fold5: mtry= 9 ## + Fold5: mtry=10 ## - Fold5: mtry=10 ## Aggregating results ## Selecting tuning parameters ## Fitting mtry = 6 on full training set my_rf ## Random Forest ## ## 28 samples ## 10 predictors ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 22, 22, 23, 22, 23 ## Resampling results across tuning parameters: ## ## mtry RMSE Rsquared MAE ## 2 2.579696 0.8997291 2.181764 ## 3 2.543545 0.9002549 2.181950 ## 4 2.538643 0.9059724 2.193849 ## 5 2.467108 0.9142192 2.129060 ## 6 2.457545 0.9203487 2.130067 ## 7 2.510544 0.9103018 2.168747 ## 8 2.541710 0.9096472 2.210990 ## 9 2.539938 0.9136301 2.198482 ## 10 2.592113 0.9070706 2.214741 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was mtry = 6. plot(my_rf) Metrics::rmse(Test$mpg,predict(my_rf,Test)) ## [1] 2.23303 If you have a questions about what hyper parameters can be tuned for a given method then you can refer to the online caret documentation Here is a screenshot of the table of supported models and associated tuning parameters. Another way to do this within the caret package itself is that if you already know the abbreviation for the specific method you wish to use (e.g. “rf”) then you can use some built in functions to help you. Remember that caret does not replace or rewrite functions, it merely provides a nice wrapper around them. Since each underlying model is it a standalone R package and associated set of functions you can always call them directly. modelLookup(&quot;rf&quot;) ## model parameter label forReg forClass probModel ## 1 rf mtry #Randomly Selected Predictors TRUE TRUE TRUE Here we get the hyper parameters for the ranger function. We see that it has three hyper parameters that could be varied in some way to influence a final model. modelLookup(&quot;ranger&quot;) ## model parameter label forReg forClass probModel ## 1 ranger mtry #Randomly Selected Predictors TRUE TRUE TRUE ## 2 ranger splitrule Splitting Rule TRUE TRUE TRUE ## 3 ranger min.node.size Minimal Node Size TRUE TRUE TRUE If you just want a list of all the models supported by caret then do something like this: models &lt;- modelLookup()[,1:3] nrow(models) ## [1] 502 # Search for a Model models[models$model==&quot;rf&quot;,] ## model parameter label ## 365 rf mtry #Randomly Selected Predictors models[models$model==&quot;ranger&quot;,] ## model parameter label ## 351 ranger mtry #Randomly Selected Predictors ## 352 ranger splitrule Splitting Rule ## 353 ranger min.node.size Minimal Node Size So in the case of the ranger function there are actually three hyper parameters that could be tuned. my_ctrl &lt;- trainControl( method = &quot;cv&quot;, number = 3, verboseIter = FALSE ) my_ranger &lt;- train( mpg ~ ., Train, method = &quot;ranger&quot;, tuneLength = 6, trControl = my_ctrl ) my_ranger ## Random Forest ## ## 28 samples ## 10 predictors ## ## No pre-processing ## Resampling: Cross-Validated (3 fold) ## Summary of sample sizes: 18, 19, 19 ## Resampling results across tuning parameters: ## ## mtry splitrule RMSE Rsquared MAE ## 2 variance 2.761408 0.8672659 2.374636 ## 2 extratrees 2.719210 0.8528162 2.323624 ## 3 variance 2.745672 0.8706840 2.383784 ## 3 extratrees 2.744949 0.8486200 2.367447 ## 5 variance 2.834880 0.8593408 2.431688 ## 5 extratrees 2.779045 0.8534449 2.397243 ## 6 variance 2.827772 0.8627602 2.415079 ## 6 extratrees 2.761680 0.8580494 2.404353 ## 8 variance 2.858153 0.8550033 2.461909 ## 8 extratrees 2.776434 0.8535960 2.438577 ## 10 variance 2.853339 0.8602301 2.480190 ## 10 extratrees 2.822600 0.8541223 2.444921 ## ## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 5 ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were mtry = 2, splitrule = extratrees ## and min.node.size = 5. Metrics::rmse(Test$mpg,predict(my_ranger,Test)) ## [1] 3.042224 plot(my_ranger) "],
["classification-problems.html", "Chapter 6 Classification Problems 6.1 Performance Measures 6.2 Important Terminology 6.3 A Basic Model 6.4 Selecting The Correct Threshold / Alpha 6.5 Hypothesis Testing 6.6 Confusion Matrix 6.7 Picking the Right Metric 6.8 Wait. Where Are We ? 6.9 Better Ways To Compute The ROC Curve 6.10 ROC Curve Summary", " Chapter 6 Classification Problems Next up we consider the issue of building a model to predict a binary (e.g. “yes” / “no” or “positive /”negative“) outcome although we might also predict more than one class. For the sake of explanation we’ll keep our attention to the”two class&quot; situation. 6.1 Performance Measures With Linear Regression we were predicting a continuous outcome with the goal of being able to minimize the RMSE (root mean square error). In classification problems we need a metric or “performance measure” that we can use to judge the effectiveness of any model we create. Typically in classification we would be predicting some binary outcome such as whether someone has a disease or not. In this case it would not make sense to use something like RMSE. Other measures such as Accuracy, Precision, or Sensitivity are more appropriate. An example might help - we’ll be spending some time with the PimaIndiansDiabetes dataframe that is part of the mlbench package. This is part of the mlbench package or you can read it in straight from the internet. # Option install install.packages(&quot;mlbench&quot;) Once you have it installed then load it into the work space as follows: data(&quot;PimaIndiansDiabetes&quot;) # Get a shorter handle. I hate typing. pm &lt;- PimaIndiansDiabetes Or just read it in. url &lt;- &quot;https://raw.githubusercontent.com/steviep42/bios534_spring_2020/master/data/pima.csv&quot; pm &lt;- read.csv(url) The description of the data set is as follows: So we now have some data on which we can build a model. Specifically, there is a variable in the data called “diabetes” which indicates the disease / diabetes status (“pos” or “neg”) of the person. It would be good to come up with a model that we could use with incoming data to determine if someone has diabetes. 6.2 Important Terminology In predictive modeling there are some common terms to consider: 6.3 A Basic Model Since we are attempting to predict a binary outcome here (“pos” or “neg”) we’ll need to use something other than linear regression which is used to predict numeric outcomes. We’ll go with Logistic Regression as it is a tried and true method for doing this type of thing. Let’s use the native glm function to do this since it will motivate some important concepts. We’ll split the data into a train / test pair using the createDataPartition function from caret. We’ll go with an 80% / 20% split. You’ve seen this before with the linear modelling examples. set.seed(891) idx &lt;- createDataPartition(pm$diabetes, p=.80, list=FALSE) glm_train &lt;- pm[idx,] glm_test &lt;- pm[-idx,] glm_model &lt;- glm(diabetes ~ ., data = glm_train, family = &quot;binomial&quot;) # Next well make some predictions using the test data glm_preds &lt;- predict(glm_model, glm_test, type=&quot;response&quot;) glm_preds[1:10] ## 2 6 8 22 23 30 33 ## 0.05836318 0.15559574 0.67209318 0.31189099 0.93064630 0.27846273 0.05726965 ## 36 38 40 ## 0.14730284 0.39319711 0.53563224 What do we get back from our prediction ? These are probabilities that, for each row in the test data frame, represent the likelihood of that person being positive for diabetes. The trick then is to figure out the threshold value (aka “alpha value”) over which we would classify the person as being positive for diabetes. To answer this question, we need to back up a bit and recall that we are dealing with a curve like the one below which is a sigmoid function. The idea is to take our probabilities, which range between 0 and 1, and then pick a threshold over which we would classify that person as being positive for diabetes. 6.4 Selecting The Correct Threshold / Alpha The temptation is to select 0.5 as the threshold such that if a returned probability exceeds 0.5 then we classify the associated subject as being “positive” for the disease. But then this assumes that the probabilities are distributed perfectly. This is frequently NOT the case though it doesn’t stop people from using 0.5. Here is another view of the situation. The above represents a perfect classifier wherein we can cleanly distinguish between True Positives and Negatives. Note that, the cutoff point is at 0.5 which represents an ideal case. However, in most situations, what we have is something like this: 6.4.1 Moving The Threshold What happens if we move our threshold towards 0 ? We would definitely get more of the actual positive cases. What if we moved it to say 0.1 ? We would probably get ALL of the True Positives at the expense of getting a lot of False Positives. What happens if we move our threshold towards 1 ? We would definitely get more of the actual negative cases. What if we moved it to say 0.9 ? We would probably get ALL of the True Negatives at the expense of getting a lot of False Negatives. 6.4.2 Distribution of Predicted Probabilities We might first wish to look at the distribution of the returned probabilities before making a decision about where to set the threshold. You should now be able to clearly that simply selecting 0.5 in a general case might not be the best approach. par(mfrow=c(1,2)) boxplot(glm_preds, main=&quot;GLM Model Probabilities&quot;) grid() plot(glm_preds[order(glm_preds)],type=&quot;l&quot;, main=&quot;Prediction Probabilities&quot;,ylab=&quot;probability&quot;,xlab=&quot;Patients&quot;) grid() par(mfrow=c(1,1)) The median is somewhere around .25 so we could use that for now although we are just guessing. glm_label_preds &lt;- ifelse(glm_preds &gt; 0.25,&quot;pos&quot;,&quot;neg&quot;) # We have to make the labels into a factor since # the diabetes column is a factor in the original data dset glm_label_preds &lt;- factor(glm_label_preds, levels = levels(glm_test[[&quot;diabetes&quot;]])) glm_label_preds[1:10] ## 2 6 8 22 23 30 33 36 38 40 ## neg neg pos pos pos pos neg neg pos pos ## Levels: neg pos 6.5 Hypothesis Testing Now, before we dig into the details our classifier, remember that most things in statistics and classification revolves around the idea of a hypothesis. In this case, the “null” hypothesis is that a patient does NOT have the disease whereas the alternative hypothesis is that they do. Well, for a statistician that’s a bit strong. Let’s just say that if there is enough evidence to reject the null hypothesis then we will. Anyway, the larger idea is that we might apply our test to someone and subsequently determine, by mistake, that they have a disease when in fact they don’t. - This would be an example of a &quot;false positive&quot; also known as a &quot;Type I Error&quot;. It is also possible that we apply the test to someone and we say that the do not have the disease when they actually do. - This is known as a &quot;false negative&quot; also known as a Type II Error&quot; Here we fail to reject the null hypothesis for this person. A perfect test would have zero false positives and zero false negatives 6.6 Confusion Matrix So now we have our predictions in terms of actual labels that we could then use to compare to the actual labels that are stored in the “diabetes” column of the test data frame. This table provides the basis for computing a number of performance measures such as accuracy, precision, sensitivity, specificity and others. In predictive modeling we are always interested in how well any given model will perform on “new” data. # How does this compare to the truth ? my_confusion &lt;- table(predicted = glm_label_preds, actual = glm_test$diabetes) my_confusion ## actual ## predicted neg pos ## neg 59 6 ## pos 41 47 Let’s break this down since it is really important to know how to use this construct. First, we notice that there are N = 153 people in this study. True Positives - With respect to the second row - we predicted that 47 people have the disease that actually do have it. You could then say that the number of TRUE POSITIVES (abbreviated as “TP”) is 47. False Positives - We also predicted that 41 people have the condition when they in fact do not. We could then say that the number of FALSE POSITIVES, abbreviated as “FP”, is 41. False Negatives - In the first row we predicted that 6 people do NOT have the disease/condition when they actually do. So you could say that the number of FALSE NEGATIVES (abbreviated as FN) is 6. True Negatives - We also predicted that 59 people do not have the condition and they do not. So then the number of TRUE NEGATIVES (abbreviated as TN) is also 59. 6.6.1 Computing Performance Metrics Now comes the fun part in that you might be concerned with specific metrics to assess the quality of your model in specific terms. Since our model, such as it is, seems to relate to the quality of a medical diagnostic we might be concerned with its accuracy, precision, and sensitivity. The first two terms in particular are frequently used synonymously when they are not the same thing.Below is a graphic from Wikipedia which presents many (if not all) of the metrics that can be computed against a confusion matrix. We’ll focus on some specific metrics as they will assist our understanding of how to assess a model. my_confusion ## actual ## predicted neg pos ## neg 59 6 ## pos 41 47 sum(my_confusion) ## [1] 153 6.6.1.1 Accuracy So let’s take the number of observed True Positives and True Negatives, add them together, and divide them by the total number of patients in the study group to arrive at what is known as the Accuracy of our model. Another way to think of the denominator is as the sum of all observed results, True and False. Accuracy = (TP + TN) / (TP + TN + FP + FN) = (59 + 47)/153 = 0.69 accuracy &lt;- (my_confusion[1,1] + my_confusion[2,2]) / sum(my_confusion) (accuracy %&gt;% round(.,2)) ## [1] 0.69 6.6.1.2 Precision How precise is the model ? This is also known as Positive Predictive Value. We take the number of True Posties (TP) and divide that by the sum of True Positives (TP) and False Positives (FP). The denominator is the sum of row 2 in our matrix. Precision = TP / (TP + FP) = 47 / (47 + 41) = 0.53 precision &lt;- my_confusion[2,2]/(my_confusion[2,1]+my_confusion[2,2]) precision %&gt;% round(.,2) ## [1] 0.53 It is helpful to know that Precision is also known as the PPV “Positive Predictive Value” since it is concerned with the ratio of True Positives over the sum of all Positive related quantities including the False Positives. The larger the number of FP then the smaller the ratio which results in a lower precision. 6.6.1.3 Sensitivity Sensitivity is related to Precision except the ratio we look at is the number of True Positives (TP) divided by the sum of True Positives and False Negatives (which are actually Positives). This tells us how frequently we find a positive case given that it is actually positive. Sensitivity = TP / (TP + FN) = 47 / (47 + 6) = 0.89 sensitivity &lt;- my_confusion[2,2]/(my_confusion[2,2]+my_confusion[1,2]) sensitivity %&gt;% round(.,2) ## [1] 0.89 Sensitivity also has synonyms: recall, hit rate, or True Positive Rate (TPR). For example, the concept of True Positive Rate might be more intuitive for you to understand although scientific medical literature might reference Sensitivity. 6.6.1.4 Specificity Specificity tells us how frequently we find a negative case given that it is actually negative. This is also known as the “True Negative Rate” Specificity = TN / (TN + FP) = 59 / (59 + 41) = 0.59 specificity &lt;- my_confusion[1,1] / (my_confusion[1,1]+ my_confusion[2,1]) (specificity %&gt;% round(.,2)) ## [1] 0.59 6.6.1.5 False Positive Rate We compute the FPR as follows: False Positive Rate = FP / (FP + TN) = 41 / (41 + 59) = .41 fpr &lt;- my_confusion[2,1] / (my_confusion[2,1] + my_confusion[1,1]) (fpr %&gt;% round(.,2)) ## [1] 0.41 6.7 Picking the Right Metric There are more ratios we could compute some of which might be more relevant to our classification issue. In reality, picking the “right” metric is a function of your domain of study. Frequently, the sensitivity and specificity are used in medical testing scenarios as is the false positive rate. But you should search the literature in your area of interest to determine what is commonly used. We could say much more about these metrics but we’ll keep it simple for now. For now, we’ll use both the True Postie Rate and the False Positive Rate. 6.8 Wait. Where Are We ? We’ve been doing a lot. We did the following: Built a model against the training data Used the model to make a prediction against the test data Took the probabilities from Step #2 and Selected a threshold / alpha value (e.g. .3) and Decided that probabilities over that threshold would be “pos” Created a table of outcomes (confusion matrix) to compare predictions vs reality Computed some important ratios While this process was useful the resulting confusion matrix corresponded to just ONE particular threshold ? What if we had picked another value ? We would then get a different confusion matrix as well as different performance measures. In effect we would have to repeat steps 1-6 all over again !!! Let’s find a way to generalize these steps. First, let’s create a function that allows us to compute the True Positive Rate (aka “Sensitivity”) and the False Positive Rate ( 1 - Specificity). If we apply it to our predictions from our example in progress, the output would be as follows. get_tprfpr &lt;- function(pred,true) { myt &lt;- table(pred,true) tpr &lt;- myt[2,2]/(myt[2,2]+myt[1,2]) fpr &lt;- myt[2,1] / (myt[2,1] + myt[1,1]) return(c(tpr=tpr,fpr=fpr)) } get_tprfpr(glm_label_preds,glm_test$diabetes) ## tpr fpr ## 0.8867925 0.4100000 We could now use this function to compute these metrics for any set of predictions vs actual outcomes. We could generalize this function to accept an alpha so we could explore the full probability domain (0 - 1) and then plot the TPR vs FPR. This is, in effect, creating something known as a ROC Curve aka Receiver Operating Characteristic Curve. get_tprfpr &lt;- function(thresh=.25,probs=glm_preds) { diabetes &lt;- ifelse(probs &gt; thresh,&quot;pos&quot;,&quot;neg&quot;) myt &lt;- table(diabetes,glm_test$diabetes) tpr &lt;- myt[2,2]/(myt[2,2]+myt[1,2]) fpr &lt;- myt[2,1] / (myt[2,1] + myt[1,1]) return(c(tpr=tpr,fpr=fpr,alpha=thresh)) } Let’s look at a sequence of alpha values: metrics &lt;- t(sapply(seq(0.01,.95,.09),function(x) get_tprfpr(x))) plot(tpr~fpr,metrics, ylim=c(0,1),xlim=c(0,1), main=&quot;Steve&#39;s Super Cool ROC Curve&quot;, xlab=&quot;False Positve Rate (1-Specificity)&quot;, ylab=&quot;True Positive Rate&quot;,type=&quot;l&quot;) grid() abline(a=0, b=1,lty=2) # Put the associated threshold values on the plot to help you identify # the right value to maximize the AUC (Area Under Curve) text(metrics[,2],metrics[,1],labels=metrics[,3],cex=0.8) It turns out that area under an ROC curve is a measure of the usefulness of a test in general, where a greater area means a more useful test. Ideally we would want the area under the curve (also known as “AUC”) to be as close to 1 as possible. The dashed line above represents a classifier that basically “guesses” the outcome (pos vs neg) using a “coin flip” mentality. Here are some more examples of curves including one that is “perfect”. B and C are okay where as D represents mere guessing. Here is another view of determining how good a ROC curve is: So, our classifier does better than that but certainly not perfectly. Now, we also care about the threshold that gives us a good balance between the TPR and FPR. I mean if we wanted a max AUC with no other concerns, we would also be accepting a very high FPR. So this is why looking at the curve is useful. 6.9 Better Ways To Compute The ROC Curve So by now your head might be reeling from all the details and tedium associated with selecting alpha values, computing matrices, and plotting ROC curves though I it should be no surprise that R (as well as Python) has a number of functions that can compute these things for you. As an example, if we wanted to plot the ROC curve we generated by hand we could use the ROCR package. It takes the probabilities returned by our first prediction object as well as the known labels in the glm_test data frame. library(ROCR) pred &lt;- ROCR::prediction(predictions = glm_preds, labels = glm_test$diabetes) perf &lt;- ROCR::performance(pred, &quot;tpr&quot;, &quot;fpr&quot;) ROCR::plot(perf,colorize=T, print.cutoffs.at=seq(0,1,by=0.1), lwd=3,las=1,main=&quot;A Pretty ROC Curve&quot;) abline(a = 0, b = 1) grid() # Get the optimal AUC auc_ROCR &lt;- ROCR::performance(pred,measure=&quot;auc&quot;) auc_ROCR &lt;- auc_ROCR@y.values[[1]] cat(&quot;Optimal AUC is: &quot;,auc_ROCR,&quot;\\n&quot;) ## Optimal AUC is: 0.8677358 And if we wanted to see the auc associated with the “optimal” alpha we could use some functions to get that for us: pm_model_glm_probs &lt;- predict(glm_model,glm_test,type=&quot;response&quot;) myRoc &lt;- pROC::roc(diabetes~pm_model_glm_probs, auc=TRUE,data=glm_test) pROC::coords(myRoc, &quot;best&quot;, ret = &quot;threshold&quot;,transpose = TRUE) ## threshold ## 0.3850002 get_tprfpr(.3850002) ## tpr fpr alpha ## 0.7735849 0.1600000 0.3850002 And while I’m at it, I might as well show you how easy it is to compute a confusion matrix corresponding to the ideal threshold. glm_label_preds &lt;- ifelse(glm_preds &gt; 0.385,&quot;pos&quot;,&quot;neg&quot;) # We have to make the labels into a factor since # the diabetes column is a factor in the original data dset glm_label_preds &lt;- factor(glm_label_preds, levels = levels(glm_test[[&quot;diabetes&quot;]])) # How does this compare to the truth ? my_confusion &lt;- table(predicted = glm_label_preds, actual = glm_test$diabetes) We could use a function from the caret package called confusionMatrix to show us the relevant metrics. Much better than doing it by hand. caret::confusionMatrix(my_confusion,positive=&quot;pos&quot;) ## Confusion Matrix and Statistics ## ## actual ## predicted neg pos ## neg 84 12 ## pos 16 41 ## ## Accuracy : 0.817 ## 95% CI : (0.7465, 0.8748) ## No Information Rate : 0.6536 ## P-Value [Acc &gt; NIR] : 6.204e-06 ## ## Kappa : 0.6029 ## ## Mcnemar&#39;s Test P-Value : 0.5708 ## ## Sensitivity : 0.7736 ## Specificity : 0.8400 ## Pos Pred Value : 0.7193 ## Neg Pred Value : 0.8750 ## Prevalence : 0.3464 ## Detection Rate : 0.2680 ## Detection Prevalence : 0.3725 ## Balanced Accuracy : 0.8068 ## ## &#39;Positive&#39; Class : pos ## We could also work directly with our labelled predictions and known labels. caret::confusionMatrix(glm_label_preds, glm_test$diabetes,positive=&quot;pos&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction neg pos ## neg 84 12 ## pos 16 41 ## ## Accuracy : 0.817 ## 95% CI : (0.7465, 0.8748) ## No Information Rate : 0.6536 ## P-Value [Acc &gt; NIR] : 6.204e-06 ## ## Kappa : 0.6029 ## ## Mcnemar&#39;s Test P-Value : 0.5708 ## ## Sensitivity : 0.7736 ## Specificity : 0.8400 ## Pos Pred Value : 0.7193 ## Neg Pred Value : 0.8750 ## Prevalence : 0.3464 ## Detection Rate : 0.2680 ## Detection Prevalence : 0.3725 ## Balanced Accuracy : 0.8068 ## ## &#39;Positive&#39; Class : pos ## 6.10 ROC Curve Summary The above can be confusing although what you will soon discover is that being able to compute the AUC (Area Under Curve) will be sufficient to judge the quality of a model - well in general it’s a good start. The caret package can do that for you so you don’t need to use the various functions about to find that. You might want to put up a ROC curve based on some predictions in which case you would still need to use one of the above functions. If you just want to see a basic ROC Curve then take this approach which will give you both the AUC and a ROC Curve albeit it much less “pretty” than the one above. # Use the colAUC function from the caTools package pm_model_glm_probs &lt;- predict(glm_model,glm_test,type=&quot;response&quot;) caTools::colAUC(pm_model_glm_probs,glm_test$diabetes,plotROC=TRUE) ## [,1] ## neg vs. pos 0.8677358 "],
["classification-example.html", "Chapter 7 Classification Example 7.1 Exploratory Plots 7.2 Generalized Linear Models 7.3 Random Forests 7.4 Target Variable Format 7.5 Addressing Class Imbalance", " Chapter 7 Classification Example Now that we’ve got an idea about how we might judge the performance quality of classification problem let’s look at the mechanics of implementing a classification model using the caret package. We’ve already seen it in action on a regression problem where we were predicting the MPG for the mtcars data frame. We’ll be sticking with the Pima Indians dataset. In case you have forgotten, here are the variables in the data frame pregnant - Number of times pregnant glucose - Plasma glucose concentration (glucose tolerance test) pressure - Diastolic blood pressure (mm Hg) triceps - Triceps skin fold thickness (mm) insulin - 2-Hour serum insulin (mu U/ml) mass - Body mass index (weight in kg/(height in m)\\^2) pedigree - Diabetes pedigree function age - Age (years) diabetes - Class variable (test for diabetes) url &lt;- &quot;https://raw.githubusercontent.com/steviep42/bios534_spring_2020/master/data/pima.csv&quot; pm &lt;- read.csv(url) So let’s look at some exploratory plots to see if there is anything interesting happening. We’ll use the Data Explorer package to help us with this although both R and Python have various packages to help with this kind of thing. 7.1 Exploratory Plots We’ll look use some stock plots from the DataExplorer package to get a feel for the data. Look at correlations between the variables to see if any are strongly correlated with the variable we wish to predict or any other variables. Let’s start out with the plot_intro function which can provide an overview of our data. It turns out that our data is pretty clean. There are no rows with missing values and we have only one categorical feature. plot_intro(pm) Let’s see if there are any string correlations we need to be aware of. plot_correlation(pm, type=&quot;continuous&quot;) There are more diabetes “negative” people than “positive”. plot_bar(pm) The histograms help us see what variables might be normally distributed although most of our features are skewed which makes sense in this case. For example, as people age, they tend to die so it’s not surprising that we have by far more young people. It looks to me that the insulin data is a little odd and might warrant greater consideration. plot_histogram(pm) This plot will show us side by side boxplots of the features as a function of “pos” or “neg”. This is helpful to determine if, for example, there might be significant differences between glucose levels across the positive and negative groups. It makes sense that there might be. Insulin might be also although it’s not totally apparent from the following graph. This is the kind of thing you would do to zone in on important variables. plot_boxplot(pm,by=&quot;diabetes&quot;) This plot will help us see if any of our features are normally distributed: plot_qq(pm,by=&quot;diabetes&quot;) It turns out that Data Explorer will help us create a detailed report involving all of these plot tops. create_report(pm, y = &quot;diabetes&quot;) At this point we know that we want to predict “diabetes” and that perhaps glucose is an important variables in the data. We also don’t observe many strong correlations in the data so multicollinearity isn’t a concern. We also don’t see strong evidence in the PCA plot that the data would benefit from a PCA transformation. One thing that we might consider doing is scaling the data since the features do not share the same measurement scale. We’ll take this into consideration. 7.2 Generalized Linear Models Let’s pick a technique to model the data with the ultimate goal of being able to predict whether someone has diabetes or not. We’ll start with the glm function in R. We’ll take a kitchen sink approach where we predict the diabetes variable (“yes” or “no”) based on the rest of the information in the data frame. set.seed(123) idx &lt;- createDataPartition(pm$diabetes, p = .8, list = FALSE, times = 1) head(idx) ## Resample1 ## [1,] 1 ## [2,] 3 ## [3,] 4 ## [4,] 6 ## [5,] 7 ## [6,] 8 train &lt;- pm[ idx,] test &lt;- pm[-idx,] # nrow(train) ## [1] 615 nrow(test) ## [1] 153 If we used the non caret approach we might do something like the following: pm_model_glm &lt;- glm(diabetes ~ ., data = train, family=&quot;binomial&quot;) pm_model_fitpreds &lt;- predict(pm_model_glm,test, type=&quot;response&quot;) fitpredt &lt;- ifelse(pm_model_fitpreds &gt; .385,&quot;pos&quot;,&quot;neg&quot;) fitpreds &lt;- factor(fitpredt,level=levels(test$diabetes)) caret::confusionMatrix(fitpreds, test$diabetes, positive=&quot;pos&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction neg pos ## neg 85 17 ## pos 15 36 ## ## Accuracy : 0.7908 ## 95% CI : (0.7178, 0.8523) ## No Information Rate : 0.6536 ## P-Value [Acc &gt; NIR] : 0.0001499 ## ## Kappa : 0.534 ## ## Mcnemar&#39;s Test P-Value : 0.8596838 ## ## Sensitivity : 0.6792 ## Specificity : 0.8500 ## Pos Pred Value : 0.7059 ## Neg Pred Value : 0.8333 ## Prevalence : 0.3464 ## Detection Rate : 0.2353 ## Detection Prevalence : 0.3333 ## Balanced Accuracy : 0.7646 ## ## &#39;Positive&#39; Class : pos ## And if you haven’t yet enough of ROC curves just yet, we could put up one of those. library(caTools) colAUC(pm_model_fitpreds,test$diabetes,plotROC=TRUE) ## [,1] ## neg vs. pos 0.8924528 But wait, we’ve already been through the whole ROC curve, AUC, confusion matrix route so why would we take a manual approach if we have the caret package readily available ? We can explore any number methods, implement K Fold Cross Validation,and get feedback on the performance measures all at the same time. Let’s reframe our above work using the caret package conveniences. We want to use Cross Fold validation here. ctrl &lt;- trainControl(method = &quot;cv&quot;, number = 5) pm_glm_mod &lt;- train(form = diabetes ~ ., data = train, trControl = ctrl, method = &quot;glm&quot;, family = &quot;binomial&quot;) pm_glm_mod ## Generalized Linear Model ## ## 615 samples ## 8 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 492, 492, 492, 492, 492 ## Resampling results: ## ## Accuracy Kappa ## 0.7642276 0.4521906 pm_glm_mod$results ## parameter Accuracy Kappa AccuracySD KappaSD ## 1 none 0.7642276 0.4521906 0.01991455 0.05311404 So we get an estimate of a 77% accuracy rate when the model is applied to out of sample data. This isn’t so impressive but we aren’t here to solve that problem (at least not just yet). So let’s make some predictions use thing test data to see what the Accuracy rate is. pm_glm_pred_labels &lt;- predict(pm_glm_mod,test) confusionMatrix(pm_glm_pred_labels,test$diabetes) ## Confusion Matrix and Statistics ## ## Reference ## Prediction neg pos ## neg 91 21 ## pos 9 32 ## ## Accuracy : 0.8039 ## 95% CI : (0.7321, 0.8636) ## No Information Rate : 0.6536 ## P-Value [Acc &gt; NIR] : 3.3e-05 ## ## Kappa : 0.5426 ## ## Mcnemar&#39;s Test P-Value : 0.04461 ## ## Sensitivity : 0.9100 ## Specificity : 0.6038 ## Pos Pred Value : 0.8125 ## Neg Pred Value : 0.7805 ## Prevalence : 0.6536 ## Detection Rate : 0.5948 ## Detection Prevalence : 0.7320 ## Balanced Accuracy : 0.7569 ## ## &#39;Positive&#39; Class : neg ## Actually not too bad. The train function provides is with an object that contains lots of information but in no way interferes with the results of the glm model. It’s as if you had built it using the standalone glm function which means that you can easily examine the model diagnostics: summary(pm_glm_mod$finalModel) ## ## Call: ## NULL ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.4719 -0.7674 -0.4402 0.7776 2.9436 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -7.8116450 0.7694301 -10.153 &lt; 2e-16 *** ## pregnant 0.0998300 0.0358381 2.786 0.00534 ** ## glucose 0.0342306 0.0040533 8.445 &lt; 2e-16 *** ## pressure -0.0148671 0.0055567 -2.676 0.00746 ** ## triceps -0.0006103 0.0076247 -0.080 0.93621 ## insulin -0.0007117 0.0009565 -0.744 0.45681 ## mass 0.0806695 0.0165458 4.876 1.09e-06 *** ## pedigree 0.9355556 0.3388561 2.761 0.00576 ** ## age 0.0154356 0.0102718 1.503 0.13291 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 796.05 on 614 degrees of freedom ## Residual deviance: 598.41 on 606 degrees of freedom ## AIC: 616.41 ## ## Number of Fisher Scoring iterations: 5 This includes the ability to see the various diagnostic plots: plot(pm_glm_mod$finalModel) We can certainly change the scoring metric to prioritize, for example, the area under the associated ROC curve. We just need to make some adjustments to the trainControl argument list and the train argument list. But these changes are minor. ctrl &lt;- trainControl(method = &quot;cv&quot;, number = 5, classProbs = T, summaryFunction = twoClassSummary) pm_glm_mod &lt;- train(form = diabetes ~ ., data = train, trControl = ctrl, metric = &quot;ROC&quot;, method = &quot;glm&quot;, family = &quot;binomial&quot;) pm_glm_mod$results ## parameter ROC Sens Spec ROCSD SensSD SpecSD ## 1 none 0.8198256 0.885 0.5488372 0.02909113 0.03235545 0.07463631 7.3 Random Forests Let’s use random forests to see what results we get. Random forests are robust to over fitting and are fairly easy to implement. They can improve accuracy by fitting many trees. Each tree is fit to a resampled version of the input data (usually a bootstrap). This is known as bootstrap aggregation or “bagged” trees. At each split, the function takes a random sample of columns (the mtry argument). The function we will use here, ranger, has three hyper parameters which could be set to a range of values which, in turn, could influence the resulting model. With glm, we didn’t really have a hyper parameter. We’ll worry about hyper parameters later. Let’s try this to see how it performs via the ranger method ctrl &lt;- trainControl(method = &quot;cv&quot;, number = 5, classProbs = T, summaryFunction = twoClassSummary) pm_ranger_mod &lt;- train(form = diabetes ~ ., data = train, trControl = ctrl, metric = &quot;ROC&quot;, method = &quot;ranger&quot;) By default the training process will move through different values of the algorithms hyper parameters - this is called “tuning”. caret knows what hyper parameters the method supports and can cycle through possible valid values of these hyper parameters. pm_ranger_mod ## Random Forest ## ## 615 samples ## 8 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 492, 492, 492, 492, 492 ## Resampling results across tuning parameters: ## ## mtry splitrule ROC Sens Spec ## 2 gini 0.8200000 0.8650 0.5162791 ## 2 extratrees 0.8232558 0.8850 0.5302326 ## 5 gini 0.8216279 0.8500 0.5441860 ## 5 extratrees 0.8243314 0.8800 0.5348837 ## 8 gini 0.8210756 0.8525 0.5674419 ## 8 extratrees 0.8235756 0.8800 0.5441860 ## ## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 1 ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were mtry = 5, splitrule = extratrees ## and min.node.size = 1. The object can be plotted. Here we see that the max AUC of .825 occurs when mtry is 3 and the Gini criterion is used to evaluate a tree. plot(pm_ranger_mod) max(pm_ranger_mod[[&quot;results&quot;]]$ROC) ## [1] 0.8243314 preds &lt;- predict(pm_ranger_mod,test) confusionMatrix(preds,test$diabetes) ## Confusion Matrix and Statistics ## ## Reference ## Prediction neg pos ## neg 93 20 ## pos 7 33 ## ## Accuracy : 0.8235 ## 95% CI : (0.7537, 0.8804) ## No Information Rate : 0.6536 ## P-Value [Acc &gt; NIR] : 2.528e-06 ## ## Kappa : 0.5864 ## ## Mcnemar&#39;s Test P-Value : 0.02092 ## ## Sensitivity : 0.9300 ## Specificity : 0.6226 ## Pos Pred Value : 0.8230 ## Neg Pred Value : 0.8250 ## Prevalence : 0.6536 ## Detection Rate : 0.6078 ## Detection Prevalence : 0.7386 ## Balanced Accuracy : 0.7763 ## ## &#39;Positive&#39; Class : neg ## 7.4 Target Variable Format If you notice, the format of the Pima data frame has indicated that the diabetes variable is a factor which is a special format in R to indicate categories. This is useful since the various R functions will generally know how to work with these variables without being told what to do. str(pm$diabetes) ## Factor w/ 2 levels &quot;neg&quot;,&quot;pos&quot;: 2 1 2 1 2 1 2 1 2 2 ... Sometimes you will read in data where the values are a 0 or a 1 although in that case you would still need to inform R that this variable is a factor else there would be a problem. Consider the following which reads in a slightly different version of the Pima data set where the diabetes variable has values of 0 and 1 to indicate “negative” or “positive”, respectively. aurl &lt;- &quot;https://raw.githubusercontent.com/steviep42/bios534_spring_2020/master/data/pima_10.csv&quot; pm_10 &lt;- read.csv(aurl) str(pm_10) ## &#39;data.frame&#39;: 768 obs. of 9 variables: ## $ pregnant: int 6 1 8 1 0 5 3 10 2 8 ... ## $ glucose : int 148 85 183 89 137 116 78 115 197 125 ... ## $ pressure: int 72 66 64 66 40 74 50 0 70 96 ... ## $ triceps : int 35 29 0 23 35 0 32 0 45 0 ... ## $ insulin : int 0 0 0 94 168 0 88 0 543 0 ... ## $ mass : num 33.6 26.6 23.3 28.1 43.1 25.6 31 35.3 30.5 0 ... ## $ pedigree: num 0.627 0.351 0.672 0.167 2.288 ... ## $ age : int 50 31 32 21 33 30 26 29 53 54 ... ## $ diabetes: int 1 0 1 0 1 0 1 0 1 1 ... Okay, so as far as R is concerned, the diabetes variable is an integer. We could try to use this in a call to the train function and it will fail. This is just like what happened when we were attempting to use regression. train(diabetes ~ ., data = pm_10, method = &quot;glm&quot;) You are trying to do regression and your outcome only has two possible values Are you trying to do classification? If so, use a 2 level factor as your outcome column.Generalized Linear Model So just as we did with the mtcars data frame, we’ll need to turn this into a factor: pm_10 &lt;- pm_10 %&gt;% mutate(diabetes=factor(diabetes)) train(diabetes ~ ., data = pm_10, method = &quot;glm&quot;) ## Generalized Linear Model ## ## 768 samples ## 8 predictor ## 2 classes: &#39;0&#39;, &#39;1&#39; ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 768, 768, 768, 768, 768, 768, ... ## Resampling results: ## ## Accuracy Kappa ## 0.7675321 0.4635273 Now, what would have happened had we been stuck with the version of the data frame that has “pos” and “neg” ? We still have the pm data frame. str(pm) ## &#39;data.frame&#39;: 768 obs. of 9 variables: ## $ pregnant: int 6 1 8 1 0 5 3 10 2 8 ... ## $ glucose : int 148 85 183 89 137 116 78 115 197 125 ... ## $ pressure: int 72 66 64 66 40 74 50 0 70 96 ... ## $ triceps : int 35 29 0 23 35 0 32 0 45 0 ... ## $ insulin : int 0 0 0 94 168 0 88 0 543 0 ... ## $ mass : num 33.6 26.6 23.3 28.1 43.1 25.6 31 35.3 30.5 0 ... ## $ pedigree: num 0.627 0.351 0.672 0.167 2.288 ... ## $ age : int 50 31 32 21 33 30 26 29 53 54 ... ## $ diabetes: Factor w/ 2 levels &quot;neg&quot;,&quot;pos&quot;: 2 1 2 1 2 1 2 1 2 2 ... While this is good to go, we could have processed it as follows which is something that you might have to do in other languages. This is a form of one hot encoding although the values in this case occupy one column since that is the column to be predicted. pm_alt &lt;- pm %&gt;% mutate(diabetes=ifelse(diabetes==&quot;pos&quot;,1,0)) %&gt;% mutate(diabetes = factor(diabetes)) str(pm_alt) ## &#39;data.frame&#39;: 768 obs. of 9 variables: ## $ pregnant: int 6 1 8 1 0 5 3 10 2 8 ... ## $ glucose : int 148 85 183 89 137 116 78 115 197 125 ... ## $ pressure: int 72 66 64 66 40 74 50 0 70 96 ... ## $ triceps : int 35 29 0 23 35 0 32 0 45 0 ... ## $ insulin : int 0 0 0 94 168 0 88 0 543 0 ... ## $ mass : num 33.6 26.6 23.3 28.1 43.1 25.6 31 35.3 30.5 0 ... ## $ pedigree: num 0.627 0.351 0.672 0.167 2.288 ... ## $ age : int 50 31 32 21 33 30 26 29 53 54 ... ## $ diabetes: Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 1 2 1 2 1 2 1 2 2 ... 7.5 Addressing Class Imbalance We have something of a problem in the Pima data. If we look at the number of positive cases vs the negative cases there is an imbalance which might be impacting the construction of our model. We have almost twice as many negative cases as we do positive. It’s not clear that this is a problem and if this proportion accurately reflects the prevalence diabetes in a larger population then perhaps we should accept this. However, we might not know the true prevalence either in the Pima population or a more general one. pm %&gt;% count(diabetes) ## # A tibble: 2 x 2 ## diabetes n ## &lt;fct&gt; &lt;int&gt; ## 1 neg 500 ## 2 pos 268 What techniques exist to deal with this ? There is the concept of “Sub Sampling” which uses sampling (e.g. bootstrap) to produce a training set that balances out the distribution of cases. This includes down sampling, up sampling, and hybrid sampling. Of course there are packages that do this although the caret package itself has some functions to help. We’ll present a down sampling example. Down sampling “randomly subset all the classes in the training set so that their class frequencies match the least prevalent class”. In this case we’ll use sampling to create a new training set where the two classes counts are equal. Whether this is advisable is another question altogether but let’s see the impact it has on the predictive model. down_train &lt;- downSample(x = train[, -ncol(train)], y = train$diabetes, yname=&quot;diabetes&quot;) table(down_train$diabetes) ## ## neg pos ## 215 215 Next we’ll build our model as before except now we’ll use the down sampled training data. This provides better performance in the ROC and Specificity measures but we experience a reduction in Sensitivity. ctrl &lt;- trainControl(method = &quot;cv&quot;, number = 5, classProbs = TRUE, summaryFunction = twoClassSummary) pm_glm_down &lt;- train(form = diabetes ~ ., data = down_train, trControl = ctrl, metric = &quot;ROC&quot;, method = &quot;glm&quot;, preProc = c(&quot;center&quot;, &quot;scale&quot;)) pm_glm_down ## Generalized Linear Model ## ## 430 samples ## 8 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## Pre-processing: centered (8), scaled (8) ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 344, 344, 344, 344, 344 ## Resampling results: ## ## ROC Sens Spec ## 0.8133045 0.7674419 0.7348837 Check the predictions: down_preds &lt;- predict(pm_glm_down,test,type=&quot;prob&quot;) colAUC(down_preds[,2],test$diabetes,plotROC = TRUE) ## [,1] ## neg vs. pos 0.8939623 So how does this compare to the pm_glm_model we created earlier using the training set that reflected the less balanced data ? pm_glm_mod ## Generalized Linear Model ## ## 615 samples ## 8 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 492, 492, 492, 492, 492 ## Resampling results: ## ## ROC Sens Spec ## 0.8198256 0.885 0.5488372 unbalanced_preds &lt;- predict(pm_glm_mod,test,type=&quot;prob&quot;) colAUC(unbalanced_preds[,2],test$diabetes,plotROC = TRUE) ## [,1] ## neg vs. pos 0.8924528 "],
["decision-trees.html", "Chapter 8 Decision Trees 8.1 Advantages 8.2 A Classification Example 8.3 Digging Deeper 8.4 Gini Index 8.5 Regression Trees 8.6 Parameters vs Hyperparameters 8.7 Grid Searching 8.8 Bagged Trees 8.9 Random Forests 8.10 Boosted Trees 8.11 Using caret", " Chapter 8 Decision Trees Tree-based methods employ a segmentation strategy that partitions the feature / predictor space into a series of decisions which has the added benefit of being easy to understand. Think of it as a flow chart for making decisions. The viewer of the chart is presented with a diagram that offers outcomes in response to (yes / no) questions (decisions) about important predictors found in the data set. 8.1 Advantages The advantages of tree-based methods include that 1 - The model is generally easy to interpret - The path to a decision is plainly spelled out (assuming that the number of tree splits is easy enough to trace). - The method can handle numeric and categorical - One does not generally need to pre process or normalize data - Missing data is less of a big deal Disadvantages include: Large trees are hard to follow - variance can be high Trees can be overly complex Overfitting can be a problem 8.2 A Classification Example Let’s use the Pima Indians data set as it relates to predicting whether someone has diabetes. This data is provided by the mlbench package. The relevant variables are: pregnant - Number of times pregnant glucose - Plasma glucose concentration (glucose tolerance test) pressure - Diastolic blood pressure (mm Hg) triceps - Triceps skin fold thickness (mm) insulin - 2-Hour serum insulin (mu U/ml) mass - Body mass index (weight in kg/(height in m)\\^2) pedigree - Diabetes pedigree function age - Age (years) diabetes - Class variable (test for diabetes) library(mlbench) data(PimaIndiansDiabetes) pm &lt;- PimaIndiansDiabetes diabetes_mod_1 &lt;- rpart(diabetes ~ .,pm, method=&quot;class&quot;,cp=0.017) rpart.plot(x = diabetes_mod_1) That’s pretty understandable and you could show this to someone and they would probably get it without too much explanation as long as they had an awareness of the features in the data set. The “node” at the top is called the “root node” and the lines are “branches” that go either to a “terminal node” or to “leaf nodes” which involve some comparison. It’s a flow chart for decisions about whether someone has diabetes or not. Note that there is also information about what percentages of “pos” or “neg” there are in each branch. diabetes_preds &lt;- predict(diabetes_mod_1,type=&quot;class&quot;) table(diabetes_preds,pm$diabetes) ## ## diabetes_preds neg pos ## neg 470 131 ## pos 30 137 # Calculate the confusion matrix for the test set caret::confusionMatrix(diabetes_preds, pm$diabetes, positive=&quot;pos&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction neg pos ## neg 470 131 ## pos 30 137 ## ## Accuracy : 0.7904 ## 95% CI : (0.7598, 0.8186) ## No Information Rate : 0.651 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.4944 ## ## Mcnemar&#39;s Test P-Value : 3.245e-15 ## ## Sensitivity : 0.5112 ## Specificity : 0.9400 ## Pos Pred Value : 0.8204 ## Neg Pred Value : 0.7820 ## Prevalence : 0.3490 ## Detection Rate : 0.1784 ## Detection Prevalence : 0.2174 ## Balanced Accuracy : 0.7256 ## ## &#39;Positive&#39; Class : pos ## Perhaps you noticed that in the example I included an argument called cp which corresponds to a “complexity parameter” which influences how the tree splits at various nodes. &gt; The main role of this parameter is to save computing time by pruning off splits that are obviously not worthwhile. Essentially,the user informs the program that any split which does not improve the fit by cp will likely be pruned off by cross-validation, and that hence the program need not pursue it. We’ll explore this in more detail momentarily. This is what is known as a “hyperparameter”. 8.3 Digging Deeper Let’s create a training set that comprises 80% of the data with a holdout set of 20%. set.seed(123) percent &lt;- .80 train_idx &lt;- sample(1:nrow(pm), round(percent*nrow(pm))) train_idx[1:10] ## [1] 415 463 179 526 195 118 299 229 244 14 # Subset the pm data frame to training indices only pm_train &lt;- pm[train_idx, ] # Exclude the training indices to create the test set pm_test &lt;- pm[-train_idx, ] # Train the model (to predict &#39;default&#39;) pm_class_tree &lt;- rpart(formula = diabetes ~ ., data = pm_train, method = &quot;class&quot;) # Look at the model output pm_class_tree ## n= 614 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 614 216 neg (0.64820847 0.35179153) ## 2) glucose&lt; 143.5 475 113 neg (0.76210526 0.23789474) ## 4) glucose&lt; 103.5 187 17 neg (0.90909091 0.09090909) * ## 5) glucose&gt;=103.5 288 96 neg (0.66666667 0.33333333) ## 10) mass&lt; 26.9 72 7 neg (0.90277778 0.09722222) * ## 11) mass&gt;=26.9 216 89 neg (0.58796296 0.41203704) ## 22) age&lt; 30.5 119 34 neg (0.71428571 0.28571429) ## 44) pressure&gt;=22 112 28 neg (0.75000000 0.25000000) * ## 45) pressure&lt; 22 7 1 pos (0.14285714 0.85714286) * ## 23) age&gt;=30.5 97 42 pos (0.43298969 0.56701031) ## 46) age&gt;=56.5 9 2 neg (0.77777778 0.22222222) * ## 47) age&lt; 56.5 88 35 pos (0.39772727 0.60227273) ## 94) pedigree&lt; 0.201 11 3 neg (0.72727273 0.27272727) * ## 95) pedigree&gt;=0.201 77 27 pos (0.35064935 0.64935065) * ## 3) glucose&gt;=143.5 139 36 pos (0.25899281 0.74100719) ## 6) mass&lt; 29.95 27 12 neg (0.55555556 0.44444444) ## 12) pressure&gt;=74.5 9 1 neg (0.88888889 0.11111111) * ## 13) pressure&lt; 74.5 18 7 pos (0.38888889 0.61111111) * ## 7) mass&gt;=29.95 112 21 pos (0.18750000 0.81250000) * rpart.plot(pm_class_tree) 8.3.1 Evaluating performance So this is where things get interesting. We’ll use a confusion matrix to help us figure some things out about this model. # Generate predicted classes using the model object pm_class_pred &lt;- predict(object = pm_class_tree, newdata = pm_test, type = &quot;class&quot;) # Calculate the confusion matrix for the test set caret::confusionMatrix(pm_class_pred, pm_test$diabetes, positive=&quot;pos&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction neg pos ## neg 79 20 ## pos 23 32 ## ## Accuracy : 0.7208 ## 95% CI : (0.6429, 0.79) ## No Information Rate : 0.6623 ## P-Value [Acc &gt; NIR] : 0.07215 ## ## Kappa : 0.3845 ## ## Mcnemar&#39;s Test P-Value : 0.76037 ## ## Sensitivity : 0.6154 ## Specificity : 0.7745 ## Pos Pred Value : 0.5818 ## Neg Pred Value : 0.7980 ## Prevalence : 0.3377 ## Detection Rate : 0.2078 ## Detection Prevalence : 0.3571 ## Balanced Accuracy : 0.6949 ## ## &#39;Positive&#39; Class : pos ## We can also look at the Area Under the ROC Curve. library(Metrics) dt_pred &lt;- predict(pm_class_tree, newdata = pm_test, type=&quot;prob&quot;) converted &lt;- ifelse(pm_test$diabetes == &quot;pos&quot;,1,0) aucval &lt;- Metrics::auc(actual = converted, predicted = dt_pred[,2]) aucval ## [1] 0.7391591 8.3.2 Tree Splitting The resulting tree can be thought of as an upside down tree with the root at the top. The “trunk” proceeds downward and splits into subsets based on some decision (hence the word “decision” in the title). When classifying data the idea is to segment or partition data into groups/regions where each group contains or represents a single class (“yes/no”, “positive/negative”). These groups or regions would represent a “pure” region. This is not always possible so a best effort is made. These regions are separated by decision boundaries which are used to make decisions. We’ll plot some example data to illustrate the case. 8.4 Gini Index So we have to find a way to make the decisions such that the resulting regions are as pure as possible. This could be measuring the degree of impurity or purity - so we are either maximizing purity or minimizing impurity The so called “Gini index”&quot; gives us the degree or measure of impurity. The lower the Gini index, the lower the degree of impurity (this higher purity). The higher the Gini index the higher the degree of impurity (this lower purity). The decision tree will select the split that minimizes or lowers the Gini index. There are other measures or indices that can be used such as the “information” measure. Let’s train two models that use a different splitting criterion (“gini” and “information”) and then use the test set to choose a “best” model. To do this you’ll use the parms argument of the rpart function. This argument takes a named list that contains values of different parameters to influence how the model is trained. Finally, to assess the models we’ll use the ce function from the Metrics package to show the proportion of elements in actual that are not equal to the corresponding element in predicted. # Train a gini-based model pm_gini_mod &lt;- rpart(formula = diabetes ~ ., data = pm_train, method = &quot;class&quot;, parms = list(split = &quot;gini&quot;)) # Train an information-based model pm_info_mod &lt;- rpart(formula = diabetes ~ ., data = pm_train, method = &quot;class&quot;, parms = list(split = &quot;information&quot;)) # Create some predictions gini_pred &lt;- predict(object = pm_gini_mod, newdata = pm_test, type = &quot;class&quot;) # Generate predictions on the validation set using the information model info_pred &lt;- predict(object = pm_info_mod, newdata = pm_test, type = &quot;class&quot;) # Compare classification error Metrics::ce(actual = pm_test$diabetes, predicted = gini_pred) ## [1] 0.2792208 Metrics::ce(actual = pm_test$diabetes, predicted = info_pred) ## [1] 0.2597403 8.5 Regression Trees Predicting numeric outcomes can also be of interest. Given some patient characteristics relative to a disease, we might want to predict a viral load quantity. A gambler might want to predict a final score for a team. Unlike, classification problems, we are looking at estimating a numeric outcome. R has a built in function for this called “lm” which can be used but we can also use Trees to do this since, after, all, it does some nice things for us like not having to worry about normalizing data (not that that is hard) or the mixture of quantitative and categorical data. 8.5.1 Performance Measure Since we are predicting a numeric outcome we would like to come up with a metric to help us figure out if the model we have is good or not. With classification situations we can employ confusion matrices and ROC curves. Here we will use something more simplistic but effective - Root Mean Square Error. The formula looks like the following where P represents a vector of predictions and O represents a vector of the observed (true) values. \\[ RMSE = \\sqrt\\frac{\\sum_i^n(P_i-O_i)^2}{n} \\] We could even write our own function for this although the Metrics package has a function called rmse to do this. Let’s build a classification tree model on the Pima Indians data to predict the body mass of a participant. # Train the model mass_pima_mod &lt;- rpart(formula = mass ~ ., data = pm_train, method = &quot;anova&quot;) # Look at the model output print(mass_pima_mod) ## n= 614 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 614 36909.6400 31.96971 ## 2) triceps&lt; 30.5 422 21954.9400 29.74147 ## 4) diabetes=neg 293 13608.7800 28.26007 ## 8) pressure&lt; 45 20 2689.8690 21.19500 ## 16) age&lt; 27 12 1622.3870 16.16667 * ## 17) age&gt;=27 8 308.9587 28.73750 * ## 9) pressure&gt;=45 273 9847.4740 28.77766 ## 18) triceps&lt; 22.5 190 6889.5350 27.84947 * ## 19) triceps&gt;=22.5 83 2419.5400 30.90241 * ## 5) diabetes=pos 129 6242.6950 33.10620 ## 10) glucose&lt; 125.5 47 3036.8640 30.57660 ## 20) pregnant&gt;=5.5 19 1699.6170 26.62632 * ## 21) pregnant&lt; 5.5 28 839.5686 33.25714 * ## 11) glucose&gt;=125.5 82 2732.7020 34.55610 * ## 3) triceps&gt;=30.5 192 8254.2630 36.86719 ## 6) pregnant&gt;=1.5 128 4020.7590 35.12813 ## 12) triceps&lt; 35.5 64 2192.8970 33.11406 * ## 13) triceps&gt;=35.5 64 1308.6360 37.14219 * ## 7) pregnant&lt; 1.5 64 3072.1590 40.34531 ## 14) glucose&lt; 128.5 38 1014.3150 38.05000 * ## 15) glucose&gt;=128.5 26 1565.0400 43.70000 ## 30) pressure&lt; 75 10 122.8810 38.67000 * ## 31) pressure&gt;=75 16 1031.0190 46.84375 * # Plot the tree model rpart.plot(x = mass_pima_mod, yesno = 2, type = 0, extra = 0) Let’s compute the RMSE for this model. # Generate predictions on a test set pred &lt;- predict(object = mass_pima_mod, # model object newdata = pm_test) # test dataset # Compute the RMSE Metrics::rmse(actual = pm_test$mass, predicted = pred) ## [1] 6.627414 Is this Good ? Bad ? Okay ? We don’t know. We’ll need to do some things like Cross Validation. And then there are additional arguments to the rpart function that we could use to influence how the model does its job. These are referred to as “hyperparamters”. 8.6 Parameters vs Hyperparameters Model parameters are things that are generated as part of the modeling process. These might be things like slope and intercept from a linear model or, in the case of an rpart model, the number of splits in the final tree or the total number of leaves. Hyper parameters (sometimes called metaparameters) represent information that is supplied in the form of an argument prior the call to the method to generate results. These parameters might not be something one can intelligently set without some experimentation. Of course, most modeling functions one would call in R have default values for various arguments but this does not mean that the defaults are appropriate for all cases. To see the hyper parameters of the rpart function, check the help page for rpart.control. Tuning the hyperparameters for rpart would involve adjusting the following hyper parameters or using some post processing function to refine the model relative to these parameters: cp which is the complexity parameter (default is .01) - smaller values means more complexity minsplit the minimum number of observations that must exist in a node before a split is attempted (default is 20) maxdepth maximum number of nodes between a final node and root node There are other hyper parameters but we can start with these. The rpart.control function will do something called cross validation which involves repeatedly running the rpart some number of times (10 by default) while internally specifying different values for the above mentioned hyper parameters. In effect, it is doing some work for you so you don’t have to. At the end of the run it will produce a table for inspection. The results of this table can then be used to “prune” the tree model to get a “better” tree - one that performs better than an “un pruned” tree. Let’s look at the pima mass model: # You can plot the modelling object rpart.plot(mass_pima_mod,yesno = 2, type = 0, extra = 0) # You can inspect the table manually mass_pima_mod$cptable ## CP nsplit rel error xerror xstd ## 1 0.18153603 0 1.0000000 1.0015656 0.09381646 ## 2 0.05698963 1 0.8184640 0.8447727 0.08092338 ## 3 0.03146457 2 0.7614743 0.8067354 0.07952153 ## 4 0.02902872 3 0.7300098 0.7901790 0.07936648 ## 5 0.02055084 4 0.7009810 0.7721064 0.07632954 ## 6 0.01458696 5 0.6804302 0.7798124 0.07806591 ## 7 0.01406747 6 0.6658432 0.8012778 0.07730342 ## 8 0.01335162 7 0.6517758 0.7928406 0.07622939 ## 9 0.01315114 8 0.6384242 0.7932476 0.07707739 ## 10 0.01113909 10 0.6121219 0.8022725 0.07716219 ## 11 0.01000000 11 0.6009828 0.8042465 0.07301588 We want to pick the value of CP from the table that corresponds to the minimum xerror value. This can also be deduced from the plot but let’s work with the table: cpt &lt;- as.data.frame.matrix(mass_pima_mod$cptable) cpt_val &lt;- cpt[order(cpt$xerror),][1,]$CP We’ll use the CP value associated with the minimum error which in this case turns out to be 0.01. This is now passed to the prune function. mass_pima_mod_opt &lt;- prune(mass_pima_mod, cp = cpt_val) # Plot the optimized model rpart.plot(mass_pima_mod_opt, yesno = 2, type = 0, extra = 0) Does this optimized model perform any better ? Not really, because the optimal CP value turned out to be 0.01 which is actually the same as the default. # Generate predictions on a test set pred &lt;- predict(object = mass_pima_mod_opt, # model object newdata = pm_test) # test dataset # Compute the RMSE Metrics::rmse(actual = pm_test$mass, predicted = pred) ## [1] 6.790943 8.7 Grid Searching We might want to review several different models that correspond to various hyperparamter sets. Our goal is to find the best performing model based on a systematic approach that allows us to assess each model in a fair way. There are functions that can help us build a “grid” of hyperparameter values that can then “feed” the function arguments. So we train models on a combination of these values and compare them using the RMSE for regression or ROC / Confusion Matrix for classification setups. Setting up the grid involves a manual process (although as we will eventually see) the caret package can help automate this for us. Knowing about the valid values for a hyperparameter is critical so some experimentation is important. The following process sets up a data frame of two columns each of which corresponds to a hyperparamter of the rpart function. The intent here is to call rpart a number of times using each row of the below data frame to supply the values for the respective arguments in rpart. mysplits &lt;- seq(1, 35, 5) mydepths &lt;- seq(5,40, 10) my_cool_grid &lt;- expand.grid(minsplit = mysplits, maxdepth = mydepths) head(my_cool_grid) ## minsplit maxdepth ## 1 1 5 ## 2 6 5 ## 3 11 5 ## 4 16 5 ## 5 21 5 ## 6 26 5 We’ll generate models, predictions, and RMSE values and stash them for later review. Once again, we’ll be predicting the mass variable in the Pima Indians data frame. do_grid_search &lt;- function(minsplit,maxdepth) { # Setup some book keeping structures mods &lt;- list() preds &lt;- list() myrmse &lt;- vector() mygrid &lt;- expand.grid(minsplit=minsplit, maxdepth=maxdepth) for (ii in 1:nrow(mygrid)) { minsplit &lt;- mygrid[ii,]$minsplit maxdepth &lt;- mygrid[ii,]$maxdepth # Build the Model mods[[ii]] &lt;- rpart(mass ~ ., data = pm_train, method = &quot;anova&quot;, minsplit = minsplit, maxdepth = maxdepth) # Now predict against the test data preds[[ii]] &lt;- predict(mods[[ii]], newdata = pm_test) # Get RMSE myrmse[ii] &lt;- Metrics::rmse(actual = pm_test$mass, predicted = preds[[ii]]) } # Find the model that has the lowest rmse idx &lt;- which.min(myrmse) # Get the control parameters for the best model optimal_model &lt;- mods[[idx]] rmseval &lt;- myrmse[idx] return(list(optimal_model=optimal_model,optim_rmse=rmseval)) } Now run the function and get the parameters corresponding to the optimal model. results &lt;- do_grid_search(seq(1, 4, 1),seq(1, 6, 1)) results$optim_rmse ## [1] 6.851048 results$optimal_model$control ## $minsplit ## [1] 2 ## ## $minbucket ## [1] 1 ## ## $cp ## [1] 0.01 ## ## $maxcompete ## [1] 4 ## ## $maxsurrogate ## [1] 5 ## ## $usesurrogate ## [1] 2 ## ## $surrogatestyle ## [1] 0 ## ## $maxdepth ## [1] 4 ## ## $xval ## [1] 10 8.8 Bagged Trees Now we look at bagged trees which involves looking at many trees in aggregate - this is an ensemble method. It helps to reduce the variance associated with a single decision tree which can be highly sensitive to changes in data. The term bagging refers to “bootstrap aggregation”. To review, the Decision Tree process can be represented like this: The bootstrap method of sampling will resample the training data some number of times (with replacement) and retrain a number of models on the resampled data to average out the error. This looks something like: The input data set is resampled with replacement some number of times (e.g. 10,50, 100) The resampled data is usually a subset of the data which leaves some portion of the data available to use a mini test set for prediction (“out of bag data”&quot;) Get the RMSE from the prediction Average out the RMSE So you get a lot of different trees whose performance can be averaged over boostrapped data sets which might include observations several times or not at all. This should result in less variance. A reason NOT to use bagged trees involves the idea that a collection of trees is not nearly as easy to look at as a single decision tree. We can actually write our own version of bagged trees using a for loop. Sort of like what we did above. First, let’s get our test / train pair for the Pima Indians data. set.seed(123) percent &lt;- .80 train_idx &lt;- sample(1:nrow(pm), round(percent*nrow(pm))) train_idx[1:10] ## [1] 415 463 179 526 195 118 299 229 244 14 # Subset the pm data frame to training indices only pm_train &lt;- pm[train_idx, ] # Exclude the training indices to create the test set pm_test &lt;- pm[-train_idx, ] Next, well create 50 different trees based on 50 bootstrapped samples of the training data. We will sample WITH replacement. This means that some of the rows from the data will be repeated and some will be left out all together. We can figure out what rows were not included and use them to create a test set referred to as “out of bag” samples. training_boot_idx &lt;- sample(1:nrow(pm),replace=TRUE) test_boot_idx &lt;- !(1:32 %in% training_boot_idx) Let’s start looping library(Metrics) modl &lt;- list() predl &lt;- list() aucval &lt;- vector() acc &lt;- vector() number_of_boostraps &lt;- 50 for (ii in 1:number_of_boostraps) { training_boot_idx &lt;- sample(1:nrow(pm),replace=TRUE) test_boot_idx &lt;- !(1:32 %in% training_boot_idx) modl[[ii]] &lt;- rpart(diabetes ~ ., data = pm[training_boot_idx,], method = &quot;class&quot; ) # This list will contain / hold the models build on the bootstrap predl[[ii]] &lt;- predict(modl[[ii]], newdata=pm[test_boot_idx,], type=&quot;prob&quot;) converted &lt;- ifelse(pm[test_boot_idx,]$diabetes == &quot;pos&quot;,1,0) # Let&#39;s create an estimate of the AUC aucval[ii] &lt;- Metrics::auc(actual = converted, predicted = predl[[ii]][,2]) } Now check all of the accuracy estimates and then average them. Remember this is supposed to help us to better estimate the out of sample error. boxplot(aucval) mean(aucval) ## [1] 0.8217163 Now we’ll compare this to the function called “bagging” from the ipred package which does bagged trees directly. library(ipred) bagged_pm &lt;- bagging(diabetes ~ ., data = pm_train, coob = TRUE) bagged_pred &lt;- predict(bagged_pm, newdata = pm_test, type=&quot;prob&quot;) converted &lt;- ifelse(pm_test$diabetes == &quot;pos&quot;,1,0) aucval &lt;- Metrics::auc(actual = converted, predicted = bagged_pred[,2]) aucval ## [1] 0.8312594 This is better than the 0.7804305 we got when using a single Decision tree. Single decision trees are sometimes called “weak learners” because 8.9 Random Forests In bagging, all features are used when considering if and when to split. However, with the Random Forest approach, a subset of features are selected at random at each split in a decision tree. You could think of random forests as being an extension of Bagged Trees. Typically they are also an improvement. As with other tree methods, we don’t have to worry a lot about preprocessing the data although we could if we wanted to. Basically, one of the main advantages of tree methods is that it tolerates a combination of data types on different scales which makes it good as a “go to” method for beginners. Random forests will sample some number of features when considering a split. This can be influenced by a hyperparameter called mtry which is limited to the number of features in the data set. library(randomForest) # Train a Random Forest set.seed(1) # for reproducibility pm_rf_model &lt;- randomForest(formula = diabetes ~ ., data = pm_train) # Print the model output print(pm_rf_model) ## ## Call: ## randomForest(formula = diabetes ~ ., data = pm_train) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 2 ## ## OOB estimate of error rate: 25.57% ## Confusion matrix: ## neg pos class.error ## neg 336 62 0.1557789 ## pos 95 121 0.4398148 Let’s look at the out of bag error matrix head(pm_rf_model$err.rate) ## OOB neg pos ## [1,] 0.3274336 0.2816901 0.4047619 ## [2,] 0.3185596 0.2533333 0.4264706 ## [3,] 0.2979684 0.2127660 0.4472050 ## [4,] 0.2862903 0.2012579 0.4382022 ## [5,] 0.2987013 0.2144928 0.4484536 ## [6,] 0.2929825 0.1934605 0.4729064 plot(pm_rf_model) Do some predictions with this model. rf_pred &lt;- predict(pm_rf_model, newdata = pm_test, type=&quot;prob&quot;) converted &lt;- ifelse(pm_test$diabetes == &quot;pos&quot;,1,0) aucval &lt;- Metrics::auc(actual = converted, predicted = rf_pred[,2]) aucval ## [1] 0.8453054 A slight improvement over the bagged trees. 8.10 Boosted Trees We’ll finish off this section with a discussion on boosted trees which represents an extension to random forests. Here is how we have been progressing thus far: Single Decision Tree Bagged Decision Trees (Aggregated Trees using all features) Random Forests (Many Trees using a number of of sampled features) Methods 2 and 3 will use bootstrap sampling on the input data which means there will be sampling with replacement to generate a training set. After a tree is built then it will be applied to the OOB (Out Of Band) data left over from the bootstrap sample. This will then be used in the computation of an average. With boosting, we don’t try to keep up with the idea of reducing the variance emerging from a number of individual trees (aka “learners”). Nor do we consider each tree as being independent and later try to integrate it into an average tree. With boosting we create a sequence of trees such that any subsequent tree represents an improvement on the previous tree(s) thus there is some dependency in the interest of improvement. The process attempts to learn from previous “mistakes” in the creation of down stream trees. Boosting looks at the residuals from a tree and pays attention to any problematic observations when creating a subsequent tree. Here is an example. We’ll need to do a bit more prep on the data though prior to use: prep_pm_train &lt;- pm_train prep_pm_train$diabetes &lt;- ifelse(pm_train$diabetes==&quot;pos&quot;,1,0) We had to turn the “pos” / “neg” values into, respectively, 1 and 0. The gbm function requires this for classification problems. library(gbm) pm_boost &lt;- gbm(diabetes ~ . , distribution=&quot;bernoulli&quot;, data = prep_pm_train,n.trees=1000) summary(pm_boost) ## var rel.inf ## glucose glucose 24.381141 ## mass mass 16.145442 ## pedigree pedigree 14.771739 ## age age 12.973055 ## insulin insulin 9.674303 ## pressure pressure 8.549152 ## pregnant pregnant 7.542437 ## triceps triceps 5.962731 boost_pred &lt;- predict(pm_boost, newdata = pm_test, type=&quot;response&quot;, n.trees=50) converted &lt;- ifelse(pm_test$diabetes == &quot;pos&quot;,1,0) aucval &lt;- Metrics::auc(actual = converted, predicted = boost_pred) aucval ## [1] 0.8378582 Of course, we can use the caret package to knit these altogether into one framework that dispenses wit the need to understand the details of each method although it is important for you to see examples of standalone functions since 1) it can help your understanding of the method and 2) in languages such as Python there is no equivalent to caret. 8.11 Using caret We looked at individual methods including rpart, bagging, random forests, and boosted trees. Each of these had its own approach and hyperparameters. With caret we can do basically plugin methods. control &lt;- trainControl(method=&quot;cv&quot;, number=10, summaryFunction = twoClassSummary, classProbs = TRUE, savePredictions = TRUE, verboseIter = FALSE) rpart_caret &lt;- train(diabetes ~ ., data = pm_test, method = &quot;rpart&quot;, metric = &quot;ROC&quot;, trControl = control) rpart_caret ## CART ## ## 154 samples ## 8 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 139, 138, 139, 139, 139, 137, ... ## Resampling results across tuning parameters: ## ## cp ROC Sens Spec ## 0.02884615 0.6598333 0.8027273 0.4166667 ## 0.04807692 0.6637424 0.8027273 0.4600000 ## 0.17307692 0.5006364 0.8518182 0.1600000 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was cp = 0.04807692. Next, we try out the bagging approach bagging_caret &lt;- train(diabetes ~ ., data = pm_test, method = &quot;treebag&quot;, metric = &quot;ROC&quot;, trControl = control) bagging_caret ## Bagged CART ## ## 154 samples ## 8 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 139, 138, 137, 138, 139, 139, ... ## Resampling results: ## ## ROC Sens Spec ## 0.7810152 0.8336364 0.5566667 Next we try out the random forest function rf_caret &lt;- train(diabetes ~ ., data = pm_test, method = &quot;rf&quot;, metric = &quot;ROC&quot;, trControl = control) rf_caret ## Random Forest ## ## 154 samples ## 8 predictor ## 2 classes: &#39;neg&#39;, &#39;pos&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 138, 139, 138, 139, 138, 139, ... ## Resampling results across tuning parameters: ## ## mtry ROC Sens Spec ## 2 0.8137576 0.8636364 0.5500000 ## 5 0.8098939 0.8527273 0.5533333 ## 8 0.7989091 0.8527273 0.5333333 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was mtry = 2. And finally, the boosted approach: boosted_caret &lt;- train(diabetes ~ ., data = pm_test, method = &quot;gbm&quot;, metric = &quot;ROC&quot;, verbose = FALSE, tuneLength = 10, trControl = control) Let’s look at how they all performed: cat(&quot;Results for Rpart \\n&quot;) ## Results for Rpart rpart_caret$results[1,] ## cp ROC Sens Spec ROCSD SensSD SpecSD ## 1 0.02884615 0.6598333 0.8027273 0.4166667 0.1334386 0.09554349 0.2206892 cat(&quot;Results for Bagging \\n&quot;) ## Results for Bagging bagging_caret$results[1,] ## parameter ROC Sens Spec ROCSD SensSD SpecSD ## 1 none 0.7810152 0.8336364 0.5566667 0.09758653 0.07879371 0.2096735 cat(&quot;Results for Random Forest \\n&quot;) ## Results for Random Forest rf_caret$results[1,] ## mtry ROC Sens Spec ROCSD SensSD SpecSD ## 1 2 0.8137576 0.8636364 0.55 0.1457962 0.1036523 0.2389535 cat(&quot;Results for Boosted Trees \\n&quot;) ## Results for Boosted Trees boosted_caret$results[1,] ## shrinkage interaction.depth n.minobsinnode n.trees ROC Sens Spec ## 1 0.1 1 10 50 0.799 0.8445455 0.5333333 ## ROCSD SensSD SpecSD ## 1 0.106574 0.109967 0.2591534 "],
["using-methods-other-than-lm.html", "Chapter 9 Using Methods Other Than lm 9.1 Parameters vs Hyperparameters 9.2 Hyperparameter Tuning 9.3 Using Validation Data Sets", " Chapter 9 Using Methods Other Than lm So when considering our initial example of using the lm function I think (hope) that I have convinced you that using caret to call lm is the way to go since it generalizes the process of cross fold validation and the presentation of results. It’s also quite convenient in that we can use other methods than lm to do some predictions. To that end, let’s look at random forests to see if it improves the situation. Note that we aren’t, at least at this point, trying to understand the underlying details and subtleties of any of the alternative functions we might use although that is ultimately very important. However, we’ll defer the conversation until later. Many people are surprised to learn that random Forests (or even a single Decision Tree) can be used to predict a numeric outcome, but they can be. The advantages of using random Forests include the following: - easy to use - resistant to overfitting - accurate use for non linear models Some problems include: - the rd function requires setting hyperparameters - adjustment of hyperparameters can be specific to the data set - default vlaues will requie adjustment or &quot;tuning&quot; For now, let’s set up a “shoot out” between the lm function and the rf function to see if the latter yields a lower RMSE than the former (or vice versa). set.seed(124) new_idx &lt;- createDataPartition(mtcars$mpg,p=.80,list=FALSE) new_train &lt;- mtcars[new_idx,] new_test &lt;- mtcars[-new_idx,] caret_lm &lt;- train(mpg ~ ., data = new_train, method = &quot;lm&quot;) caret_lm$results ## intercept RMSE Rsquared MAE RMSESD RsquaredSD MAESD ## 1 TRUE 4.937432 0.5675623 4.014161 1.333183 0.1958169 1.249055 Metrics::rmse(new_test$mpg, predict(caret_lm,new_test)) ## [1] 5.087608 So let’s see what the random forest function will give us with the same data. Because we are using caret, all we have to is sub in the desired method which is “rf”. caret_rf &lt;- train(mpg ~ ., data = new_train, method = &quot;rf&quot;) caret_rf$results ## mtry RMSE Rsquared MAE RMSESD RsquaredSD MAESD ## 1 2 2.767412 0.8678962 2.238987 1.086212 0.06200765 0.7913614 ## 2 6 2.673043 0.8756824 2.162140 1.005430 0.06150165 0.7275747 ## 3 10 2.803810 0.8587926 2.288413 1.013513 0.06827013 0.7410137 Metrics::rmse(new_test$mpg, predict(caret_rf,new_test)) ## [1] 2.145429 So the random Forest approach, in this case, produces a much lower RMSE for the test data frame. The larger question relates to why there is more information in the output for the rf model. What is the mtry argument and why does it take on three different values during the execution of the function ? caret_rf ## Random Forest ## ## 28 samples ## 10 predictors ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 28, 28, 28, 28, 28, 28, ... ## Resampling results across tuning parameters: ## ## mtry RMSE Rsquared MAE ## 2 2.767412 0.8678962 2.238987 ## 6 2.673043 0.8756824 2.162140 ## 10 2.803810 0.8587926 2.288413 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was mtry = 6. The mtry argument is a hyperparameter which represents information that is supplied in the form of an argument prior to the call to the. These parameters might not be something one can intelligently set without some experimentation. 9.1 Parameters vs Hyperparameters Model parameters are things that are generated as part of the modeling process. They are the product or result of model fitting. These might be things like slope and intercept from a linear model - or coefficients. Hyperparameters have default values for various arguments but this does not mean that the defaults are appropriate for all cases. So with rf there is a hyperarameter called mtry that influences the outcome but is not necessarily something that we know how to optimally set. The mtry value is the number of variables that are randomly sampled at each tree split. If we had called the random Forest function without using caret we could supply a number of values for mtry to try to arrive at the “best” value to produce the lowest RMSE. library(randomForest) non_caret_rf &lt;- randomForest(mpg~., data= new_train, mtry=3, # This is the default importance=TRUE) # Check out the RMSE of the preditcions Metrics::rmse(new_test$mpg,predict(non_caret_rf,new_test)) ## [1] 1.903625 We could also write a loop to do this for multiple values of mtry. make_mtcars_rf &lt;- function(mtry=3) { my_rf &lt;- randomForest(mpg~., data = new_train, mtry = mtry, importance = TRUE) # Check out the predictions rmse_rf &lt;- Metrics::rmse(new_test$mpg,predict(my_rf,new_test)) return(rmse_rf) } The following will call the randomForest package 5 times. Starting with the first iteration, the value of mtry will be 3, the next time it will be 4, and so on until the last iteration where it will be 8. This is just an experiment to see if varying mtry will help minimize the RMSE of our model. We also have to be careful not to pick incorrect values for mtry so reading the help page for the randomForest package would be helpful. For now, let’s assume that what we are doing is okay. sapply(3:8,make_mtcars_rf) ## [1] 1.982152 2.084413 2.101195 2.124388 2.051716 2.184972 While this is fine, it would be nice if there were an easier way to handle this process. Besides, if we pick another method (e.g. the ranger function) then we have to deal with whatever arguments and hyperparameters that method requires. The above is an example of what we would have to do if we didn’t have something like caret to help us try out different values of the mtry hyperparameter. As we saw from our earlier work, the model caret_rf tried three different values of the mtry hyperparameter: caret_rf ## Random Forest ## ## 28 samples ## 10 predictors ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 28, 28, 28, 28, 28, 28, ... ## Resampling results across tuning parameters: ## ## mtry RMSE Rsquared MAE ## 2 2.767412 0.8678962 2.238987 ## 6 2.673043 0.8756824 2.162140 ## 10 2.803810 0.8587926 2.288413 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was mtry = 6. As a matter of fact we can plot this model and it will show us something interesting. In particular, we now see that using values less than 6 predictors / columns / features results in lower RMSE. If we use more, than the RMSE goes up. This is the power of methods that use hyperparamters. If we move through a number of values for the mtry then perhaps we can find the best value to get the best result. plot(caret_rf) Telling caret to use more values of mtry is possible. This will explicitly try all values from 1 - 10 inclusive. Note that the model gets built for each value of mtry. This causes the model building process to take longer than it normally would. caret_rf &lt;- train(mpg ~ ., data = new_train, method = &quot;rf&quot;, tuneLength = 10) ## note: only 9 unique complexity parameters in default grid. Truncating the grid to 9 . caret_rf$results ## mtry RMSE Rsquared MAE RMSESD RsquaredSD MAESD ## 1 2 2.654331 0.8487161 2.177264 1.0179793 0.09043523 0.7344729 ## 2 3 2.592350 0.8515155 2.112745 1.0215747 0.09291606 0.7504804 ## 3 4 2.546599 0.8536404 2.063396 0.9918813 0.08917800 0.7268040 ## 4 5 2.522124 0.8549801 2.040184 0.9875859 0.09233036 0.7462059 ## 5 6 2.560619 0.8462381 2.078882 0.9751577 0.09803020 0.7460211 ## 6 7 2.546294 0.8443172 2.065910 0.9712595 0.09897416 0.7483275 ## 7 8 2.584603 0.8390093 2.097366 0.9849969 0.10136125 0.7640514 ## 8 9 2.592787 0.8366038 2.109758 0.9985790 0.10237274 0.7735958 ## 9 10 2.610255 0.8337551 2.118641 1.0035417 0.10468172 0.7828991 Metrics::rmse(new_test$mpg, predict(caret_rf,new_test)) ## [1] 2.071873 It looks like a value of 4 will produce the lowest value for RMSE. plot(caret_rf) 9.2 Hyperparameter Tuning The process of finding the “right” values for these parameters is generally referred to as “hypermarket tuning”. Different values are supplied for each invocation of a method (as we did in the above example) to see the effect on the model. We might do this many times to arrive at the optimal parameter set to produce a model that offers the “best” explanatory and predictive power. Just to review - things like coefficients and residuals are parameters that are generated by a call to the lm function. They don’t actually exist until the function does some work. The hyperparameters are specific to whatever algorithm (and supporting R function) you are using. Concepts such as coefficients and intercept, however, are parameters that would be generated in this case by lm. More generally, what if we wanted to use other functions to do some predicting ? You can check this page for a list of caret supported methods along with any hyperparamters available for tuning. Obviously, if you know the underlying method you can refer to the help page for it to see what parameters exist. 9.2.1 Multiple Hyperparameters ? Let’s look at another method for random forests such as the ranger function which alleges to build trees very rapidly. We can easily call it via the train function. If you consult the reference manual for the caret implementation of ranger you will see that it supports three hyperparameters: mtry, splitrule, and min.node.size. caret_ranger &lt;- train(mpg ~ ., data = new_train, method = &quot;ranger&quot;) caret_ranger$results ## mtry min.node.size splitrule RMSE Rsquared MAE RMSESD RsquaredSD ## 1 2 5 variance 2.593815 0.8466324 2.066727 0.6209003 0.04837170 ## 2 2 5 extratrees 2.766129 0.8264357 2.242318 0.6277065 0.05777494 ## 3 6 5 variance 2.483004 0.8656660 1.919371 0.6014174 0.04497003 ## 4 6 5 extratrees 2.654512 0.8372522 2.111165 0.6095019 0.05395181 ## 5 10 5 variance 2.578429 0.8524675 1.990380 0.6556106 0.05489347 ## 6 10 5 extratrees 2.645518 0.8399955 2.080374 0.6240255 0.05368885 ## MAESD ## 1 0.4988512 ## 2 0.5171023 ## 3 0.4986984 ## 4 0.5167884 ## 5 0.5321517 ## 6 0.5299911 Metrics::rmse(new_test$mpg, predict(caret_ranger,new_test)) ## [1] 2.22648 And then we can plot these results to get a more intuitive view of the output. plot(caret_ranger) caret_ranger &lt;- train(mpg ~ ., data = new_train, method = &quot;ranger&quot;, tuneLength = 10) ## note: only 9 unique complexity parameters in default grid. Truncating the grid to 9 . caret_ranger$results ## mtry min.node.size splitrule RMSE Rsquared MAE RMSESD RsquaredSD ## 1 2 5 variance 2.535142 0.8705777 2.050717 0.7213109 0.08406100 ## 2 2 5 extratrees 2.733620 0.8449910 2.229738 0.7697078 0.09607066 ## 3 3 5 variance 2.459643 0.8792912 1.974891 0.7102015 0.08193415 ## 4 3 5 extratrees 2.697460 0.8492856 2.205555 0.7174634 0.09468130 ## 5 4 5 variance 2.413853 0.8811367 1.928837 0.6799630 0.08712652 ## 6 4 5 extratrees 2.664466 0.8534908 2.171931 0.7522685 0.09407159 ## 7 5 5 variance 2.396935 0.8826333 1.902248 0.6642884 0.08740512 ## 8 5 5 extratrees 2.646204 0.8562650 2.158095 0.6929833 0.09171305 ## 9 6 5 variance 2.403365 0.8830201 1.898374 0.6594777 0.08517967 ## 10 6 5 extratrees 2.614236 0.8584534 2.132242 0.7040789 0.09267746 ## 11 7 5 variance 2.414677 0.8798765 1.904939 0.6577313 0.08974818 ## 12 7 5 extratrees 2.632442 0.8594405 2.140217 0.7004326 0.09158392 ## 13 8 5 variance 2.430565 0.8782065 1.923389 0.6579862 0.09169315 ## 14 8 5 extratrees 2.618759 0.8590369 2.135648 0.6871433 0.09196265 ## 15 9 5 variance 2.449120 0.8742522 1.934580 0.6459582 0.08864581 ## 16 9 5 extratrees 2.604208 0.8594866 2.117187 0.6775269 0.09265309 ## 17 10 5 variance 2.487449 0.8686236 1.969850 0.6753278 0.09618672 ## 18 10 5 extratrees 2.604154 0.8607695 2.122085 0.6736732 0.08758355 ## MAESD ## 1 0.5480455 ## 2 0.6150415 ## 3 0.5414353 ## 4 0.5737425 ## 5 0.5282255 ## 6 0.6117592 ## 7 0.5140223 ## 8 0.5710811 ## 9 0.5101976 ## 10 0.5748181 ## 11 0.5246354 ## 12 0.5756883 ## 13 0.5161000 ## 14 0.5715926 ## 15 0.5073912 ## 16 0.5709619 ## 17 0.5192270 ## 18 0.5584563 Metrics::rmse(new_test$mpg, predict(caret_ranger,new_test)) ## [1] 2.085479 plot(caret_ranger) 9.2.2 Custom Tuning Grid So it’s possible to go even deeper when tuning hyperparameters. We can create a custom tuning grid instead of letting caret pick the values. Of course, using tuneLength is fine but the custom tuning grid allows for finer grained control. It’s easy, all we need to do is to specify our own “tuneGrid” and then pass it to train. MyGrid &lt;- data.frame( mtry = c(2, 4, 6), splitrule = &quot;variance&quot;, min.node.size = 4 ) # Now we supply it when calling train caret_ranger &lt;- train(mpg ~ ., data = new_train, method = &quot;ranger&quot;, tuneGrid = MyGrid) caret_ranger$results ## mtry splitrule min.node.size RMSE Rsquared MAE RMSESD RsquaredSD ## 1 2 variance 4 2.644805 0.8612962 2.187830 0.5518203 0.06821914 ## 2 4 variance 4 2.551675 0.8629956 2.056296 0.5058089 0.07012752 ## 3 6 variance 4 2.554660 0.8598010 2.049961 0.5235632 0.07389630 ## MAESD ## 1 0.4520530 ## 2 0.4366614 ## 3 0.4463890 Metrics::rmse(new_test$mpg, predict(caret_ranger,new_test)) ## [1] 1.964188 9.3 Using Validation Data Sets When building a model, we generate a training and test data set. We use the former to build a model and, if we are using something like the caret package, that process involves cross fold validation or bootstrap sampling to generate a good estimate for out-of-sample error. We then apply the model to the test data frame. If we are using a method that has hyperparamters then maybe we want an intermediate data set to help validate our ultimate choice of hyperparameters. By taking this approach we can still keep our test data set off to the side for later use with the trained and validated model. Using this idea doesn’t require us to do much beyond generating a third data set. "],
["picking-the-best-model.html", "Chapter 10 Picking The Best Model 10.1 An Example 10.2 More Comparisons 10.3 Using the resamples() function 10.4 Model Performance 10.5 Feature Selection", " Chapter 10 Picking The Best Model It’s always of interest to compare the results of one model to another (or even more) to determine the “best” model to share with some else. On the other hand, it’s easy to get carried away with trying out different models in an attempt to make performance improvements especially when the might only be marginally better. It’s also wise to pick methods that you know how to reasonably defend over those that you can’t. For example, picking a Neural Net model might result in better accuracy although if you are challenged on the results in some way, would you be able to address all concerns ? If a logistic regression model resulted in a comparable result then you might should stick with that result since it’s a well-known method that few would question. 10.1 An Example As an example, let’s go back to the linear modeling example back in the section wherein we introduced the caret package. set.seed(123) # Make this example reproducible idx &lt;- createDataPartition(mtcars$mpg, p = .8, list = FALSE, times = 1) head(idx) ## Resample1 ## [1,] 1 ## [2,] 3 ## [3,] 4 ## [4,] 5 ## [5,] 6 ## [6,] 8 example_Train &lt;- mtcars[ idx,] example_Test &lt;- mtcars[-idx,] # control &lt;- trainControl(method = &quot;cv&quot;, # Cross Fold number = 5, # 5 Folds verboseIter = FALSE) # Verbose # Train the model set.seed(123) # Make this example reproducible my_lm &lt;- train(mpg ~ wt, example_Train, method = &quot;lm&quot;, preProc = c(&quot;center&quot;,&quot;scale&quot;), trControl = control) my_lm ## Linear Regression ## ## 28 samples ## 1 predictor ## ## Pre-processing: centered (1), scaled (1) ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 22, 22, 23, 22, 23 ## Resampling results: ## ## RMSE Rsquared MAE ## 2.674278 0.8128595 2.255329 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE plot(mpg~wt,data=example_Train,main=&quot;Train Data&quot;) abline(lm_fit$finalModel,lty=2) So does this model perform better than a model built using a support vector machine ? It’s easy to generate such a model by reusing much of the same information from above. This is arguably one of the best features of the caret package as it helps us execute any number of models and then assess their performance on new data. Let’s look at our models thus far. In fact, it’s so easy to generate them with caret, we’ll just make them here again. Let’s set a common trainControl list. We’ll use the Train and Test sets from above. #set.seed(123) # Make this example reproducible my_svm &lt;- train( mpg ~ wt, example_Train, method = &quot;svmRadial&quot;, preProc = c(&quot;center&quot;,&quot;scale&quot;), trControl = control ) my_svm ## Support Vector Machines with Radial Basis Function Kernel ## ## 28 samples ## 1 predictor ## ## Pre-processing: centered (1), scaled (1) ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 23, 21, 23, 22, 23 ## Resampling results across tuning parameters: ## ## C RMSE Rsquared MAE ## 0.25 3.827559 0.7700785 2.907836 ## 0.50 3.127704 0.8035070 2.418092 ## 1.00 2.962344 0.7735412 2.441182 ## ## Tuning parameter &#39;sigma&#39; was held constant at a value of 4.935503 ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were sigma = 4.935503 and C = 1. So let’s plot the training data as well as the resulting regression line (in the color black) coming from the lm object. We’ll also plot the results from the Support Vector Machine predictions on the same graph (in red). From this plot it appears that the Support Vector Machine does a better job of “following” the actual data - at least for the training data. We might even be able to improve the SVM performance if we tune the hyperparameters but the default without tuning seems better than the lm. It’s this type of observation that leads one to consider if there are yet other methods that would result in even better results. plot(mpg~wt,data=example_Train,main=&quot;Train Data&quot;) abline(lm_fit$finalModel) grid() svm_preds &lt;- predict(my_svm,example_Train) pdf &lt;- data.frame(cbind(Train$wt,svm_preds)) points(svm_preds~V1,data=pdf[order(pdf$V1),],col=&quot;red&quot;,pch=4,type=&quot;l&quot;) legend(&quot;topright&quot;,c(&quot;lm&quot;,&quot;svm&quot;),col=c(&quot;black&quot;,&quot;red&quot;),lty=1,cex=1) In terms of the RMSE for the Test set predictions, which method is better (i.e. the lowest RMSE) ? lm_preds &lt;- predict(my_lm,example_Train) svm_preds &lt;- predict(my_svm,example_Train) # cat(&quot;RMSE for lm is: &quot;,Metrics::rmse(example_Test$mpg,lm_preds),&quot;\\n&quot;) ## RMSE for lm is: 9.656877 cat(&quot;RMSE for smv is: &quot;,Metrics::rmse(example_Test$mpg,svm_preds),&quot;\\n&quot;) ## RMSE for smv is: 9.148044 10.2 More Comparisons Let’s keep going. We’ll use some more methods to see how they perform on the same data. The cool thing about caret is that we can reuse the same control object and seeds to facilitate reproducibility. We’ll pick two other methods in addition to the ones we have to see how we can compare them all. In reality, this would be just the beginning of the process, not the end since if we pick a method with hyperparameters then we would want to tune those. control &lt;- trainControl(method = &quot;cv&quot;, # Cross Fold number = 5, # 5 Folds verboseIter = FALSE) # Verbose Note that we will be predicting the MPG value as a function of all variables in the data frame. # Train the lm model set.seed(123) comp_mod_lm &lt;- train(mpg ~ ., example_Train, method = &quot;lm&quot;, preProc = c(&quot;center&quot;,&quot;scale&quot;), trControl = control) # Train the SVM Radial Model set.seed(123) comp_mod_svm &lt;- train(mpg ~ ., example_Train, method = &quot;svmRadial&quot;, preProc = c(&quot;center&quot;,&quot;scale&quot;), trControl = control) # Train the Lasso and Elastic-Net Regularized Generalized Linear Models set.seed(123) comp_mod_glmnet &lt;- train(mpg ~ ., example_Train, method = &quot;glmnet&quot;, preProc = c(&quot;center&quot;,&quot;scale&quot;), trControl = control) # Train the Random Forest Model set.seed(123) comp_mod_ranger &lt;- train(mpg ~ ., example_Train, method = &quot;ranger&quot;, preProc = c(&quot;center&quot;,&quot;scale&quot;), trControl = control) 10.3 Using the resamples() function Now, here comes the “magic”. Because we built four different modeling objects on the same data set and seeds we can now use the resamples function to collect, analyze, and visualize a set of results. This is pretty powerful. results &lt;- resamples(list(LM = comp_mod_lm, SVM = comp_mod_svm, GLMNET = comp_mod_glmnet, RANGER = comp_mod_ranger)) results ## ## Call: ## resamples.default(x = list(LM = comp_mod_lm, SVM = comp_mod_svm, GLMNET ## = comp_mod_glmnet, RANGER = comp_mod_ranger)) ## ## Models: LM, SVM, GLMNET, RANGER ## Number of resamples: 5 ## Performance metrics: MAE, RMSE, Rsquared ## Time estimates for: everything, final model fit dotplot(results) 10.4 Model Performance Of course, we can now use the Test data frame to see how the RMSE looks on the holdout data frame. Metrics::rmse(example_Test$mpg,predict(comp_mod_lm,example_Test)) ## [1] 4.808981 Metrics::rmse(example_Test$mpg,predict(comp_mod_svm,example_Test)) ## [1] 3.65783 Metrics::rmse(example_Test$mpg,predict(comp_mod_svm,example_Test)) ## [1] 3.65783 Metrics::rmse(example_Test$mpg,predict(comp_mod_ranger,example_Test)) ## [1] 2.288412 Another way to look at these models is to use the diff function. (difs &lt;- diff(results)) ## ## Call: ## diff.resamples(x = results) ## ## Models: LM, SVM, GLMNET, RANGER ## Metrics: MAE, RMSE, Rsquared ## Number of differences: 6 ## p-value adjustment: bonferroni # Get the summary summary(difs) ## ## Call: ## summary.diff.resamples(object = difs) ## ## p-value adjustment: bonferroni ## Upper diagonal: estimates of the difference ## Lower diagonal: p-value for H0: difference = 0 ## ## MAE ## LM SVM GLMNET RANGER ## LM -0.52020 0.40699 0.36900 ## SVM 0.67373 0.92719 0.88921 ## GLMNET 1.00000 0.14273 -0.03799 ## RANGER 1.00000 0.04069 1.00000 ## ## RMSE ## LM SVM GLMNET RANGER ## LM -0.51910 0.44889 0.44274 ## SVM 0.78455 0.96799 0.96184 ## GLMNET 0.58495 0.03576 -0.00615 ## RANGER 0.95365 0.03587 1.00000 ## ## Rsquared ## LM SVM GLMNET RANGER ## LM -0.02850 -0.13294 -0.15787 ## SVM 1.0000 -0.10444 -0.12937 ## GLMNET 0.6034 0.6498 -0.02493 ## RANGER 0.4937 0.3675 0.5323 dotplot(difs) 10.5 Feature Selection Features are the columns in your data set. Up until now we have not been concerned with the formula being specified choosing rather to focus on how to run models with the caret package. However, knowing how to select the “best” subset of features is important since an overspcified formula might result in very long training times and, even then, it might not be that good of a model for predicting out of sample error. Of course, various methods have ways to deal with this problem. For example, Stepwise regression is one way to look at combinations of predictor variables to arrive at the optimal feature set according to some score (e.g. AIC, BIC). This process is implemented recursively. However, none of this should be a substitute for solid intuition about the data or knowing how features vary with each other (if at all). Still, packages such as caret have ways to assist with feature selection. 10.5.1 Recursive Feature Elimination The general idea with this approach is to build models using combinations of predictors to arrive at the best model according to some metric such as RMSE. Some predictors might be discarded along the way resulting in a “leaner” feature set that would then hopefully be easier to defend than a more complex or fully specified feature set. There is no free lunch here in that blindly accepting the features handed to you by a recursive or automatic method should not be considered authoritative especially if you have a reason to believe that some key feature has been excluded. Many people, however, like to use these methods as a starting point. You would still need to review the diagnostics associated with a final model to determine if it is statistically sound. According to the caret documentation there are a large number of supported models that contain some form of embedded or built-in feature selection. Such functions are doing you a favor (or not) by showing you the importance of contributing features. 10.5.1.1 An Example Let’s work with the lm function again to see if we can find some interesting features using some caret functions. The main function for Recursive Feature Elimination is rfe which, like the train function, accepts a control object to help guide the process. Here we are telling the rfe function to use some helper functions to assess predictors. We don’t need to pass it a model - it handles these things under the hood. In this case we’ll use 10 Fold Cross Validation. set.seed(123) control &lt;- rfeControl(functions=lmFuncs, method=&quot;cv&quot;,number=10) results &lt;- rfe(mtcars[,2:11], # Predictor features mtcars[,1], # Predicted features - mpg sizes=c(1:5), # pick groups of predictors 1-5 rfeControl=control) results ## ## Recursive feature selection ## ## Outer resampling method: Cross-Validated (10 fold) ## ## Resampling performance over subset size: ## ## Variables RMSE Rsquared MAE RMSESD RsquaredSD MAESD Selected ## 1 3.113 0.8486 2.762 1.878 0.2403 1.811 ## 2 3.134 0.8392 2.798 1.650 0.2689 1.568 ## 3 2.700 0.8872 2.421 1.366 0.1543 1.203 * ## 4 2.916 0.8655 2.512 1.444 0.1664 1.239 ## 5 2.992 0.8884 2.633 1.438 0.1461 1.253 ## 10 3.391 0.8864 3.055 1.447 0.1614 1.283 ## ## The top 3 variables (out of 3): ## wt, am, drat plot(results,type=c(&quot;o&quot;,&quot;g&quot;)) What we get back is some idea about the important features. We could then build a model with caret that uses only these features to see if the suggested RMSE value mentioned in the rfe process matches. 10.5.2 Redundant Feature Removal The caret package has some functions that can help us identify highly correlated variables that might be a candidates for removal prior to use in building a model. Let’s go back to the mtcars data set as it exists by default. One of the variables that is highly correlated with others is mpg Since that is the one we are trying to predict, we’ll keep it around. correlations &lt;- cor(mtcars[,-1]) # Find all correlated variables above .75 (highcorr &lt;- findCorrelation(correlations, cutoff=.75)) ## [1] 1 2 9 # We might want to remove these from the data frame before modeling mtcars[,-1][,highcorr] ## cyl disp gear ## Mazda RX4 6 160.0 4 ## Mazda RX4 Wag 6 160.0 4 ## Datsun 710 4 108.0 4 ## Hornet 4 Drive 6 258.0 3 ## Hornet Sportabout 8 360.0 3 ## Valiant 6 225.0 3 ## Duster 360 8 360.0 3 ## Merc 240D 4 146.7 4 ## Merc 230 4 140.8 4 ## Merc 280 6 167.6 4 ## Merc 280C 6 167.6 4 ## Merc 450SE 8 275.8 3 ## Merc 450SL 8 275.8 3 ## Merc 450SLC 8 275.8 3 ## Cadillac Fleetwood 8 472.0 3 ## Lincoln Continental 8 460.0 3 ## Chrysler Imperial 8 440.0 3 ## Fiat 128 4 78.7 4 ## Honda Civic 4 75.7 4 ## Toyota Corolla 4 71.1 4 ## Toyota Corona 4 120.1 3 ## Dodge Challenger 8 318.0 3 ## AMC Javelin 8 304.0 3 ## Camaro Z28 8 350.0 3 ## Pontiac Firebird 8 400.0 3 ## Fiat X1-9 4 79.0 4 ## Porsche 914-2 4 120.3 5 ## Lotus Europa 4 95.1 5 ## Ford Pantera L 8 351.0 5 ## Ferrari Dino 6 145.0 5 ## Maserati Bora 8 301.0 5 ## Volvo 142E 4 121.0 4 "],
["data-pre-processing.html", "Chapter 11 Data Pre Processing 11.1 Types of Pre Processing 11.2 Missing Values 11.3 Scaling 11.4 Low Variance Variables 11.5 PCA - Principal Components Analysis 11.6 Order of Pre-Processing 11.7 Identifying Redundant Features 11.8 Handling Categories 11.9 Binning", " Chapter 11 Data Pre Processing Data rarely arrives in a form that is directly suitable for use with a modeling method. There are a number of considerations to make such as how to handle missing data, highly correlated variables, and class imbalances - some categories are over or under represented. Additionally, some variables, also known as “features”, will require transformation or will need to be used to create new variables. 11.1 Types of Pre Processing Consider the case where the measured data (the numeric data) might be on different scales (e.g. height vs weight). This might result in the need to scale and center the data. Some methods take this into consideration whereas others do not. Suffice it to say that data prep can be an ongoing process that requires a number of experiments before arriving at the best form of data. 11.2 Missing Values This is a frequent situation in real life. Think of patients checking in for clinic visits over time. Sometimes they come for their appointments, sometimes they don’t. Sometimes when they do come, their information has changed or some diagnostic test is repeated with a new result which is entered or not. Or, whomever maintains the patient database, decides to add in some new variables to measure for all patients moving forward. This means that all existing patients will have missing values for those new variables. To see how this manifests practically in predictive learning consider the following version of the mtcars data frame which has some missing values: library(readr) url &lt;- &quot;https://raw.githubusercontent.com/steviep42/utilities/master/data/mtcars_na.csv&quot; mtcars_na &lt;- read.csv(url, stringsAsFactors = FALSE) 11.2.1 Finding Rows with Missing Data Ar first glance it looks like all features have valid variable values but we can look for missing values which, in R, are indicated by NA Base R provides a number of commands to do this. First let’s see how many rows there are in the data frame. nrow(mtcars_na) ## [1] 32 Now let’s see how many rows have at least one column with a missing value. So we have eight rows in the data frame that contain one or more missing values. sum(complete.cases(mtcars_na)) ## [1] 24 11.2.2 Finding Columns With Missing Data What columns have missing values ? Here we leverage the use of the apply family of functions along with the ability to create anonymous functions on the fly. Both R and Python provide this capability. We see that the wt column has three missing values and the carb feature has six missing values. sapply(mtcars_na,function(x) sum(is.na(x))) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 0 0 0 0 0 3 0 0 0 0 6 If we actually wanted to see all rows with missing values: mtcars_na[!complete.cases(mtcars_na),] ## mpg cyl disp hp drat wt qsec vs am gear carb ## 2 21.0 6 160.0 110 3.90 NA 17.02 0 1 4 4 ## 9 22.8 4 140.8 95 3.92 NA 22.90 1 0 4 2 ## 10 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 NA ## 12 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 NA ## 19 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 NA ## 20 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 NA ## 23 15.2 8 304.0 150 3.15 NA 17.30 0 0 3 NA ## 28 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 NA We might also consider using the DataExplorer package to help us identify the “missingness” of the data: DataExplorer::plot_missing(mtcars_na) What do we do with these ? It depends on a number of things. How hard is it to get this type of data ? If it’s rare information then we probably want to keep as much of the data as possible. In fact, if most of the data in a given row is present maybe one strategy is to tell whatever modeling method we use to ignore the missing values (na.rm = TRUE). Some methods might do this by default without you even asking. However, if we have plenty of data we could just filter out any row from the data frame that contains any missing values. In this case we would lose eight rows of data. This is low stakes data but if this were rare or hard to obtain information then we wouldn’t want to do this. mtcars_no_na &lt;- mtcars_na[complete.cases(mtcars_na),] nrow(mtcars_no_na) ## [1] 24 11.2.3 Use the Median Approach What could we do ? Well we could keep all rows even if they contains NAs and then use imputation methods to supply values for the missing information. There are R packages that do this but one quick way to do this without going that route is to replace the missing value in the wt column with the median value for the entire column. Using median is appropriate when the missing values are of the “missing at random” variety. There might some bias in the data that introduces a “not at random” situation. We’ll look at that case momentarily. Let’s look at a boxplot of the wt feature. boxplot(mtcars_na$wt, na.rm=TRUE, main=&quot;Distribution of the wt feature&quot;) grid() To make the substitution would involve the following. First we need to find out which row numbers have missing values for the wt feature. (missing_wt_indices &lt;- which(is.na(mtcars_na$wt))) ## [1] 2 9 23 Use this information with the data frame bracket notation see the rows where the NAs occur for the wt feature mtcars_na[missing_wt_indices,] ## mpg cyl disp hp drat wt qsec vs am gear carb ## 2 21.0 6 160.0 110 3.90 NA 17.02 0 1 4 4 ## 9 22.8 4 140.8 95 3.92 NA 22.90 1 0 4 2 ## 23 15.2 8 304.0 150 3.15 NA 17.30 0 0 3 NA Now do the replacement mtcars_na[missing_wt_indices,]$wt &lt;- median(mtcars_na$wt,na.rm=TRUE) Verify that the replacement was done successfully. If so, then we should see the value of 3.44 in place of the previous NA values. mtcars_na[missing_wt_indices,] ## mpg cyl disp hp drat wt qsec vs am gear carb ## 2 21.0 6 160.0 110 3.90 3.44 17.02 0 1 4 4 ## 9 22.8 4 140.8 95 3.92 3.44 22.90 1 0 4 2 ## 23 15.2 8 304.0 150 3.15 3.44 17.30 0 0 3 NA 11.2.4 Package-based Approach This seems like a lot of work and maybe it is if you aren’t up to date with your R skills although, conceptually, this is straightforward and simple. The Hmisc package provides an easy way to do this: #library(Hmisc) # Reload the version of mtcars with missing values url &lt;- &quot;https://raw.githubusercontent.com/steviep42/utilities/master/data/mtcars_na.csv&quot; mtcars_na &lt;- read.csv(url, strip.white = FALSE) The Hmisc package provides and impute function to do the work for us. Check it out. Notice how it finds the rows for which the wt feature is missing. Hmisc::impute(mtcars_na$wt, median) ## 1 2 3 4 5 6 7 8 9 10 11 ## 2.620 3.440* 2.320 3.215 3.440 3.460 3.570 3.190 3.440* 3.440 3.440 ## 12 13 14 15 16 17 18 19 20 21 22 ## 4.070 3.730 3.780 5.250 5.424 5.345 2.200 1.615 1.835 2.465 3.520 ## 23 24 25 26 27 28 29 30 31 32 ## 3.440* 3.840 3.845 1.935 2.140 1.513 3.170 2.770 3.570 2.780 To do the replacement is straightforward. mtcars_na$wt &lt;- Hmisc::impute(mtcars_na$wt,median) 11.2.5 Using caret Another imputation approach is to use the K-Nearest Neighbors method to find observations that are similar to the ones that contain missing data. The missing values can then be filled using information from the most similar observations. We won’t go into that choosing rather to use the convenience offered by the caret package to help us. # Make sure caret is loaded # library(caret) So if we choose to use caret we can use the preProcess function to signal our intent to use imputation - in this case the K-Nearest Neighbors technique. KNN imputation is particularly useful for dealing with the “not at random” situation where there could be bias in the way missing values occur. This approach looks at similar observations to those containing missing values which means that it will attempt to fill in missing values as a function of a number of variables as opposed to just one. One subtlety here is that caret requires us to use an alternative to the formula interface (e.g. mpg ~ .) approach when using the train function - at least the version of caret that I am currently using. # Get a fresh copy of the mtcars_na data frame mtcars_na &lt;- read.csv(url,stringsAsFactors = FALSE) # Set the seed for reproducibility set.seed(123) # Get the indices for a 70/30 split train_idx &lt;- createDataPartition(mtcars_na$mpg, p=.70, list = FALSE) # Split into a test / train pair train &lt;- mtcars[ train_idx,] test &lt;- mtcars[-train_idx,] # Note how we specify knnImpute. Another other option # includes medianImpute X &lt;- train[,-1] # Use Every column EXCEPT mpg Y &lt;- train$mpg # This is what we want to predict lmFit &lt;- train(X,Y, method = &quot;lm&quot;, metric = &quot;RMSE&quot;, preProcess = &quot;knnImpute&quot;) ## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit may ## be misleading ## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit may ## be misleading ## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit may ## be misleading ## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit may ## be misleading lmFit ## Linear Regression ## ## 24 samples ## 10 predictors ## ## Pre-processing: nearest neighbor imputation (10), centered (10), scaled (10) ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 24, 24, 24, 24, 24, 24, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 8.723271 0.3681689 6.47164 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE # See the RMSE for the test data preds &lt;- predict(lmFit, test) Metrics::rmse(test$mpg,preds) ## [1] 3.98902 11.3 Scaling In terms of what methods benefit (or require) you to scale data prior to use, consider that any method that uses the idea of “distance” will require this. This helps address the situation wherein one the size and range of one feature might overshadow another. For example, look at the range of features in the mtcars dataframe. Not only are the variables on different scales (e.g. MPG vs Weight vs Horse Power), a feature such as displacement might over influence a distance calculation when compared to qsec. summary(mtcars) ## mpg cyl disp hp drat ## Min. :10.40 Min. :4.000 Min. : 71.1 Min. : 52.0 Min. :2.760 ## 1st Qu.:15.43 1st Qu.:4.000 1st Qu.:120.8 1st Qu.: 96.5 1st Qu.:3.080 ## Median :19.20 Median :6.000 Median :196.3 Median :123.0 Median :3.695 ## Mean :20.09 Mean :6.188 Mean :230.7 Mean :146.7 Mean :3.597 ## 3rd Qu.:22.80 3rd Qu.:8.000 3rd Qu.:326.0 3rd Qu.:180.0 3rd Qu.:3.920 ## Max. :33.90 Max. :8.000 Max. :472.0 Max. :335.0 Max. :4.930 ## wt qsec vs am gear ## Min. :1.513 Min. :14.50 Min. :0.0000 Min. :0.0000 Min. :3.000 ## 1st Qu.:2.581 1st Qu.:16.89 1st Qu.:0.0000 1st Qu.:0.0000 1st Qu.:3.000 ## Median :3.325 Median :17.71 Median :0.0000 Median :0.0000 Median :4.000 ## Mean :3.217 Mean :17.85 Mean :0.4375 Mean :0.4062 Mean :3.688 ## 3rd Qu.:3.610 3rd Qu.:18.90 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:4.000 ## Max. :5.424 Max. :22.90 Max. :1.0000 Max. :1.0000 Max. :5.000 ## carb ## Min. :1.000 ## 1st Qu.:2.000 ## Median :2.000 ## Mean :2.812 ## 3rd Qu.:4.000 ## Max. :8.000 11.3.1 Methods That Benefit From Scaling The following approaches benefit from scaling: Linear/non-linear regression, logistic regression, KNN, SVM, Neural Networks, clustering algorithms like k-means clustering. Methods that employ PCA and dimensionality reduction should use scaled data. In R and Python, some of the individual functions might have arguments to activate the scaling as part of the process. 11.3.2 Methods That Do Not Require Scaling Methods that don’t require scaling (or whose results don’t rely upon it) include rule-based algorithms such as Decision trees and more generally CART - Random Forests, Gradient Boosted Decision Even if you scale the data the relative relationships will be preserved post scaling so the decision to split a tree won’t be impacted. 11.3.3 How To Scale You could do your own scaling and centering which might be helpful to understand what is going on. First, when we say “scaling” we typically mean “centering” and “scaling”. 11.3.3.1 Centering Take a vector (or column) of numeric data and find the mean. Then subtract the computed mean value from each element of the vector / column. We will use the wt column from mtcars as an example. (centered &lt;- mtcars$wt - mean(mtcars$wt)) ## [1] -0.59725 -0.34225 -0.89725 -0.00225 0.22275 0.24275 0.35275 -0.02725 ## [9] -0.06725 0.22275 0.22275 0.85275 0.51275 0.56275 2.03275 2.20675 ## [17] 2.12775 -1.01725 -1.60225 -1.38225 -0.75225 0.30275 0.21775 0.62275 ## [25] 0.62775 -1.28225 -1.07725 -1.70425 -0.04725 -0.44725 0.35275 -0.43725 #center &lt;- apply(mtcars,2,function(x) (x - mean(x))) 11.3.3.2 Scaling This step involves taking the standard deviation of the vector / column. Then divide each value in the vector / column by this computed standard deviation. We’ll process the centered data from the previous computation. (scaled &lt;- centered/sd(centered)) ## [1] -0.610399567 -0.349785269 -0.917004624 -0.002299538 0.227654255 0.248094592 ## [7] 0.360516446 -0.027849959 -0.068730634 0.227654255 0.227654255 0.871524874 ## [13] 0.524039143 0.575139986 2.077504765 2.255335698 2.174596366 -1.039646647 ## [19] -1.637526508 -1.412682800 -0.768812180 0.309415603 0.222544170 0.636460997 ## [25] 0.641571082 -1.310481114 -1.100967659 -1.741772228 -0.048290296 -0.457097039 ## [31] 0.360516446 -0.446876870 11.3.3.3 Scaling a Data frame We could continue to do this by hand centered &lt;- apply(mtcars,2,function(x) (x - mean(x))) scaled &lt;- apply(centered,2,function(x) x/sd(x)) However, there is a function in R called scale which will do this for us. It has the added benefit of providing some attributes in the output that we could later use to de-scale the scaled data. This is useful if we build a model with scaled data because the predictions will be in terms of the scaled data which might not make sense to a third party. So you would then need to scale the predictions back into the units of the original data. Here is how the scale function works: (smtcars &lt;- scale(mtcars)) ## mpg cyl disp hp drat ## Mazda RX4 0.15088482 -0.1049878 -0.57061982 -0.53509284 0.56751369 ## Mazda RX4 Wag 0.15088482 -0.1049878 -0.57061982 -0.53509284 0.56751369 ## Datsun 710 0.44954345 -1.2248578 -0.99018209 -0.78304046 0.47399959 ## Hornet 4 Drive 0.21725341 -0.1049878 0.22009369 -0.53509284 -0.96611753 ## Hornet Sportabout -0.23073453 1.0148821 1.04308123 0.41294217 -0.83519779 ## Valiant -0.33028740 -0.1049878 -0.04616698 -0.60801861 -1.56460776 ## Duster 360 -0.96078893 1.0148821 1.04308123 1.43390296 -0.72298087 ## Merc 240D 0.71501778 -1.2248578 -0.67793094 -1.23518023 0.17475447 ## Merc 230 0.44954345 -1.2248578 -0.72553512 -0.75387015 0.60491932 ## Merc 280 -0.14777380 -0.1049878 -0.50929918 -0.34548584 0.60491932 ## Merc 280C -0.38006384 -0.1049878 -0.50929918 -0.34548584 0.60491932 ## Merc 450SE -0.61235388 1.0148821 0.36371309 0.48586794 -0.98482035 ## Merc 450SL -0.46302456 1.0148821 0.36371309 0.48586794 -0.98482035 ## Merc 450SLC -0.81145962 1.0148821 0.36371309 0.48586794 -0.98482035 ## Cadillac Fleetwood -1.60788262 1.0148821 1.94675381 0.85049680 -1.24665983 ## Lincoln Continental -1.60788262 1.0148821 1.84993175 0.99634834 -1.11574009 ## Chrysler Imperial -0.89442035 1.0148821 1.68856165 1.21512565 -0.68557523 ## Fiat 128 2.04238943 -1.2248578 -1.22658929 -1.17683962 0.90416444 ## Honda Civic 1.71054652 -1.2248578 -1.25079481 -1.38103178 2.49390411 ## Toyota Corolla 2.29127162 -1.2248578 -1.28790993 -1.19142477 1.16600392 ## Toyota Corona 0.23384555 -1.2248578 -0.89255318 -0.72469984 0.19345729 ## Dodge Challenger -0.76168319 1.0148821 0.70420401 0.04831332 -1.56460776 ## AMC Javelin -0.81145962 1.0148821 0.59124494 0.04831332 -0.83519779 ## Camaro Z28 -1.12671039 1.0148821 0.96239618 1.43390296 0.24956575 ## Pontiac Firebird -0.14777380 1.0148821 1.36582144 0.41294217 -0.96611753 ## Fiat X1-9 1.19619000 -1.2248578 -1.22416874 -1.17683962 0.90416444 ## Porsche 914-2 0.98049211 -1.2248578 -0.89093948 -0.81221077 1.55876313 ## Lotus Europa 1.71054652 -1.2248578 -1.09426581 -0.49133738 0.32437703 ## Ford Pantera L -0.71190675 1.0148821 0.97046468 1.71102089 1.16600392 ## Ferrari Dino -0.06481307 -0.1049878 -0.69164740 0.41294217 0.04383473 ## Maserati Bora -0.84464392 1.0148821 0.56703942 2.74656682 -0.10578782 ## Volvo 142E 0.21725341 -1.2248578 -0.88529152 -0.54967799 0.96027290 ## wt qsec vs am gear ## Mazda RX4 -0.610399567 -0.77716515 -0.8680278 1.1899014 0.4235542 ## Mazda RX4 Wag -0.349785269 -0.46378082 -0.8680278 1.1899014 0.4235542 ## Datsun 710 -0.917004624 0.42600682 1.1160357 1.1899014 0.4235542 ## Hornet 4 Drive -0.002299538 0.89048716 1.1160357 -0.8141431 -0.9318192 ## Hornet Sportabout 0.227654255 -0.46378082 -0.8680278 -0.8141431 -0.9318192 ## Valiant 0.248094592 1.32698675 1.1160357 -0.8141431 -0.9318192 ## Duster 360 0.360516446 -1.12412636 -0.8680278 -0.8141431 -0.9318192 ## Merc 240D -0.027849959 1.20387148 1.1160357 -0.8141431 0.4235542 ## Merc 230 -0.068730634 2.82675459 1.1160357 -0.8141431 0.4235542 ## Merc 280 0.227654255 0.25252621 1.1160357 -0.8141431 0.4235542 ## Merc 280C 0.227654255 0.58829513 1.1160357 -0.8141431 0.4235542 ## Merc 450SE 0.871524874 -0.25112717 -0.8680278 -0.8141431 -0.9318192 ## Merc 450SL 0.524039143 -0.13920420 -0.8680278 -0.8141431 -0.9318192 ## Merc 450SLC 0.575139986 0.08464175 -0.8680278 -0.8141431 -0.9318192 ## Cadillac Fleetwood 2.077504765 0.07344945 -0.8680278 -0.8141431 -0.9318192 ## Lincoln Continental 2.255335698 -0.01608893 -0.8680278 -0.8141431 -0.9318192 ## Chrysler Imperial 2.174596366 -0.23993487 -0.8680278 -0.8141431 -0.9318192 ## Fiat 128 -1.039646647 0.90727560 1.1160357 1.1899014 0.4235542 ## Honda Civic -1.637526508 0.37564148 1.1160357 1.1899014 0.4235542 ## Toyota Corolla -1.412682800 1.14790999 1.1160357 1.1899014 0.4235542 ## Toyota Corona -0.768812180 1.20946763 1.1160357 -0.8141431 -0.9318192 ## Dodge Challenger 0.309415603 -0.54772305 -0.8680278 -0.8141431 -0.9318192 ## AMC Javelin 0.222544170 -0.30708866 -0.8680278 -0.8141431 -0.9318192 ## Camaro Z28 0.636460997 -1.36476075 -0.8680278 -0.8141431 -0.9318192 ## Pontiac Firebird 0.641571082 -0.44699237 -0.8680278 -0.8141431 -0.9318192 ## Fiat X1-9 -1.310481114 0.58829513 1.1160357 1.1899014 0.4235542 ## Porsche 914-2 -1.100967659 -0.64285758 -0.8680278 1.1899014 1.7789276 ## Lotus Europa -1.741772228 -0.53093460 1.1160357 1.1899014 1.7789276 ## Ford Pantera L -0.048290296 -1.87401028 -0.8680278 1.1899014 1.7789276 ## Ferrari Dino -0.457097039 -1.31439542 -0.8680278 1.1899014 1.7789276 ## Maserati Bora 0.360516446 -1.81804880 -0.8680278 1.1899014 1.7789276 ## Volvo 142E -0.446876870 0.42041067 1.1160357 1.1899014 0.4235542 ## carb ## Mazda RX4 0.7352031 ## Mazda RX4 Wag 0.7352031 ## Datsun 710 -1.1221521 ## Hornet 4 Drive -1.1221521 ## Hornet Sportabout -0.5030337 ## Valiant -1.1221521 ## Duster 360 0.7352031 ## Merc 240D -0.5030337 ## Merc 230 -0.5030337 ## Merc 280 0.7352031 ## Merc 280C 0.7352031 ## Merc 450SE 0.1160847 ## Merc 450SL 0.1160847 ## Merc 450SLC 0.1160847 ## Cadillac Fleetwood 0.7352031 ## Lincoln Continental 0.7352031 ## Chrysler Imperial 0.7352031 ## Fiat 128 -1.1221521 ## Honda Civic -0.5030337 ## Toyota Corolla -1.1221521 ## Toyota Corona -1.1221521 ## Dodge Challenger -0.5030337 ## AMC Javelin -0.5030337 ## Camaro Z28 0.7352031 ## Pontiac Firebird -0.5030337 ## Fiat X1-9 -1.1221521 ## Porsche 914-2 -0.5030337 ## Lotus Europa -0.5030337 ## Ford Pantera L 0.7352031 ## Ferrari Dino 1.9734398 ## Maserati Bora 3.2116766 ## Volvo 142E -0.5030337 ## attr(,&quot;scaled:center&quot;) ## mpg cyl disp hp drat wt qsec ## 20.090625 6.187500 230.721875 146.687500 3.596563 3.217250 17.848750 ## vs am gear carb ## 0.437500 0.406250 3.687500 2.812500 ## attr(,&quot;scaled:scale&quot;) ## mpg cyl disp hp drat wt ## 6.0269481 1.7859216 123.9386938 68.5628685 0.5346787 0.9784574 ## qsec vs am gear carb ## 1.7869432 0.5040161 0.4989909 0.7378041 1.6152000 The last rows make reference to attributes which can be considered as “meta information”. The presence of this information does not prevent you from using the scaled data for other operations. But if you needed to de-scaled the data you would need the standard deviation for each column and the mean of each column to reverse the centering and scaling process. That info is stashed in the attributes: attr(smtcars,&quot;scaled:scale&quot;) ## mpg cyl disp hp drat wt ## 6.0269481 1.7859216 123.9386938 68.5628685 0.5346787 0.9784574 ## qsec vs am gear carb ## 1.7869432 0.5040161 0.4989909 0.7378041 1.6152000 attr(smtcars,&quot;scaled:center&quot;) ## mpg cyl disp hp drat wt qsec ## 20.090625 6.187500 230.721875 146.687500 3.596563 3.217250 17.848750 ## vs am gear carb ## 0.437500 0.406250 3.687500 2.812500 If we wanted to de-scaled the data frame that we just scaled we could something like this: unscaledmt &lt;- sweep(smtcars,2, attr(smtcars,&quot;scaled:scale&quot;),&quot;*&quot;) uncentermt &lt;- sweep(unscaledmt,2, attr(smtcars,&quot;scaled:center&quot;),&quot;+&quot;) # Compare them uncentermt[1:3,] ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 mtcars[1:3,] ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Ugh. What a pain ! But, if we wanted to use the scaled data to build a model, here is what we would do: smtcars &lt;- scale(mtcars) mtcars_center_scaled &lt;- data.frame(smtcars) myLm &lt;- lm(mpg~., data = mtcars_center_scaled) # Do some predicting. preds &lt;- predict(myLm,mtcars_center_scaled) # The results are scaled. How to get them back to unscaled preds[1:5] ## Mazda RX4 Mazda RX4 Wag Datsun 710 Hornet 4 Drive ## 0.4162772 0.3353706 1.0220793 0.1902753 ## Hornet Sportabout ## -0.3977454 So how would we re express this information in terms of the unscaled data ? It’s tedious but doable. We leverage the fact that the scaled version of the data has attributes we can use to convert the predictions back to their unscaled form. (unscaled_preds &lt;- preds * attr(smtcars,&quot;scaled:scale&quot;)[&#39;mpg&#39;] + attr(smtcars,&quot;scaled:center&quot;)[&#39;mpg&#39;]) ## Mazda RX4 Mazda RX4 Wag Datsun 710 Hornet 4 Drive ## 22.59951 22.11189 26.25064 21.23740 ## Hornet Sportabout Valiant Duster 360 Merc 240D ## 17.69343 20.38304 14.38626 22.49601 ## Merc 230 Merc 280 Merc 280C Merc 450SE ## 24.41909 18.69903 19.19165 14.17216 ## Merc 450SL Merc 450SLC Cadillac Fleetwood Lincoln Continental ## 15.59957 15.74222 12.03401 10.93644 ## Chrysler Imperial Fiat 128 Honda Civic Toyota Corolla ## 10.49363 27.77291 29.89674 29.51237 ## Toyota Corona Dodge Challenger AMC Javelin Camaro Z28 ## 23.64310 16.94305 17.73218 13.30602 ## Pontiac Firebird Fiat X1-9 Porsche 914-2 Lotus Europa ## 16.69168 28.29347 26.15295 27.63627 ## Ford Pantera L Ferrari Dino Maserati Bora Volvo 142E ## 18.87004 19.69383 13.94112 24.36827 Does this match what we would get had we not scaled the data in the first place ? myLm &lt;- lm(mpg~., data = mtcars) # Do some predicting. (preds &lt;- predict(myLm,mtcars)) ## Mazda RX4 Mazda RX4 Wag Datsun 710 Hornet 4 Drive ## 22.59951 22.11189 26.25064 21.23740 ## Hornet Sportabout Valiant Duster 360 Merc 240D ## 17.69343 20.38304 14.38626 22.49601 ## Merc 230 Merc 280 Merc 280C Merc 450SE ## 24.41909 18.69903 19.19165 14.17216 ## Merc 450SL Merc 450SLC Cadillac Fleetwood Lincoln Continental ## 15.59957 15.74222 12.03401 10.93644 ## Chrysler Imperial Fiat 128 Honda Civic Toyota Corolla ## 10.49363 27.77291 29.89674 29.51237 ## Toyota Corona Dodge Challenger AMC Javelin Camaro Z28 ## 23.64310 16.94305 17.73218 13.30602 ## Pontiac Firebird Fiat X1-9 Porsche 914-2 Lotus Europa ## 16.69168 28.29347 26.15295 27.63627 ## Ford Pantera L Ferrari Dino Maserati Bora Volvo 142E ## 18.87004 19.69383 13.94112 24.36827 all.equal(preds,unscaled_preds) ## [1] TRUE 11.3.3.4 caret and preprocess But the caret package will help you do this using the preProcess function. Here we can actually request that the data is “centered” and “scaled” as part of the model assembly process. We could do this before the call to the Train function but then we would also have to convert the training and test data ourselves. In the following situation, the data will be centered and scaled though the returned RMSE will be in terms of the unscaled data. training_idx &lt;- createDataPartition(mtcars$mpg, p =.80, list=FALSE) training &lt;- mtcars[training_idx,] test &lt;- mtcars[-training_idx,] myLm &lt;- train(mpg ~ ., data = training, method = &quot;lm&quot;, preProcess = c(&quot;center&quot;,&quot;scale&quot;) ) ## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit may ## be misleading To verify that the data is being centered and scaled within the call the train function, checkout the myLm object: # This myLm$finalModel$model[1:5,] ## .outcome cyl disp hp drat wt ## Mazda.RX4 21.0 -0.1169061 -0.5638891 -0.5495661 0.5813586 -0.60017925 ## Datsun.710 22.8 -1.2080299 -0.9832489 -0.7922316 0.4848103 -0.91982835 ## Hornet.4.Drive 21.4 -0.1169061 0.2264428 -0.5495661 -1.0020333 0.03379148 ## Hornet.Sportabout 18.7 0.9742177 1.0490331 0.3782728 -0.8668657 0.27352831 ## Duster.360 14.3 0.9742177 1.0490331 1.3774838 -0.7510078 0.41204292 ## qsec vs am gear carb ## Mazda.RX4 -0.7564127 -0.9141741 1.2207620 0.4446792 0.7113900 ## Datsun.710 0.4135782 1.0548163 1.2207620 0.4446792 -1.0994209 ## Hornet.4.Drive 0.8652490 1.0548163 -0.7899048 -0.9387672 -1.0994209 ## Hornet.Sportabout -0.4516709 -0.9141741 -0.7899048 -0.9387672 -0.4958173 ## Duster.360 -1.0938054 -0.9141741 -0.7899048 -0.9387672 0.7113900 # matches this scale(training)[1:5,] ## mpg cyl disp hp drat ## Mazda RX4 0.1421120 -0.1169061 -0.5638891 -0.5495661 0.5813586 ## Datsun 710 0.4297605 -1.2080299 -0.9832489 -0.7922316 0.4848103 ## Hornet 4 Drive 0.2060339 -0.1169061 0.2264428 -0.5495661 -1.0020333 ## Hornet Sportabout -0.2254388 0.9742177 1.0490331 0.3782728 -0.8668657 ## Duster 360 -0.9285795 0.9742177 1.0490331 1.3774838 -0.7510078 ## wt qsec vs am gear ## Mazda RX4 -0.60017925 -0.7564127 -0.9141741 1.2207620 0.4446792 ## Datsun 710 -0.91982835 0.4135782 1.0548163 1.2207620 0.4446792 ## Hornet 4 Drive 0.03379148 0.8652490 1.0548163 -0.7899048 -0.9387672 ## Hornet Sportabout 0.27352831 -0.4516709 -0.9141741 -0.7899048 -0.9387672 ## Duster 360 0.41204292 -1.0938054 -0.9141741 -0.7899048 -0.9387672 ## carb ## Mazda RX4 0.7113900 ## Datsun 710 -1.0994209 ## Hornet 4 Drive -1.0994209 ## Hornet Sportabout -0.4958173 ## Duster 360 0.7113900 This is convenient because if we now wish to use the predict function, it will scale the test data that we provide for use with the predict function. What we get back will be in terms of the uncenter and unscaled predicted variable (mpg). We do not have to suffer through the conversion reverse scaling process ourselves. (preds &lt;- predict(myLm,test)) ## Mazda RX4 Wag Valiant Chrysler Imperial Porsche 914-2 ## 22.125590 21.707846 8.500784 24.215849 11.3.4 Order of Processing Note that the order of processing is important. You should center and then scale. Underneath the hood, the centering operation finds the mean of a feature (column) and then subtracts it from each value therein. So the following are equivalent: (mt_center &lt;- mtcars$wt - mean(mtcars$wt)) ## [1] -0.59725 -0.34225 -0.89725 -0.00225 0.22275 0.24275 0.35275 -0.02725 ## [9] -0.06725 0.22275 0.22275 0.85275 0.51275 0.56275 2.03275 2.20675 ## [17] 2.12775 -1.01725 -1.60225 -1.38225 -0.75225 0.30275 0.21775 0.62275 ## [25] 0.62775 -1.28225 -1.07725 -1.70425 -0.04725 -0.44725 0.35275 -0.43725 # as.vector(scale(mtcars$wt,center=T,scale=F)) ## [1] -0.59725 -0.34225 -0.89725 -0.00225 0.22275 0.24275 0.35275 -0.02725 ## [9] -0.06725 0.22275 0.22275 0.85275 0.51275 0.56275 2.03275 2.20675 ## [17] 2.12775 -1.01725 -1.60225 -1.38225 -0.75225 0.30275 0.21775 0.62275 ## [25] 0.62775 -1.28225 -1.07725 -1.70425 -0.04725 -0.44725 0.35275 -0.43725 After centering, the scaling is done by dividing the (centered) columns of the data frame by their respective standard deviations. In terms of dealing with missing data and scaling, one should first do imputation followed by centering and scaling. In a call to the train function, this would look like the following. We’ll use our version of mtcars that has missing data. url &lt;- &quot;https://raw.githubusercontent.com/steviep42/utilities/master/data/mtcars_na.csv&quot; mtcars_na &lt;- read.csv(url,stringsAsFactors = FALSE) training_idx &lt;- createDataPartition(mtcars_na$mpg, p =.80, list=FALSE) training &lt;- mtcars_na[training_idx,] test &lt;- mtcars_na[-training_idx,] X &lt;- training[,-1] Y &lt;- training$mpg myLm &lt;- train(X, Y, method = &quot;lm&quot;, preProcess = c(&quot;knnImpute&quot;,&quot;center&quot;,&quot;scale&quot;) ) ## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit may ## be misleading ## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit may ## be misleading 11.4 Low Variance Variables Some variables exhibit low variance and might be nearly constant. Such variables can be detected by using some basic functions in R before you begin to build a model. As an example, we’ll use the mtcars data frame and introduce a low variance variable - actually, we’ll make it a constant. data(mtcars) set.seed(123) mtcars_nzv &lt;- mtcars # Make drat low variance mtcars_nzv$drat &lt;- 3.0 # Pretty low isn&#39;t it ? var(mtcars_nzv$drat) ## [1] 0 What if we use this variable when making a model. We’ll get a lot of problems. While this is a contrived example, it is possible to get this situation when using cross fold validation where the data is segmented into smaller subsets where a variable can be zero variance. x &lt;- mtcars_nzv[,-1] y &lt;- mtcars_nzv[,1] lowVarLm &lt;- train( x,y, method=&quot;lm&quot;, preProcess=c(&quot;center&quot;,&quot;scale&quot;)) The caret package has an option to the preProcess argument that allows us to remove such variables so it won’t impact the resulting model. Notice that we remove the near zero variance variables before we center which happens before scaling. This is the recommended order. x &lt;- mtcars_nzv[,-1] y &lt;- mtcars_nzv[,1] lowVarLm &lt;- train( x,y, method=&quot;lm&quot;, preProcess=c(&quot;nzv&quot;,&quot;center&quot;,&quot;scale&quot;)) The above now works. There is a subtle consideration at work here. We could pre process the data with nzv or zv where the former removes “near” zero variance features and the latter removes constant-valued features. With near zero variance features there is a way to specify tolerance for deciding whether a feature has near zero variance. Think of nzv as being slightly more permissive and flexible whereas zv eliminates zero variance variables. Sometimes you might want to keep near zero variance features around simply because there could be some interesting information therein. In general we could remove the constant or near zero variance features before passing the data to the train function. Caret has a standalone function called nearZeroVariance to do this. We’ll it doesn’t actually remove the feature but it will tell us which column(s) exhibit very low variance or have a constant value. In this case it is column number 5 which corresponds to drat. We already knew that. nearZeroVar(mtcars_nzv) ## [1] 5 # Find the nzv columns mtcars_nzv[nearZeroVar(mtcars_nzv)] ## drat ## Mazda RX4 3 ## Mazda RX4 Wag 3 ## Datsun 710 3 ## Hornet 4 Drive 3 ## Hornet Sportabout 3 ## Valiant 3 ## Duster 360 3 ## Merc 240D 3 ## Merc 230 3 ## Merc 280 3 ## Merc 280C 3 ## Merc 450SE 3 ## Merc 450SL 3 ## Merc 450SLC 3 ## Cadillac Fleetwood 3 ## Lincoln Continental 3 ## Chrysler Imperial 3 ## Fiat 128 3 ## Honda Civic 3 ## Toyota Corolla 3 ## Toyota Corona 3 ## Dodge Challenger 3 ## AMC Javelin 3 ## Camaro Z28 3 ## Pontiac Firebird 3 ## Fiat X1-9 3 ## Porsche 914-2 3 ## Lotus Europa 3 ## Ford Pantera L 3 ## Ferrari Dino 3 ## Maserati Bora 3 ## Volvo 142E 3 There is a data frame in the caret package which exhibits this behavior in a more organic fashion - that is the data measurements of a number of features are near zero variance. data(BloodBrain) str(bbbDescr,0) ## &#39;data.frame&#39;: 208 obs. of 134 variables: # Which columns are suspected of having near zero variance ? nearZeroVar(bbbDescr) ## [1] 3 16 17 22 25 50 60 # What are their names ? names(bbbDescr[,nearZeroVar(bbbDescr)]) ## [1] &quot;negative&quot; &quot;peoe_vsa.2.1&quot; &quot;peoe_vsa.3.1&quot; &quot;a_acid&quot; &quot;vsa_acid&quot; ## [6] &quot;frac.anion7.&quot; &quot;alert&quot; 11.5 PCA - Principal Components Analysis So one of the problems with data can be what is called multicollinearity where high correlations exist between variables in a data set. Consider the mtcars data frame for example. Let’s assume that we want to predict whether a given car has an automatic transmission (0) or manual (1). We’ll remove other columns from the data frame that represent categorical data so we can focus on the continuous numeric variables. 11.5.1 Identify The Factors sapply(mtcars,function(x) length(unique(x))) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 25 3 27 22 22 29 30 2 2 3 6 Let’s eliminate cyl,vs,gear, and carb. We’ll also remove the rownames since, if we don’t, then they will cause problems when we create the biplot of the principal components. mtcars_data &lt;- mtcars[,c(1,3:7,9)] rownames(mtcars_data) &lt;- NULL Now, let’s look at the correlation matrix to see if we have highly correlated variables. We do have several correlations that exceed .7 which is sufficiently high to consider that they might cause problems when building models. The caret package has a findCorrelation function that can remove predictors that exhibit a correlation above a certain threshold but we’ll see how PCA can help - so we’ll leave them in for now. 11.5.2 Check For High Correlations # Get correlations just for the predictor variables DataExplorer::plot_correlation(mtcars_data[,-7]) This is a graphic equivalent of this command: cor(mtcars_data[,-7]) ## mpg disp hp drat wt qsec ## mpg 1.0000000 -0.8475514 -0.7761684 0.68117191 -0.8676594 0.41868403 ## disp -0.8475514 1.0000000 0.7909486 -0.71021393 0.8879799 -0.43369788 ## hp -0.7761684 0.7909486 1.0000000 -0.44875912 0.6587479 -0.70822339 ## drat 0.6811719 -0.7102139 -0.4487591 1.00000000 -0.7124406 0.09120476 ## wt -0.8676594 0.8879799 0.6587479 -0.71244065 1.0000000 -0.17471588 ## qsec 0.4186840 -0.4336979 -0.7082234 0.09120476 -0.1747159 1.00000000 11.5.3 So Why Use PCA ? There are several functions for doing Principal Components Analysis on this data. But why are we even thinking about PCA ? Well, it helps us deal with highly correlated data by reducing the dimensionality of a data set. In the mtcars data frame we don’t have that many variables / columns but wouldn’t it be nice to transform the data in a way that reduced the number of columns that we had to consider while also dealing with the multicollinearity ? This is what PCA can do for us. In reality we are using the eigenvectors of the covariance matrix of the original data. We use them to transform the original data into a reduced number of columns to consider. To get the ball rolling, we’ll use the prcomp function. scaled_mtcars_data &lt;- scale(mtcars_data[,-7]) prcs &lt;- princomp(scaled_mtcars_data) prcs ## Call: ## princomp(x = scaled_mtcars_data) ## ## Standard deviations: ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 ## 2.0140855 1.0546249 0.5682775 0.3866999 0.3477012 0.2243967 ## ## 6 variables and 32 observations. summary(prcs) ## Importance of components: ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 ## Standard deviation 2.0140855 1.0546249 0.56827746 0.38669985 0.34770121 ## Proportion of Variance 0.6978994 0.1913520 0.05555944 0.02572676 0.02079933 ## Cumulative Proportion 0.6978994 0.8892514 0.94481088 0.97053763 0.99133697 ## Comp.6 ## Standard deviation 0.224396671 ## Proportion of Variance 0.008663031 ## Cumulative Proportion 1.000000000 11.5.4 Check The BiPlot What we get from this summary is that PC1 accounts for roughly 70% of the variation in the data set. By the time we get to PC3, about 95% of the variation is accounted. We can also look at something called a biplot that allows us to see what variables in the first two components are influential. This information is also available just by viewing the default return information from the object itself but the biplot makes it easier to see. biplot(prcs) Back to the summary information, we cal use something called a screeplot which will help us see how many of the components to use. We don’t actually need the plot although it can help. Look at the plot and find the “elbow” which is the point at which the rate of change stabilizes. In the plot below it looks like the elbow is at PC3. In looking at the summary info, if we select three components then we are accounting for about 95% of the variation in the data. IF we selected two components then we would have about 89% of the variation accounted for. summary(prcs) ## Importance of components: ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 ## Standard deviation 2.0140855 1.0546249 0.56827746 0.38669985 0.34770121 ## Proportion of Variance 0.6978994 0.1913520 0.05555944 0.02572676 0.02079933 ## Cumulative Proportion 0.6978994 0.8892514 0.94481088 0.97053763 0.99133697 ## Comp.6 ## Standard deviation 0.224396671 ## Proportion of Variance 0.008663031 ## Cumulative Proportion 1.000000000 11.5.5 Check The ScreePlot screeplot(prcs,type=&quot;line&quot;) Let’s pick two components. We’ll multiply our data set (using (matrix multiplication) by the two “loadings” / components that we picked. This will give us our original data in terms of the two principal components. model2 &lt;- prcs$loadings[,1:2] model2_scores &lt;- as.matrix(scaled_mtcars_data) %*% model2 11.5.6 Use The Transformed Data So now we’ll use the Naive Bayes method to do build a model using the transformed data. mod2 &lt;- e1071::naiveBayes(model2_scores,factor(mtcars_data$am)) (mod2t &lt;- table(predict(mod2,model2_scores),mtcars_data$am)) ## ## 0 1 ## 0 18 0 ## 1 1 13 cat(&quot;Accuracy for PCA mod: &quot;,sum(diag(mod2t))/sum(mod2t),&quot;\\n&quot;) ## Accuracy for PCA mod: 0.96875 Compare this to a model built using the untransformed data: mod1 &lt;- e1071::naiveBayes(mtcars_data[-7],factor(mtcars_data$am)) (mod1t &lt;- table(predict(mod1,mtcars_data),mtcars_data$am)) ## ## 0 1 ## 0 16 2 ## 1 3 11 cat(&quot;Accuracy for PCA mod: &quot;,sum(diag(mod1t))/sum(mod1t),&quot;\\n&quot;) ## Accuracy for PCA mod: 0.84375 11.5.7 PLS Partial Least Squares regression is related to PCA although, in classification problems, the latter ignores the variable being predicted. PLS uses information from the variable to be predicted (the class labels) to help maximize the separation of the two classes. Of course, using pls as a method within caret is easy. control &lt;- trainControl(method=&quot;cv&quot;,number = 5) # caret_pls &lt;- train(factor(am)~., mtcars, method=&quot;pls&quot;, preProcess=c(&quot;center&quot;,&quot;scale&quot;), trControl = control) We already knew that the mtcars data frame would benefit from PCA. Here we see that PLS uses the first three Principal Components to arrive at an accuracy of 0.97 on the data set. 11.5.8 Summary So the advantages of PCA should be clear in this case. We have effectively replaced the original data by a smaller data set while also dealing with the correlation issues. We used only two components. Now, a disadvantage here is that the model with the transformed data is in terms of the components which means that the model is less transparent. Perhaps a minor price to pay for better accuracy. In terms of the caret package we can do this using the preProcess function: control &lt;- trainControl(method=&quot;cv&quot;,number = 5) # caret_nb &lt;- train(factor(am)~., mtcars_data, method=&quot;nb&quot;, preProcess=c(&quot;center&quot;,&quot;scale&quot;,&quot;pca&quot;), trControl = control) table(predicted=predict(caret_nb,mtcars),mtcars$am) ## ## predicted 0 1 ## 0 18 0 ## 1 1 13 11.6 Order of Pre-Processing In reality, if we wanted to line up the order in which to pre process data it would read something like the following: Remove Near Zero Variance Features Do Imputation (knn or median) Center Scale lowVarLm &lt;- train( mtcars[,-1],mtcars$mpg, method=&quot;lm&quot;, preProcess=c(&quot;nzv&quot;,&quot;medianImpute&quot;,&quot;center&quot;,&quot;scale&quot;)) 11.7 Identifying Redundant Features Lots of Data Scientists and model builders would like to know which features are important and which are redundant - BEFORE building the model. Many times, people just include all features and rely upon post model statistics to diagnose the model. This is fine though there are automated methods to help. Some statisticians do not like this approach (e.g. Step Wise Regression) because it emphasizes a scoring metric that is computed as a function of other measures though that process itself might be incomplete in some way. Ideally, you would have some up front idea about the data. 11.7.1 Highly Correlated Variables In an earlier section we looked at the correlations between the variables in the mtcars data frame. When there are a number of strongly correlated variables the issue of multicollinerarity might exist. One variable might be sufficient to represent the information of one or more other variables which is useful when building models because it would allow us to leave out comparatively low information variables. Techniques like PCA Principal Components Analysis can be used to recombine features in a way that explains most of the variation of data set. A simpler way might be to look for highly correlated sets of variables and remove those over a certain threshold of correlation from the data frame. Let’s look at the mtcars data frame again to see what variables are correlated. data(mtcars) correlations &lt;- cor(mtcars) correlations[1:6, 1:6] ## mpg cyl disp hp drat wt ## mpg 1.0000000 -0.8521620 -0.8475514 -0.7761684 0.6811719 -0.8676594 ## cyl -0.8521620 1.0000000 0.9020329 0.8324475 -0.6999381 0.7824958 ## disp -0.8475514 0.9020329 1.0000000 0.7909486 -0.7102139 0.8879799 ## hp -0.7761684 0.8324475 0.7909486 1.0000000 -0.4487591 0.6587479 ## drat 0.6811719 -0.6999381 -0.7102139 -0.4487591 1.0000000 -0.7124406 ## wt -0.8676594 0.7824958 0.8879799 0.6587479 -0.7124406 1.0000000 It turns out that there are lots of strong correlations going on here. The darker the circle the stronger the correlation. suppressMessages(library(corrplot)) corrplot(correlations, order=&quot;hclust&quot;) The caret package has some functions that can help us identify highly correlated variables that might be a candidates for removal prior to use in building a model. One of the variables that is highly correlated with others is mpg Since that is the one we are trying to predict, we’ll keep it around. The following columns from mtcars have a correlation of .75 (or higher) with some other variable(s) (highcorr &lt;- findCorrelation(correlations, cutoff=.75)) ## [1] 2 3 1 10 names(mtcars[,highcorr]) ## [1] &quot;cyl&quot; &quot;disp&quot; &quot;mpg&quot; &quot;gear&quot; So let’s remove those variables from consideration although we’ll keep the mpg variable since that is what we will be predicting. data(mtcars) decorrel_mtcars &lt;- mtcars[,-highcorr[highcorr != 1]] This might yield a better model though it’s tough to tell. idx &lt;- createDataPartition(decorrel_mtcars$mpg, p = .8, list = FALSE, times = 1) train &lt;- decorrel_mtcars[ idx,] test &lt;- decorrel_mtcars[-idx,] decorrel_lm &lt;- train(mpg~., data = train, method = &quot;lm&quot;, metric = &quot;RMSE&quot;) 11.7.2 Ranking Features Another approach would be to identify the important variables as determined by a rigorous, reliable and statistically aware process. Some models will report this information as part of the computation. The caret package has a function called varImp which helps to calculate relative variable importance within an object produced by the train function. What this means for us is that we can take most models built with train and pass it to the varImp function. idx &lt;- createDataPartition(mtcars$mpg, p = .8, list = FALSE, times = 1) train &lt;- mtcars[ idx,] test &lt;- mtcars[-idx,] myLm &lt;- train(mpg~., data = train, method = &quot;lm&quot;, metric = &quot;RMSE&quot;) ## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit may ## be misleading summary(myLm) ## ## Call: ## lm(formula = .outcome ~ ., data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.6545 -1.6435 -0.0332 0.9472 4.3548 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.67691 29.04361 -0.609 0.5508 ## cyl -0.10805 1.31734 -0.082 0.9356 ## disp 0.01709 0.01941 0.880 0.3909 ## hp -0.01151 0.02592 -0.444 0.6625 ## drat 1.77684 1.86252 0.954 0.3535 ## wt -4.51440 2.22558 -2.028 0.0585 . ## qsec 2.10070 1.26766 1.657 0.1158 ## vs -2.15042 3.83235 -0.561 0.5820 ## am 0.72004 2.75256 0.262 0.7968 ## gear 2.22324 2.04781 1.086 0.2928 ## carb -0.11134 0.97412 -0.114 0.9103 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.768 on 17 degrees of freedom ## Multiple R-squared: 0.8676, Adjusted R-squared: 0.7898 ## F-statistic: 11.14 on 10 and 17 DF, p-value: 1.239e-05 If you look at the summary of the model it looks pretty bleak because mot of the coefficients aren’t significant. You wouldn’t want to take your career on this model. Now let’s see what variables are considered to be important by using varImp. Remember that this might lead us to use a subset of the more prominent variables in the formation of a new model in case, for example, the coefficients in the existing model weren’t significant. plot(varImp(myLm)) How do things look in the model if we limit the formula to some of the “important” (allegedly) features ? Well, it explains more of the variance with fewer predictors (the R^2 value). Both wt and hp are significant. That’s a start. myLm &lt;- train(mpg~wt+disp+hp, data = train, method = &quot;lm&quot;, metric = &quot;RMSE&quot;) summary(myLm) ## ## Call: ## lm(formula = .outcome ~ ., data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.7930 -1.7171 -0.5208 1.1468 5.7533 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 37.102658 2.440382 15.204 8.09e-14 *** ## wt -3.705589 1.265889 -2.927 0.00737 ** ## disp -0.002895 0.011699 -0.247 0.80664 ## hp -0.030005 0.012239 -2.452 0.02188 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.798 on 24 degrees of freedom ## Multiple R-squared: 0.809, Adjusted R-squared: 0.7852 ## F-statistic: 33.89 on 3 and 24 DF, p-value: 8.601e-09 11.7.3 Feature Selection Being able to automatically include only the most important features is desirable (again an example might be Step Wise Regression). Caret provides its own function to do a simple backwards selection to recursively eliminate features based on how they contribute (or not) to the model. You could do all of this by hand yourself of course. Automatic feature selection methods can be used to build many models with different subsets of a dataset and identify those attributes that are and are not required to build an accurate model. The caret package has a function called *rfe** to implement this. However, it involves some additional setup: rfeControl &lt;- rfeControl(functions=lmFuncs, method=&quot;cv&quot;, number=10) results &lt;- rfe(mtcars[,2:11], mtcars[,1], sizes=c(2:11), rfeControl=rfeControl) print(results) ## ## Recursive feature selection ## ## Outer resampling method: Cross-Validated (10 fold) ## ## Resampling performance over subset size: ## ## Variables RMSE Rsquared MAE RMSESD RsquaredSD MAESD Selected ## 2 3.059 0.9048 2.624 1.1185 0.1043 1.0419 ## 3 3.096 0.8846 2.679 1.1977 0.1247 1.0573 ## 4 2.962 0.8884 2.555 0.8976 0.1382 0.7654 * ## 5 3.227 0.8591 2.671 1.0330 0.1798 0.8204 ## 6 3.175 0.8763 2.643 1.0883 0.1619 0.8602 ## 7 3.220 0.8743 2.666 1.1369 0.1795 0.8943 ## 8 3.162 0.8833 2.587 1.2447 0.1806 0.9916 ## 9 3.316 0.8588 2.739 1.4329 0.1891 1.2213 ## 10 3.203 0.8671 2.702 1.4511 0.1696 1.2667 ## ## The top 4 variables (out of 4): ## wt, am, drat, gear plot(results,type=c(&quot;g&quot;,&quot;o&quot;)) myLm &lt;- train(mpg~wt+am+qsec, data = train, method = &quot;lm&quot;, metric = &quot;RMSE&quot;) summary(myLm) ## ## Call: ## lm(formula = .outcome ~ ., data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.6595 -1.2609 -0.4483 1.5021 4.2625 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.5955 8.4322 0.545 0.590789 ## wt -3.7490 0.8468 -4.427 0.000178 *** ## am 2.9134 1.5280 1.907 0.068605 . ## qsec 1.4857 0.3552 4.182 0.000332 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.532 on 24 degrees of freedom ## Multiple R-squared: 0.8437, Adjusted R-squared: 0.8242 ## F-statistic: 43.19 on 3 and 24 DF, p-value: 7.907e-10 11.8 Handling Categories If you haven’t already, you should first read the section on “Levels Of Measurement” to reacquaint yourself with the differences between Nominal, Ordinal, Interval, and Ratio data. In this section we’ll deal with categories and factors which represent categories (e.g. “male”,“female”,“smoker”,“non-smoker”). These variables, while useful, need to be recoded in a way to make them useful for machine learning methods. In terms of categories, we have nominal and ordinal features with the former being names or labels and the latter being the same except with some notion of order (e.g. “low”,“medium”,“high”). 11.8.1 Examples In R, you can usually create a factor out of a feature and R will handle it correctly when applying a machine learning method. Under the hood, it turns the factors into dummy variables. Notice how the model creates variables of the type cyl6 and cyl8. Where is cyl4 ? Well, absence of cyl4 is simply when cyl6 and cyl8 do not exist for that record. mtcars_exampl &lt;- mtcars %&gt;% mutate(cyl=factor(cyl)) lm(mpg ~ .,mtcars_exampl) ## ## Call: ## lm(formula = mpg ~ ., data = mtcars_exampl) ## ## Coefficients: ## (Intercept) cyl6 cyl8 disp hp drat ## 17.81984 -1.66031 1.63744 0.01391 -0.04613 0.02635 ## wt qsec vs am gear carb ## -3.80625 0.64696 1.74739 2.61727 0.76403 0.50935 We could have created the dummy variables ourselves but we didn’t need to do that here. In Python, we generally would. But when we do it’s best to have 1 less category than is encoded by the unique feature values under consideration. If there are n=3 unique values then we would want 2 dummy variables. mtcars_exampl &lt;- mtcars %&gt;% mutate(cyl=factor(cyl)) dummy &lt;- dummyVars(~.,mtcars_exampl) dummied_up_mtcars &lt;- data.frame(predict(dummy,newdata=mtcars_exampl)) head(dummied_up_mtcars) ## mpg cyl.4 cyl.6 cyl.8 disp hp drat wt qsec vs am gear carb ## 1 21.0 0 1 0 160 110 3.90 2.620 16.46 0 1 4 4 ## 2 21.0 0 1 0 160 110 3.90 2.875 17.02 0 1 4 4 ## 3 22.8 1 0 0 108 93 3.85 2.320 18.61 1 1 4 1 ## 4 21.4 0 1 0 258 110 3.08 3.215 19.44 1 0 3 1 ## 5 18.7 0 0 1 360 175 3.15 3.440 17.02 0 0 3 2 ## 6 18.1 0 1 0 225 105 2.76 3.460 20.22 1 0 3 1 But here we would want to use the fullrank = TRUE in the call to dummyVars to accomplish the n-1 encoding. The reason we would do this is to avoid the collinearity although in this case that isn’t a problem here. We could first check the correlations plot_correlation(dummied_up_mtcars[,1:6]) mtcars_exampl &lt;- mtcars %&gt;% mutate(cyl=factor(cyl)) dummy &lt;- dummyVars(~.,mtcars_exampl,fullRank = T) dummied_up_mtcars &lt;- data.frame(predict(dummy, newdata=mtcars_exampl)) head(dummied_up_mtcars) ## mpg cyl.6 cyl.8 disp hp drat wt qsec vs am gear carb ## 1 21.0 1 0 160 110 3.90 2.620 16.46 0 1 4 4 ## 2 21.0 1 0 160 110 3.90 2.875 17.02 0 1 4 4 ## 3 22.8 0 0 108 93 3.85 2.320 18.61 1 1 4 1 ## 4 21.4 1 0 258 110 3.08 3.215 19.44 1 0 3 1 ## 5 18.7 0 1 360 175 3.15 3.440 17.02 0 0 3 2 ## 6 18.1 1 0 225 105 2.76 3.460 20.22 1 0 3 1 But again, R will do this for us just by indicating that it is a factor. But let me show you what will happen in another case with respect to the am variable if we do not use the n-1 encoding mtcars_exampl &lt;- mtcars %&gt;% mutate(am=factor(am)) dummy &lt;- dummyVars(~.,mtcars_exampl) dummied_up_mtcars &lt;- data.frame(predict(dummy, newdata=mtcars_exampl)) head(dummied_up_mtcars[,7:11]) ## qsec vs am.0 am.1 gear ## 1 16.46 0 0 1 4 ## 2 17.02 0 0 1 4 ## 3 18.61 1 0 1 4 ## 4 19.44 1 1 0 3 ## 5 17.02 0 1 0 3 ## 6 20.22 1 1 0 3 Check the correlations between am.0 and am.1. They are perfectly correlated which could cause problems in the modeling methods. We could express am.0 as a linear function of am.1. This is why using the n-1 approach helps deal with the collinearity problem. plot_correlation(dummied_up_mtcars[,7:11]) 11.8.2 Admissions Data For example, read in the following data which relates to admissions data for students applying to an academic program. This comes from UCLA Statisitical Consulting site. admissions &lt;- read.csv(&quot;https://stats.idre.ucla.edu/stat/data/binary.csv&quot;) head(admissions) ## admit gre gpa rank ## 1 0 380 3.61 3 ## 2 1 660 3.67 3 ## 3 1 800 4.00 1 ## 4 1 640 3.19 4 ## 5 0 520 2.93 4 ## 6 1 760 3.00 2 str(admissions) ## &#39;data.frame&#39;: 400 obs. of 4 variables: ## $ admit: int 0 1 1 1 0 1 1 0 1 0 ... ## $ gre : int 380 660 800 640 520 760 560 400 540 700 ... ## $ gpa : num 3.61 3.67 4 3.19 2.93 3 2.98 3.08 3.39 3.92 ... ## $ rank : int 3 3 1 4 4 2 1 2 3 2 ... Let’s examine this data by determining the number of unique values assumed by each feature. This helps us understand if a variable is a category / factor variable or a continuous quantity. It appears that admit takes on only two distinct values and rank assumes only four which suggests that both might be more of a category than a quantity upon which we could perform lots of calculations. As it relates to admit, this is a yes / no assessment and there is no inherent order even though it has been encoded as a 0 or a 1. This might be something that we might predict with a model. We will treat the variables gre and gpa as continuous. The variable rank takes on the values 1 through 4. Institutions with a rank of 1 have the highest prestige, while those with a rank of 4 have the lowest. admissions %&gt;% summarise_all(n_distinct) ## admit gre gpa rank ## 1 2 26 132 4 You can always use the summary function to get a quick overview of the data. Notice here that admit is more of a binary entity although since R thinks it is just a number it will compute the percentiles for it. Same with the rest of the variables. We probably don’t want this but let’s hold off on doing anything about it for now since most newcomers to Data Analysis will make this mistake. Let’s see what happens. summary(admissions) ## admit gre gpa rank ## Min. :0.0000 Min. :220.0 Min. :2.260 Min. :1.000 ## 1st Qu.:0.0000 1st Qu.:520.0 1st Qu.:3.130 1st Qu.:2.000 ## Median :0.0000 Median :580.0 Median :3.395 Median :2.000 ## Mean :0.3175 Mean :587.7 Mean :3.390 Mean :2.485 ## 3rd Qu.:1.0000 3rd Qu.:660.0 3rd Qu.:3.670 3rd Qu.:3.000 ## Max. :1.0000 Max. :800.0 Max. :4.000 Max. :4.000 So with this data, an interesting problem might be to predict whether an applicant is admitted to the program based on the other variables in the data set. We could pick Logistic Regression for this activity. control &lt;- trainControl(method=&quot;cv&quot;,number=3) try_to_classify_admit &lt;- train(admit ~ ., data = admissions, method = &quot;glm&quot;, trControl = control) ## Warning in train.default(x, y, weights = w, ...): You are trying to do regression ## and your outcome only has two possible values Are you trying to do classification? ## If so, use a 2 level factor as your outcome column. Why does this bomb out ? Well, the GLM method expects us to be predicting a binary outcome which, to the human eye, the admit variable actually is. However, R doesn’t know this as it currently thinks of the admit variable as being simply a number so it thinks we are trying to predict a numeric outcome hence the error. To fix this we need to make admit a factor which is a term used in statistics to refer to categories. So let’s see the effect of doing this. While we are at it, let’s also resummarize the data to see if R treats the admit variable any differently after the conversion to factor has been made. admissions_factor_1 &lt;- admissions %&gt;% mutate(admit=factor(admit)) str(admissions_factor_1) ## &#39;data.frame&#39;: 400 obs. of 4 variables: ## $ admit: Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 2 2 2 1 2 2 1 2 1 ... ## $ gre : int 380 660 800 640 520 760 560 400 540 700 ... ## $ gpa : num 3.61 3.67 4 3.19 2.93 3 2.98 3.08 3.39 3.92 ... ## $ rank : int 3 3 1 4 4 2 1 2 3 2 ... cat(&quot;\\n Now look at the summary&quot;,&quot;\\n&quot;) ## ## Now look at the summary summary(admissions_factor_1$admit) ## 0 1 ## 273 127 So now R understands that admit is a category so it chooses to offer a table / count summary of the data as opposed to the summary statistics it would apply for numeric data. So now let’s try again to build a predictive model for admit. classify_admit_factor_1 &lt;- train(admit ~ ., data = admissions_factor_1, method = &quot;glm&quot;, trControl = control) classify_admit_factor_1$results$Accuracy ## [1] 0.6950772 classify_admit_factor_1 %&gt;% summary ## ## Call: ## NULL ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.5802 -0.8848 -0.6382 1.1575 2.1732 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.449548 1.132846 -3.045 0.00233 ** ## gre 0.002294 0.001092 2.101 0.03564 * ## gpa 0.777014 0.327484 2.373 0.01766 * ## rank -0.560031 0.127137 -4.405 1.06e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 499.98 on 399 degrees of freedom ## Residual deviance: 459.44 on 396 degrees of freedom ## AIC: 467.44 ## ## Number of Fisher Scoring iterations: 4 11.8.3 Is Rank A Category ? Okay that was nice and it appears that all of our predictors are significant with rank ,in particular, being so. However, when looking at the rank variable it looks like it is an ordinal variable as opposed to an interval variable. It’s interesting since rank is on an interval but is a rank of 0 significant ? This variable could be considered as an interval variable but it’s more likely that it is ordinal in which case we would need to turn it into a factor. admissions_factor_2 &lt;- admissions_factor_1 %&gt;% mutate(rank=factor(rank)) str(admissions_factor_2) ## &#39;data.frame&#39;: 400 obs. of 4 variables: ## $ admit: Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 2 2 2 1 2 2 1 2 1 ... ## $ gre : int 380 660 800 640 520 760 560 400 540 700 ... ## $ gpa : num 3.61 3.67 4 3.19 2.93 3 2.98 3.08 3.39 3.92 ... ## $ rank : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 3 3 1 4 4 2 1 2 3 2 ... classify_admit_factor_2 &lt;- train(admit ~ ., data = admissions_factor_2, method = &quot;glm&quot;, trControl = control) classify_admit_factor_2$results$Accuracy ## [1] 0.6924401 classify_admit_factor_2 %&gt;% summary ## ## Call: ## NULL ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.6268 -0.8662 -0.6388 1.1490 2.0790 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.989979 1.139951 -3.500 0.000465 *** ## gre 0.002264 0.001094 2.070 0.038465 * ## gpa 0.804038 0.331819 2.423 0.015388 * ## rank2 -0.675443 0.316490 -2.134 0.032829 * ## rank3 -1.340204 0.345306 -3.881 0.000104 *** ## rank4 -1.551464 0.417832 -3.713 0.000205 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 499.98 on 399 degrees of freedom ## Residual deviance: 458.52 on 394 degrees of freedom ## AIC: 470.52 ## ## Number of Fisher Scoring iterations: 4 This is interesting in that the Accuracy from this model is less than the previous one although we see that the interpretation of Rank is more nuanced here in that ranks of 3 and 4 appear to be more significant than other rank values. We might take this into consideration when thinking about how to transform the features or modify our prediction formula. 11.8.4 Relationship To One Hot Encoding Usually when you indicate to R that a variable is a factor then most procedures and functions will know how to handle that when computing various statistics and building models. Other languages might not do this for you in which case you need to employ an approach called One Hot Encoding. We can do this also in R but in many (most) cases it is not necessary as long as you explicitly create factors as indicated above. It might be helpful to first see what a one hot encoded version of the admissions data frame might look: gre gpa rank.1 rank.2 rank.3 rank.4 1 380 3.61 0 0 1 0 2 660 3.67 0 0 1 0 3 800 4.00 1 0 0 0 4 640 3.19 0 0 0 1 5 520 2.93 0 0 0 1 6 760 3.00 0 1 0 0 There are two things to notice here. The first is that the admit variable isn’t present since that is the value we will be predicting. The second thing to notice is that for each “level” of the rank variable there is a corresponding column. Now, there is the concept of a “rank deficient” fit wherein you will generally want one less variable than the number of categories present to avoid perfect collinearity. In this case the above encoding would look like: gre gpa rank.2 rank.3 rank.4 1 380 3.61 0 1 0 2 660 3.67 0 1 0 3 800 4.00 0 0 0 4 640 3.19 0 0 1 5 520 2.93 0 0 1 6 760 3.00 1 0 0 So, in cases where all rank features (2,3 and 4) were 0 would correspond to a rank of 1. In any case, here is how we would handle the situation using the One Hot Encoding approach. First, we’ll read in the data again: admissions &lt;- read.csv(&quot;https://stats.idre.ucla.edu/stat/data/binary.csv&quot;) head(admissions) ## admit gre gpa rank ## 1 0 380 3.61 3 ## 2 1 660 3.67 3 ## 3 1 800 4.00 1 ## 4 1 640 3.19 4 ## 5 0 520 2.93 4 ## 6 1 760 3.00 2 str(admissions) ## &#39;data.frame&#39;: 400 obs. of 4 variables: ## $ admit: int 0 1 1 1 0 1 1 0 1 0 ... ## $ gre : int 380 660 800 640 520 760 560 400 540 700 ... ## $ gpa : num 3.61 3.67 4 3.19 2.93 3 2.98 3.08 3.39 3.92 ... ## $ rank : int 3 3 1 4 4 2 1 2 3 2 ... Now since we want to predict admissions and we consider rank as a factor we’ll turn those into factors. As mentioned, this is all that is necessary when using R but let’s pretend that it isn’t. admissions_factors &lt;- admissions %&gt;% mutate(admit=factor(admit), rank=factor(rank)) str(admissions_factors) ## &#39;data.frame&#39;: 400 obs. of 4 variables: ## $ admit: Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 2 2 2 1 2 2 1 2 1 ... ## $ gre : int 380 660 800 640 520 760 560 400 540 700 ... ## $ gpa : num 3.61 3.67 4 3.19 2.93 3 2.98 3.08 3.39 3.92 ... ## $ rank : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 3 3 1 4 4 2 1 2 3 2 ... So now we’ll use the dummyVars function from caret to make dummy vars out using out admissions data frame. Since we are ultimately wanting to predict the admit feature, we’ll tell dummyVars that we don’t want that to be split across two columns. (dummy &lt;- dummyVars(&quot;admit ~ .&quot;,admissions_factors,fullRank = TRUE)) ## Dummy Variable Object ## ## Formula: admit ~ . ## &lt;environment: 0x7ff668326bd8&gt; ## 4 variables, 2 factors ## Variables and levels will be separated by &#39;.&#39; ## A full rank encoding is used dummied_up_admissions &lt;- data.frame(predict(dummy,newdata=admissions_factors)) ## Warning in model.frame.default(Terms, newdata, na.action = na.action, xlev = ## object$lvls): variable &#39;admit&#39; is not a factor str(dummied_up_admissions) ## &#39;data.frame&#39;: 400 obs. of 5 variables: ## $ gre : num 380 660 800 640 520 760 560 400 540 700 ... ## $ gpa : num 3.61 3.67 4 3.19 2.93 3 2.98 3.08 3.39 3.92 ... ## $ rank.2: num 0 0 0 0 0 1 0 1 0 1 ... ## $ rank.3: num 1 1 0 0 0 0 0 0 1 0 ... ## $ rank.4: num 0 0 0 1 1 0 0 0 0 0 ... head(dummied_up_admissions) ## gre gpa rank.2 rank.3 rank.4 ## 1 380 3.61 0 1 0 ## 2 660 3.67 0 1 0 ## 3 800 4.00 0 0 0 ## 4 640 3.19 0 0 1 ## 5 520 2.93 0 0 1 ## 6 760 3.00 1 0 0 So we could now use this data frame in our modeling attempts as before to see if it makes any difference. We’ll also need to use an alternative calling sequence to the train function which allows us to specify the predicted column (admit) separately from the features we are using to make that prediction. classify_admit_onehot &lt;- train(x = dummied_up_admissions, y = admissions_factors[,&#39;admit&#39;], method = &quot;glm&quot;, trControl = control) classify_admit_onehot$results$Accuracy ## [1] 0.7050462 classify_admit_onehot %&gt;% summary ## ## Call: ## NULL ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.6268 -0.8662 -0.6388 1.1490 2.0790 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.989979 1.139951 -3.500 0.000465 *** ## gre 0.002264 0.001094 2.070 0.038465 * ## gpa 0.804038 0.331819 2.423 0.015388 * ## rank.2 -0.675443 0.316490 -2.134 0.032829 * ## rank.3 -1.340204 0.345306 -3.881 0.000104 *** ## rank.4 -1.551464 0.417832 -3.713 0.000205 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 499.98 on 399 degrees of freedom ## Residual deviance: 458.52 on 394 degrees of freedom ## AIC: 470.52 ## ## Number of Fisher Scoring iterations: 4 How does this compare to the model we built earlier where we told R that both admit and rank were factors ? It’s pretty much the same. classify_admit_factor_2$results$Accuracy ## [1] 0.6924401 classify_admit_factor_2 %&gt;% summary ## ## Call: ## NULL ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.6268 -0.8662 -0.6388 1.1490 2.0790 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.989979 1.139951 -3.500 0.000465 *** ## gre 0.002264 0.001094 2.070 0.038465 * ## gpa 0.804038 0.331819 2.423 0.015388 * ## rank2 -0.675443 0.316490 -2.134 0.032829 * ## rank3 -1.340204 0.345306 -3.881 0.000104 *** ## rank4 -1.551464 0.417832 -3.713 0.000205 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 499.98 on 399 degrees of freedom ## Residual deviance: 458.52 on 394 degrees of freedom ## AIC: 470.52 ## ## Number of Fisher Scoring iterations: 4 11.9 Binning Sometimes we have data that spans a range of values such that it might make more sense to “bin” or collect the values into “buckets” represented by categories. This is akin to what a histogram does. Look at the PimaIndiansDiabetes data set for an example. Specifically, look at the pregnant feature in the form of a histogram. Most of those surveyed had less than 5 pregnancies although there are those who had at least that number of pregnancies and more. pm_new &lt;- PimaIndiansDiabetes hist(pm_new$pregnant) We could build our models using the number of pregnancies without attempting to transform the feature in any way. But let’s bin this feature. In R, we can use the cut function to chop up a numerical range of data into a specified number of bins and according to some function such as the quantile function which would give us for bins labelled from 1 to 4. This will produce sample quantiles according to the 0, 25th, 50th, 75%, and 100% percentiles. cut(pm_new$pregnant, breaks=quantile(pm$pregnant), include.lowest = TRUE,labels=1:4) ## [1] 3 1 4 1 1 3 2 4 2 4 3 4 4 1 3 4 1 4 1 1 2 4 4 4 4 4 4 1 4 3 3 2 2 3 4 3 4 4 2 ## [40] 3 2 4 4 4 4 1 1 2 4 4 1 1 3 4 4 1 4 1 1 1 2 4 3 2 4 3 1 2 1 3 2 3 4 3 1 1 4 3 ## [79] 1 2 2 2 4 1 3 2 4 2 4 1 1 3 4 3 2 3 2 1 3 1 1 1 1 1 2 1 1 3 2 1 2 4 1 3 4 3 3 ## [118] 3 3 3 1 3 2 3 1 1 2 1 1 1 3 4 2 4 2 2 1 1 1 3 2 3 2 4 3 1 4 2 3 2 1 3 4 1 4 4 ## [157] 2 1 2 4 3 4 1 2 1 3 2 3 3 2 3 3 2 1 2 4 3 1 3 3 3 1 1 3 3 4 4 1 4 3 2 4 4 4 4 ## [196] 3 1 2 3 3 1 1 1 2 3 3 4 3 1 4 2 1 4 1 4 4 3 3 3 3 1 2 4 4 1 1 1 2 3 1 3 3 1 3 ## [235] 2 3 4 1 4 1 1 3 2 3 2 4 4 1 4 1 4 2 2 1 4 1 2 2 1 4 2 2 3 2 3 3 1 2 1 2 4 2 2 ## [274] 1 4 2 4 1 3 2 1 4 4 4 2 4 3 1 3 3 1 1 2 1 1 3 2 1 4 4 1 2 3 3 2 2 4 1 1 2 3 1 ## [313] 2 2 4 2 2 2 2 3 3 2 1 4 2 1 1 4 2 3 4 2 1 4 1 1 1 3 4 4 1 1 1 3 4 4 1 2 2 3 3 ## [352] 3 2 1 2 4 1 4 4 1 3 3 3 3 3 3 3 1 2 1 2 1 1 2 2 4 1 1 3 1 1 1 1 1 1 1 3 4 3 2 ## [391] 1 3 1 3 3 2 2 1 2 2 3 3 3 4 3 2 3 1 4 1 3 1 1 1 1 2 1 3 1 2 1 2 1 2 4 3 1 1 1 ## [430] 1 2 2 1 2 1 1 4 3 1 3 1 2 3 4 3 1 1 1 1 1 1 2 1 2 2 4 1 3 4 4 4 1 4 3 4 1 1 1 ## [469] 4 3 1 1 1 4 3 1 2 4 4 3 2 1 3 1 1 1 1 1 3 4 2 2 3 3 2 3 3 2 4 3 2 2 3 4 2 4 1 ## [508] 1 2 4 4 1 4 2 2 2 4 4 4 3 2 2 3 4 2 2 1 2 1 1 2 1 1 3 1 3 1 1 1 2 4 2 4 3 1 4 ## [547] 3 3 1 3 1 2 3 1 1 4 1 4 4 4 3 1 1 3 1 2 1 3 3 1 2 2 2 2 1 1 3 2 4 2 1 3 4 4 4 ## [586] 1 4 3 2 1 4 2 2 2 3 1 1 1 1 1 1 3 1 4 3 1 1 1 1 1 2 2 4 3 4 2 3 2 4 1 2 2 3 1 ## [625] 2 3 1 1 3 3 4 1 2 1 4 4 3 2 4 1 1 3 3 3 2 2 1 1 4 1 1 1 3 2 1 2 2 1 4 2 4 1 4 ## [664] 4 3 1 3 4 3 4 3 1 4 2 4 3 4 1 2 2 2 1 1 3 3 2 2 1 1 1 4 4 2 4 2 4 2 1 3 3 2 3 ## [703] 1 2 3 3 4 2 4 2 2 3 4 1 2 4 2 4 1 3 3 1 1 3 1 3 1 1 2 2 2 4 2 2 2 3 1 4 2 1 4 ## [742] 2 1 4 4 4 1 1 2 3 3 1 2 1 4 1 4 1 1 3 2 4 4 4 2 3 1 1 ## Levels: 1 2 3 4 pm_new &lt;- pm_new %&gt;% mutate(pregnant=cut(pregnant,breaks=quantile(pregnant), include.lowest=TRUE,labels=1:4)) str(pm_new$pregnant) ## Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 3 1 4 1 1 3 2 4 2 4 ... So we could now build a model using this newly binned information. It might not make a big difference although it might give some insight into to what extent the number of pregnancies influence the model. More specifically, which bin of the pregnancy variable would be influential - if it all ? First, let’s create a model using the untransformed data as found in the PimaIndiansDiabetes data frame. control &lt;- trainControl(method=&quot;cv&quot;,number=5) cutmodel1 &lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&quot;glm&quot;, trControl=control) summary(cutmodel1$finalModel) ## ## Call: ## NULL ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.5566 -0.7274 -0.4159 0.7267 2.9297 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -8.4046964 0.7166359 -11.728 &lt; 2e-16 *** ## pregnant 0.1231823 0.0320776 3.840 0.000123 *** ## glucose 0.0351637 0.0037087 9.481 &lt; 2e-16 *** ## pressure -0.0132955 0.0052336 -2.540 0.011072 * ## triceps 0.0006190 0.0068994 0.090 0.928515 ## insulin -0.0011917 0.0009012 -1.322 0.186065 ## mass 0.0897010 0.0150876 5.945 2.76e-09 *** ## pedigree 0.9451797 0.2991475 3.160 0.001580 ** ## age 0.0148690 0.0093348 1.593 0.111192 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 993.48 on 767 degrees of freedom ## Residual deviance: 723.45 on 759 degrees of freedom ## AIC: 741.45 ## ## Number of Fisher Scoring iterations: 5 varImp(cutmodel1) ## glm variable importance ## ## Overall ## glucose 100.00 ## mass 62.35 ## pregnant 39.93 ## pedigree 32.69 ## pressure 26.09 ## age 16.01 ## insulin 13.12 ## triceps 0.00 We definitely see that pregnancy is an important feature here although, again, we don’t know what sub population might have influence, if any, on the overall model. Let’s rerun this analysis on the transformed version of the data frame. cutmodel2 &lt;- train(diabetes~., data=pm_new, method=&quot;glm&quot;, trControl=control) summary(cutmodel2$finalModel) ## ## Call: ## NULL ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.5051 -0.7199 -0.4201 0.7282 2.9632 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -8.2900923 0.7432705 -11.154 &lt; 2e-16 *** ## pregnant2 0.2020865 0.2679622 0.754 0.45075 ## pregnant3 0.4018240 0.2722524 1.476 0.13996 ## pregnant4 1.1252206 0.2909948 3.867 0.00011 *** ## glucose 0.0349990 0.0037156 9.420 &lt; 2e-16 *** ## pressure -0.0130319 0.0052424 -2.486 0.01292 * ## triceps 0.0009007 0.0069532 0.130 0.89693 ## insulin -0.0012053 0.0009026 -1.335 0.18173 ## mass 0.0890646 0.0151221 5.890 3.87e-09 *** ## pedigree 0.9126652 0.2990451 3.052 0.00227 ** ## age 0.0150488 0.0093797 1.604 0.10863 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 993.48 on 767 degrees of freedom ## Residual deviance: 722.27 on 757 degrees of freedom ## AIC: 744.27 ## ## Number of Fisher Scoring iterations: 5 varImp(cutmodel2) ## glm variable importance ## ## Overall ## glucose 100.000 ## mass 62.004 ## pregnant4 40.229 ## pedigree 31.457 ## pressure 25.364 ## age 15.876 ## pregnant3 14.493 ## insulin 12.981 ## pregnant2 6.724 ## triceps 0.000 This is interesting in that perhaps it’s the 4th bin of the pregnancy feature that has the major influence on the model. So if we look at the distribution of positive cases we see that in the positive cases there are more people from the 4th bin: pm_new %&gt;% group_by(diabetes,pregnant) %&gt;% count() %&gt;% ggplot(aes(x=diabetes,y=n,fill=pregnant)) + geom_bar(stat=&quot;identity&quot;) "],
["using-external-ml-frameworks.html", "Chapter 12 Using External ML Frameworks 12.1 Using h2o 12.2 Create Some h20 Models 12.3 Saving A Model 12.4 Using The h2o Auto ML Feature 12.5 Launching a Job", " Chapter 12 Using External ML Frameworks There are a number of companies that provide easy access to Machine Learning services including Google, Amazon, Data Robot, and H2o. In particular, the company H20.ai provides frameworks for accessible Machine Learning by experts and non-experts. They promote the idea of “citizen data science” which seeks to lower barriers to participation in the world of AI. While they have a commercial product, they also provide an open source tool: H2O is a fully open source, distributed in-memory machine learning platform with linear scalability. H2O supports the most widely used statistical &amp; machine learning algorithms including gradient boosted machines, generalized linear models, deep learning and more. Moreover, H2O provides access to an “Auto ML” service that selects methods appropriate to a given data set. This is useful to help jump start ideas. H2O also has an industry leading AutoML functionality that automatically runs through all the algorithms and their hyperparameters to produce a leaderboard of the best models. The H2O platform is used by over 18,000 organizations globally and is extremely popular in both the R &amp; Python communities. Better yet, there is an R package called, somewhat unimaginatively, “h2o” &quot;which provides: R interface for ‘H2O’, the scalable open source machine learning platform that offers parallelized implementations of many supervised and unsupervised machine learning algorithms such as Generalized Linear Models, Gradient Boosting Machines (including XGBoost), Random Forests, Deep Neural Networks (Deep Learning), Stacked Ensembles, Naive Bayes, Cox Proportional Hazards, K-Means, PCA, Word2Vec, as well as a fully automatic machine learning algorithm (AutoML). 12.1 Using h2o The package must first be installed which can done using the install.packages function (or the menu in R Studio). Loading the library is done just as you would any other library. library(h2o) The goal of using this library is not to replace the methods available to you in R but, just like the caret package, seeks to provide a uniform interface for a variety of underlying methods. This includes common methods including an “Auto ML” service that picks methods for you. Let’s apply h2o to our work. The underlying h2o architecture uses a “running instance” concept that can be initialized and accessed from R. You initialize it once per interactive session. h2o.init() H2O is not running yet, starting it now... Note: In case of errors look at the following log files: /var/folders/wh/z0v5hqgx3dzdfgz47lnbr_3w0000gn/T//RtmpRehHby/h2o_esteban_started_from_r.out /var/folders/wh/z0v5hqgx3dzdfgz47lnbr_3w0000gn/T//RtmpRehHby/h2o_esteban_started_from_r.err java version &quot;1.8.0_131&quot; Java(TM) SE Runtime Environment (build 1.8.0_131-b11) Java HotSpot(TM) 64-Bit Server VM (build 25.131-b11, mixed mode) Starting H2O JVM and connecting: .. Connection successful! R is connected to the H2O cluster: H2O cluster uptime: 2 seconds 577 milliseconds H2O cluster timezone: America/New_York H2O data parsing timezone: UTC H2O cluster version: 3.26.0.2 H2O cluster version age: 5 months and 5 days !!! H2O cluster name: H2O_started_from_R_esteban_pgj795 H2O cluster total nodes: 1 H2O cluster total memory: 1.78 GB H2O cluster total cores: 4 H2O cluster allowed cores: 4 H2O cluster healthy: TRUE H2O Connection ip: localhost H2O Connection port: 54321 H2O Connection proxy: NA H2O Internal Security: FALSE H2O API Extensions: Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4 R Version: R version 3.5.3 (2019-03-11) Your H2O cluster version is too old (5 months and 5 days)! Please download and install the latest version from http://h2o.ai/download/ Show in New WindowClear OutputExpand/Collapse Output |===========================================================| 100% Console~/Dropbox/ML/bookdown-minimal/ Console Terminal R Markdown ~/Dropbox/ML/bookdown-minimal/ Once the h2o environment has been initialized then work can begin. This will take the form of using R functions provided by the h2o package to read in data and prepare it for use with various methods. Let’s repeat the regression on mtcars using h2o functions. Since mtcars is already available in the the R environment we can easily import it into h2o. # Import mtcars mtcars_h2o_df &lt;- as.h2o(mtcars) # Idenitfy the variable to be predicted y &lt;- &quot;mpg&quot; # Put the predictor names into a vector x &lt;- setdiff(colnames(mtcars_h2o_df),y) 12.2 Create Some h20 Models Now let’s create some training and test data sets. We could do this ourselves using conventional R commands or helper functions from the caret package. However, the h2o package provides its own set of helpers. splits &lt;- h2o.splitFrame(mtcars_h2o_df, ratios=0.8,seed=1) train_h2o &lt;- splits[[1]] test_h2o &lt;- splits[[2]] train mpg cyl disp hp drat wt qsec vs am gear carb 1 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 2 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 3 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 4 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 5 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 6 14.3 8 360 245 3.21 3.570 15.84 0 0 3 4 [29 rows x 11 columns] Now let’s create a model. We’ll use the Generalized Linear Model function from h2o. It is important to note that this function is implemented from within h2o. That is, we are not in anyway using any existing R packages to do this nor are we using anything from the care package. Here we’ll request a 4-Fold, Cross Validation step as part of the model assembly. h2o_glm_model &lt;- h2o.glm(y=y,x=x,train_h2o,nfolds=4) summary(h2o_glm_model) # |===========================================================| 100% Model Details: ============== H2ORegressionModel: glm Model Key: GLM_model_R_1577927955348_1 GLM Model: summary family link regularization 1 gaussian identity Elastic Net (alpha = 0.5, lambda = 1.0664 ) number_of_predictors_total number_of_active_predictors 1 10 9 number_of_iterations training_frame 1 1 RTMP_sid_bf87_673 H2ORegressionMetrics: glm ** Reported on training data. ** MSE: 6.185253 RMSE: 2.487017 MAE: 1.940791 RMSLE: 0.1135999 Mean Residual Deviance : 6.185253 R^2 : 0.8392098 Null Deviance :1115.568 Null D.o.F. :28 Residual Deviance :179.3723 Residual D.o.F. :19 AIC :157.1413 H2ORegressionMetrics: glm ** Reported on cross-validation data. ** ** 4-fold cross-validation on training data (Metrics computed for combined holdout predictions) ** MSE: 9.520966 RMSE: 3.085606 MAE: 2.478209 RMSLE: 0.1462186 Mean Residual Deviance : 9.520966 R^2 : 0.7524955 Null Deviance :1194.241 Null D.o.F. :28 Residual Deviance :276.108 Residual D.o.F. :19 AIC :169.6498 Cross-Validation Metrics Summary: mean sd cv_1_valid cv_2_valid mae 2.4294133 0.4533141 3.3255308 2.6851656 mean_residual_deviance 9.431254 2.5729601 11.924386 13.098149 mse 9.431254 2.5729601 11.924386 13.098149 null_deviance 298.56015 73.43793 429.7452 322.68143 r2 0.7409388 0.041704282 0.6850852 0.70873845 residual_deviance 69.027 21.894964 107.31948 65.490746 rmse 2.9984744 0.46925735 3.4531705 3.6191366 rmsle 0.1363086 0.026307607 0.19733523 0.13199924 cv_3_valid cv_4_valid mae 1.6167169 2.0902402 mean_residual_deviance 3.6748443 9.027636 mse 3.6748443 9.027636 null_deviance 139.37894 302.43506 r2 0.83919096 0.73074067 residual_deviance 22.049067 81.24872 rmse 1.9169884 3.0046024 rmsle 0.0983199 0.11758001 Scoring History: timestamp duration iterations negative_log_likelihood 1 2020-01-01 20:21:13 0.000 sec 0 1115.56759 objective 1 38.46785 Variable Importances: (Extract with `h2o.varimp`) ================================================= variable relative_importance scaled_importance percentage 1 wt 1.19294625 1.00000000 0.208632891 2 cyl 0.92951526 0.77917615 0.162561772 3 disp 0.78424629 0.65740287 0.137155861 4 hp 0.69294345 0.58086729 0.121188021 5 carb 0.62287613 0.52213261 0.108934035 6 am 0.55736672 0.46721864 0.097477175 7 vs 0.46246830 0.38766902 0.080880507 8 drat 0.45447201 0.38096604 0.079482047 9 gear 0.02108593 0.01767551 0.003687692 10 qsec 0.00000000 0.00000000 0.000000000 Now we can do a prediction on the object against the test set. (h2o_glm_preds &lt;- h2o.predict(h2o_glm_model,test_h2o)) # h2o.performance(h2o_glm_model,test_h2o) |===========================================================| 100% predict 1 25.93186 2 20.67344 3 24.38237 [3 rows x 1 column] H2ORegressionMetrics: glm MSE: 6.762548 RMSE: 2.60049 MAE: 2.495891 RMSLE: 0.1076563 Mean Residual Deviance : 6.762548 R^2 : -2.052306 Null Deviance :10.87611 Null D.o.F. :2 Residual Deviance :20.28765 Residual D.o.F. :-7 AIC :36.24783 12.3 Saving A Model You can save the contents of any h2o generated model by using the h2o.saveModel() function. You could extract pieces of information from the S4 object but saving the model is easy to do - as is reading it back in. model_path &lt;- h2o.saveModel(h2o_glm_model,path=getwd(),force=TRUE) # If you need to load a previously saved model saved_model &lt;- h2o.loadModel(model_path) 12.4 Using The h2o Auto ML Feature Are you curious as to what model might be the “best” for your data ? This is a very fertile field of research that keeps growing and some feel will one be the dominant technology in ML - where a model picks a model. Sounds odd but that is where it is heading. Check the current h2o Auto ML documentation for more details. For now, most of the Auto ML services use a set of heuristics to examine data and then find the most appropriate method to build a model. The currently supported method implementations in the opensource version include: three pre-specified XGBoost GBM (Gradient Boosting Machine) models a fixed grid of GLMs, a default Random Forest (DRF) five pre-specified H2O GBMs a near-default Deep Neural Net an Extremely Randomized Forest (XRT) a random grid of XGBoost GBMs a random grid of H2O GBMs a random grid of Deep Neural Nets. 12.5 Launching a Job Of course, it all begins with the idea of specifying a performance metric such as RMSE or the area under a ROC curve. The idea here is that we specify some input, apply transformations, create a test/train pair, and then call the h2o auto function. h2o_auto_mtcars &lt;- h2o.automl(y = y, x = x, training_frame = train_h2o, leaderboard_frame = test_h2o, max_runtime_secs = 60, seed = 1, sort_metric = &quot;RMSE&quot;, project_name = &quot;mtcars&quot;) Let’s check out the object that is returned. It is an S4 object in R which means that it has “slots” which can be accessed via the “@” operator. slotNames(h2o_auto_mtcars) h2o_auto_mtcars@leaderboard [1] &quot;project_name&quot; &quot;leader&quot; &quot;leaderboard&quot; &quot;event_log&quot; [5] &quot;training_info&quot; model_id 1 GBM_grid_1_AutoML_20200101_202413_model_53 2 DeepLearning_1_AutoML_20200101_202413 3 XGBoost_grid_1_AutoML_20200101_202413_model_14 4 XGBoost_grid_1_AutoML_20200101_202413_model_5 5 GBM_grid_1_AutoML_20200101_202413_model_52 6 GBM_grid_1_AutoML_20200101_202413_model_16 mean_residual_deviance rmse mse mae rmsle 1 0.2147231 0.4633822 0.2147231 0.4138704 0.02128094 2 0.3734735 0.6111248 0.3734735 0.5076945 0.02559229 3 0.4586288 0.6772213 0.4586288 0.6318582 0.03060372 4 0.4787276 0.6919014 0.4787276 0.5760670 0.03091116 5 0.7571985 0.8701716 0.7571985 0.7831136 0.03729489 6 0.7772694 0.8816289 0.7772694 0.8020415 0.03896093 [89 rows x 6 columns] Stop the H2O instance h2o.shutdown(prompt=FALSE) "]
]
