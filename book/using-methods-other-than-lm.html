<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Using Methods Other Than lm | Predictive Learning in R</title>
  <meta name="description" content="Chapter 9 Using Methods Other Than lm | Predictive Learning in R" />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Using Methods Other Than lm | Predictive Learning in R" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Using Methods Other Than lm | Predictive Learning in R" />
  
  
  

<meta name="author" content="Steve Pittard - wsp@emory.edu" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="decision-trees.html"/>
<link rel="next" href="picking-the-best-model.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#machine-learning"><i class="fa fa-check"></i><b>1.1</b> Machine Learning</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#predictive-modeling"><i class="fa fa-check"></i><b>1.2</b> Predictive Modeling</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#in-sample-vs-out-of-sample-error"><i class="fa fa-check"></i><b>1.3</b> In-Sample vs Out-Of-Sample Error</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#performance-metrics"><i class="fa fa-check"></i><b>1.4</b> Performance Metrics</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#black-box"><i class="fa fa-check"></i><b>1.5</b> Black Box</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html"><i class="fa fa-check"></i><b>2</b> Predictive / Supervised Learning</a><ul>
<li class="chapter" data-level="2.1" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#explanation-vs-prediction"><i class="fa fa-check"></i><b>2.1</b> Explanation vs Prediction</a><ul>
<li class="chapter" data-level="2.1.1" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#titanic-data"><i class="fa fa-check"></i><b>2.1.1</b> Titanic Data</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#bias-vs-variance"><i class="fa fa-check"></i><b>2.2</b> Bias vs Variance</a><ul>
<li class="chapter" data-level="2.2.1" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#bias"><i class="fa fa-check"></i><b>2.2.1</b> Bias</a></li>
<li class="chapter" data-level="2.2.2" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#variance"><i class="fa fa-check"></i><b>2.2.2</b> Variance</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#overfitting-and-underfitting"><i class="fa fa-check"></i><b>2.3</b> Overfitting and Underfitting</a></li>
<li class="chapter" data-level="2.4" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#some-important-terminology"><i class="fa fa-check"></i><b>2.4</b> Some Important Terminology</a></li>
<li class="chapter" data-level="2.5" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#levels-of-measurement"><i class="fa fa-check"></i><b>2.5</b> Levels of Measurement</a><ul>
<li class="chapter" data-level="2.5.1" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#nominal"><i class="fa fa-check"></i><b>2.5.1</b> Nominal</a></li>
<li class="chapter" data-level="2.5.2" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#ordinal"><i class="fa fa-check"></i><b>2.5.2</b> Ordinal</a></li>
<li class="chapter" data-level="2.5.3" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#interval"><i class="fa fa-check"></i><b>2.5.3</b> Interval</a></li>
<li class="chapter" data-level="2.5.4" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#ratio"><i class="fa fa-check"></i><b>2.5.4</b> Ratio</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="a-motivating-example.html"><a href="a-motivating-example.html"><i class="fa fa-check"></i><b>3</b> A Motivating Example</a><ul>
<li class="chapter" data-level="3.1" data-path="a-motivating-example.html"><a href="a-motivating-example.html#a-more-dwtailed-workflow"><i class="fa fa-check"></i><b>3.1</b> A More Dwtailed Workflow</a></li>
<li class="chapter" data-level="3.2" data-path="a-motivating-example.html"><a href="a-motivating-example.html#visualizations"><i class="fa fa-check"></i><b>3.2</b> Visualizations</a><ul>
<li class="chapter" data-level="3.2.1" data-path="a-motivating-example.html"><a href="a-motivating-example.html#scatterplots"><i class="fa fa-check"></i><b>3.2.1</b> Scatterplots</a></li>
<li class="chapter" data-level="3.2.2" data-path="a-motivating-example.html"><a href="a-motivating-example.html#boxplots"><i class="fa fa-check"></i><b>3.2.2</b> Boxplots</a></li>
<li class="chapter" data-level="3.2.3" data-path="a-motivating-example.html"><a href="a-motivating-example.html#histograms"><i class="fa fa-check"></i><b>3.2.3</b> Histograms</a></li>
<li class="chapter" data-level="3.2.4" data-path="a-motivating-example.html"><a href="a-motivating-example.html#tables"><i class="fa fa-check"></i><b>3.2.4</b> Tables</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="a-motivating-example.html"><a href="a-motivating-example.html#correlations"><i class="fa fa-check"></i><b>3.3</b> Correlations</a></li>
<li class="chapter" data-level="3.4" data-path="a-motivating-example.html"><a href="a-motivating-example.html#building-a-model---in-sample-error"><i class="fa fa-check"></i><b>3.4</b> Building A Model - In Sample Error</a></li>
<li class="chapter" data-level="3.5" data-path="a-motivating-example.html"><a href="a-motivating-example.html#out-of-sample-data"><i class="fa fa-check"></i><b>3.5</b> Out Of Sample Data</a></li>
<li class="chapter" data-level="3.6" data-path="a-motivating-example.html"><a href="a-motivating-example.html#some-additional-considerations"><i class="fa fa-check"></i><b>3.6</b> Some Additional Considerations</a></li>
<li class="chapter" data-level="3.7" data-path="a-motivating-example.html"><a href="a-motivating-example.html#other-methods"><i class="fa fa-check"></i><b>3.7</b> Other Methods ?</a></li>
<li class="chapter" data-level="3.8" data-path="a-motivating-example.html"><a href="a-motivating-example.html#summary"><i class="fa fa-check"></i><b>3.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="training-test-data.html"><a href="training-test-data.html"><i class="fa fa-check"></i><b>4</b> Training / Test Data</a><ul>
<li class="chapter" data-level="4.1" data-path="training-test-data.html"><a href="training-test-data.html#cross-fold-validation"><i class="fa fa-check"></i><b>4.1</b> Cross Fold Validation</a></li>
<li class="chapter" data-level="4.2" data-path="training-test-data.html"><a href="training-test-data.html#create-a-function-to-automate-things"><i class="fa fa-check"></i><b>4.2</b> Create A Function To Automate Things</a></li>
<li class="chapter" data-level="4.3" data-path="training-test-data.html"><a href="training-test-data.html#repeated-cross-validation"><i class="fa fa-check"></i><b>4.3</b> Repeated Cross Validation</a></li>
<li class="chapter" data-level="4.4" data-path="training-test-data.html"><a href="training-test-data.html#bootstrap"><i class="fa fa-check"></i><b>4.4</b> Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="caret-package.html"><a href="caret-package.html"><i class="fa fa-check"></i><b>5</b> Caret Package</a><ul>
<li class="chapter" data-level="5.1" data-path="caret-package.html"><a href="caret-package.html#putting-caret-to-work"><i class="fa fa-check"></i><b>5.1</b> Putting caret To Work</a></li>
<li class="chapter" data-level="5.2" data-path="caret-package.html"><a href="caret-package.html#back-to-the-beginning"><i class="fa fa-check"></i><b>5.2</b> Back To The Beginning</a></li>
<li class="chapter" data-level="5.3" data-path="caret-package.html"><a href="caret-package.html#splitting"><i class="fa fa-check"></i><b>5.3</b> Splitting</a></li>
<li class="chapter" data-level="5.4" data-path="caret-package.html"><a href="caret-package.html#calling-the-train-function"><i class="fa fa-check"></i><b>5.4</b> Calling The train() Function</a></li>
<li class="chapter" data-level="5.5" data-path="caret-package.html"><a href="caret-package.html#one-size-fits-all"><i class="fa fa-check"></i><b>5.5</b> One Size Fits All</a></li>
<li class="chapter" data-level="5.6" data-path="caret-package.html"><a href="caret-package.html#hyperparameters"><i class="fa fa-check"></i><b>5.6</b> Hyperparameters</a></li>
<li class="chapter" data-level="5.7" data-path="caret-package.html"><a href="caret-package.html#alternative-calling-sequence"><i class="fa fa-check"></i><b>5.7</b> Alternative Calling Sequence</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="classification-problems.html"><a href="classification-problems.html"><i class="fa fa-check"></i><b>6</b> Classification Problems</a><ul>
<li class="chapter" data-level="6.1" data-path="classification-problems.html"><a href="classification-problems.html#performance-measures"><i class="fa fa-check"></i><b>6.1</b> Performance Measures</a></li>
<li class="chapter" data-level="6.2" data-path="classification-problems.html"><a href="classification-problems.html#important-terminology"><i class="fa fa-check"></i><b>6.2</b> Important Terminology</a></li>
<li class="chapter" data-level="6.3" data-path="classification-problems.html"><a href="classification-problems.html#a-basic-model"><i class="fa fa-check"></i><b>6.3</b> A Basic Model</a></li>
<li class="chapter" data-level="6.4" data-path="classification-problems.html"><a href="classification-problems.html#selecting-the-correct-threshold-alpha"><i class="fa fa-check"></i><b>6.4</b> Selecting The Correct Threshold / Alpha</a><ul>
<li class="chapter" data-level="6.4.1" data-path="classification-problems.html"><a href="classification-problems.html#moving-the-threshold"><i class="fa fa-check"></i><b>6.4.1</b> Moving The Threshold</a></li>
<li class="chapter" data-level="6.4.2" data-path="classification-problems.html"><a href="classification-problems.html#distribution-of-predicted-probabilities"><i class="fa fa-check"></i><b>6.4.2</b> Distribution of Predicted Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="classification-problems.html"><a href="classification-problems.html#hypothesis-testing"><i class="fa fa-check"></i><b>6.5</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="6.6" data-path="classification-problems.html"><a href="classification-problems.html#confusion-matrix"><i class="fa fa-check"></i><b>6.6</b> Confusion Matrix</a><ul>
<li class="chapter" data-level="6.6.1" data-path="classification-problems.html"><a href="classification-problems.html#computing-performance-metrics"><i class="fa fa-check"></i><b>6.6.1</b> Computing Performance Metrics</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="classification-problems.html"><a href="classification-problems.html#picking-the-right-metric"><i class="fa fa-check"></i><b>6.7</b> Picking the Right Metric</a></li>
<li class="chapter" data-level="6.8" data-path="classification-problems.html"><a href="classification-problems.html#wait.-where-are-we"><i class="fa fa-check"></i><b>6.8</b> Wait. Where Are We ?</a></li>
<li class="chapter" data-level="6.9" data-path="classification-problems.html"><a href="classification-problems.html#better-ways-to-compute-the-roc-curve"><i class="fa fa-check"></i><b>6.9</b> Better Ways To Compute The ROC Curve</a></li>
<li class="chapter" data-level="6.10" data-path="classification-problems.html"><a href="classification-problems.html#roc-curve-summary"><i class="fa fa-check"></i><b>6.10</b> ROC Curve Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="classification-example.html"><a href="classification-example.html"><i class="fa fa-check"></i><b>7</b> Classification Example</a><ul>
<li class="chapter" data-level="7.1" data-path="classification-example.html"><a href="classification-example.html#exploratory-plots"><i class="fa fa-check"></i><b>7.1</b> Exploratory Plots</a></li>
<li class="chapter" data-level="7.2" data-path="classification-example.html"><a href="classification-example.html#generalized-linear-models"><i class="fa fa-check"></i><b>7.2</b> Generalized Linear Models</a></li>
<li class="chapter" data-level="7.3" data-path="classification-example.html"><a href="classification-example.html#random-forests"><i class="fa fa-check"></i><b>7.3</b> Random Forests</a></li>
<li class="chapter" data-level="7.4" data-path="classification-example.html"><a href="classification-example.html#target-variable-format"><i class="fa fa-check"></i><b>7.4</b> Target Variable Format</a></li>
<li class="chapter" data-level="7.5" data-path="classification-example.html"><a href="classification-example.html#addressing-class-imbalance"><i class="fa fa-check"></i><b>7.5</b> Addressing Class Imbalance</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>8</b> Decision Trees</a><ul>
<li class="chapter" data-level="8.1" data-path="decision-trees.html"><a href="decision-trees.html#advantages"><i class="fa fa-check"></i><b>8.1</b> Advantages</a></li>
<li class="chapter" data-level="8.2" data-path="decision-trees.html"><a href="decision-trees.html#a-classification-example"><i class="fa fa-check"></i><b>8.2</b> A Classification Example</a></li>
<li class="chapter" data-level="8.3" data-path="decision-trees.html"><a href="decision-trees.html#digging-deeper"><i class="fa fa-check"></i><b>8.3</b> Digging Deeper</a><ul>
<li class="chapter" data-level="8.3.1" data-path="decision-trees.html"><a href="decision-trees.html#evaluating-performance"><i class="fa fa-check"></i><b>8.3.1</b> Evaluating performance</a></li>
<li class="chapter" data-level="8.3.2" data-path="decision-trees.html"><a href="decision-trees.html#tree-splitting"><i class="fa fa-check"></i><b>8.3.2</b> Tree Splitting</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="decision-trees.html"><a href="decision-trees.html#gini-index"><i class="fa fa-check"></i><b>8.4</b> Gini Index</a></li>
<li class="chapter" data-level="8.5" data-path="decision-trees.html"><a href="decision-trees.html#regression-trees"><i class="fa fa-check"></i><b>8.5</b> Regression Trees</a><ul>
<li class="chapter" data-level="8.5.1" data-path="decision-trees.html"><a href="decision-trees.html#performance-measure"><i class="fa fa-check"></i><b>8.5.1</b> Performance Measure</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="decision-trees.html"><a href="decision-trees.html#parameters-vs-hyperparameters"><i class="fa fa-check"></i><b>8.6</b> Parameters vs Hyperparameters</a></li>
<li class="chapter" data-level="8.7" data-path="decision-trees.html"><a href="decision-trees.html#grid-searching"><i class="fa fa-check"></i><b>8.7</b> Grid Searching</a></li>
<li class="chapter" data-level="8.8" data-path="decision-trees.html"><a href="decision-trees.html#bagged-trees"><i class="fa fa-check"></i><b>8.8</b> Bagged Trees</a></li>
<li class="chapter" data-level="8.9" data-path="decision-trees.html"><a href="decision-trees.html#random-forests-1"><i class="fa fa-check"></i><b>8.9</b> Random Forests</a></li>
<li class="chapter" data-level="8.10" data-path="decision-trees.html"><a href="decision-trees.html#boosted-trees"><i class="fa fa-check"></i><b>8.10</b> Boosted Trees</a></li>
<li class="chapter" data-level="8.11" data-path="decision-trees.html"><a href="decision-trees.html#using-caret"><i class="fa fa-check"></i><b>8.11</b> Using caret</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="using-methods-other-than-lm.html"><a href="using-methods-other-than-lm.html"><i class="fa fa-check"></i><b>9</b> Using Methods Other Than lm</a><ul>
<li class="chapter" data-level="9.1" data-path="using-methods-other-than-lm.html"><a href="using-methods-other-than-lm.html#parameters-vs-hyperparameters-1"><i class="fa fa-check"></i><b>9.1</b> Parameters vs Hyperparameters</a></li>
<li class="chapter" data-level="9.2" data-path="using-methods-other-than-lm.html"><a href="using-methods-other-than-lm.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>9.2</b> Hyperparameter Tuning</a><ul>
<li class="chapter" data-level="9.2.1" data-path="using-methods-other-than-lm.html"><a href="using-methods-other-than-lm.html#multiple-hyperparameters"><i class="fa fa-check"></i><b>9.2.1</b> Multiple Hyperparameters ?</a></li>
<li class="chapter" data-level="9.2.2" data-path="using-methods-other-than-lm.html"><a href="using-methods-other-than-lm.html#custom-tuning-grid"><i class="fa fa-check"></i><b>9.2.2</b> Custom Tuning Grid</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="using-methods-other-than-lm.html"><a href="using-methods-other-than-lm.html#using-validation-data-sets"><i class="fa fa-check"></i><b>9.3</b> Using Validation Data Sets</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html"><i class="fa fa-check"></i><b>10</b> Picking The Best Model</a><ul>
<li class="chapter" data-level="10.1" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#an-example"><i class="fa fa-check"></i><b>10.1</b> An Example</a></li>
<li class="chapter" data-level="10.2" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#more-comparisons"><i class="fa fa-check"></i><b>10.2</b> More Comparisons</a></li>
<li class="chapter" data-level="10.3" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#using-the-resamples-function"><i class="fa fa-check"></i><b>10.3</b> Using the resamples() function</a></li>
<li class="chapter" data-level="10.4" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#model-performance"><i class="fa fa-check"></i><b>10.4</b> Model Performance</a></li>
<li class="chapter" data-level="10.5" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#feature-selection"><i class="fa fa-check"></i><b>10.5</b> Feature Selection</a><ul>
<li class="chapter" data-level="10.5.1" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#recursive-feature-elimination"><i class="fa fa-check"></i><b>10.5.1</b> Recursive Feature Elimination</a></li>
<li class="chapter" data-level="10.5.2" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#redundant-feature-removal"><i class="fa fa-check"></i><b>10.5.2</b> Redundant Feature Removal</a></li>
<li class="chapter" data-level="10.5.3" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#feature-importance"><i class="fa fa-check"></i><b>10.5.3</b> Feature Importance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="data-pre-processing.html"><a href="data-pre-processing.html"><i class="fa fa-check"></i><b>11</b> Data Pre Processing</a><ul>
<li class="chapter" data-level="11.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#types-of-pre-processing"><i class="fa fa-check"></i><b>11.1</b> Types of Pre Processing</a></li>
<li class="chapter" data-level="11.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#missing-values"><i class="fa fa-check"></i><b>11.2</b> Missing Values</a><ul>
<li class="chapter" data-level="11.2.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#finding-rows-with-missing-data"><i class="fa fa-check"></i><b>11.2.1</b> Finding Rows with Missing Data</a></li>
<li class="chapter" data-level="11.2.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#finding-columns-with-missing-data"><i class="fa fa-check"></i><b>11.2.2</b> Finding Columns With Missing Data</a></li>
<li class="chapter" data-level="11.2.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#use-the-median-approach"><i class="fa fa-check"></i><b>11.2.3</b> Use the Median Approach</a></li>
<li class="chapter" data-level="11.2.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#package-based-approach"><i class="fa fa-check"></i><b>11.2.4</b> Package-based Approach</a></li>
<li class="chapter" data-level="11.2.5" data-path="data-pre-processing.html"><a href="data-pre-processing.html#using-caret-1"><i class="fa fa-check"></i><b>11.2.5</b> Using caret</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#scaling"><i class="fa fa-check"></i><b>11.3</b> Scaling</a><ul>
<li class="chapter" data-level="11.3.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#methods-that-benefit-from-scaling"><i class="fa fa-check"></i><b>11.3.1</b> Methods That Benefit From Scaling</a></li>
<li class="chapter" data-level="11.3.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#methods-that-do-not-require-scaling"><i class="fa fa-check"></i><b>11.3.2</b> Methods That Do Not Require Scaling</a></li>
<li class="chapter" data-level="11.3.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#how-to-scale"><i class="fa fa-check"></i><b>11.3.3</b> How To Scale</a></li>
<li class="chapter" data-level="11.3.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#order-of-processing"><i class="fa fa-check"></i><b>11.3.4</b> Order of Processing</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#low-variance-variables"><i class="fa fa-check"></i><b>11.4</b> Low Variance Variables</a></li>
<li class="chapter" data-level="11.5" data-path="data-pre-processing.html"><a href="data-pre-processing.html#pca---principal-components-analysis"><i class="fa fa-check"></i><b>11.5</b> PCA - Principal Components Analysis</a><ul>
<li class="chapter" data-level="11.5.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#identify-the-factors"><i class="fa fa-check"></i><b>11.5.1</b> Identify The Factors</a></li>
<li class="chapter" data-level="11.5.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#check-for-high-correlations"><i class="fa fa-check"></i><b>11.5.2</b> Check For High Correlations</a></li>
<li class="chapter" data-level="11.5.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#so-why-use-pca"><i class="fa fa-check"></i><b>11.5.3</b> So Why Use PCA ?</a></li>
<li class="chapter" data-level="11.5.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#check-the-biplot"><i class="fa fa-check"></i><b>11.5.4</b> Check The BiPlot</a></li>
<li class="chapter" data-level="11.5.5" data-path="data-pre-processing.html"><a href="data-pre-processing.html#check-the-screeplot"><i class="fa fa-check"></i><b>11.5.5</b> Check The ScreePlot</a></li>
<li class="chapter" data-level="11.5.6" data-path="data-pre-processing.html"><a href="data-pre-processing.html#use-the-transformed-data"><i class="fa fa-check"></i><b>11.5.6</b> Use The Transformed Data</a></li>
<li class="chapter" data-level="11.5.7" data-path="data-pre-processing.html"><a href="data-pre-processing.html#pls"><i class="fa fa-check"></i><b>11.5.7</b> PLS</a></li>
<li class="chapter" data-level="11.5.8" data-path="data-pre-processing.html"><a href="data-pre-processing.html#summary-1"><i class="fa fa-check"></i><b>11.5.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="data-pre-processing.html"><a href="data-pre-processing.html#order-of-pre-processing"><i class="fa fa-check"></i><b>11.6</b> Order of Pre-Processing</a></li>
<li class="chapter" data-level="11.7" data-path="data-pre-processing.html"><a href="data-pre-processing.html#identifying-redundant-features"><i class="fa fa-check"></i><b>11.7</b> Identifying Redundant Features</a><ul>
<li class="chapter" data-level="11.7.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#highly-correlated-variables"><i class="fa fa-check"></i><b>11.7.1</b> Highly Correlated Variables</a></li>
<li class="chapter" data-level="11.7.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#ranking-features"><i class="fa fa-check"></i><b>11.7.2</b> Ranking Features</a></li>
<li class="chapter" data-level="11.7.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#feature-selection-1"><i class="fa fa-check"></i><b>11.7.3</b> Feature Selection</a></li>
</ul></li>
<li class="chapter" data-level="11.8" data-path="data-pre-processing.html"><a href="data-pre-processing.html#handling-categories"><i class="fa fa-check"></i><b>11.8</b> Handling Categories</a><ul>
<li class="chapter" data-level="11.8.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#examples"><i class="fa fa-check"></i><b>11.8.1</b> Examples</a></li>
<li class="chapter" data-level="11.8.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#admissions-data"><i class="fa fa-check"></i><b>11.8.2</b> Admissions Data</a></li>
<li class="chapter" data-level="11.8.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#is-rank-a-category"><i class="fa fa-check"></i><b>11.8.3</b> Is Rank A Category ?</a></li>
<li class="chapter" data-level="11.8.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#relationship-to-one-hot-encoding"><i class="fa fa-check"></i><b>11.8.4</b> Relationship To One Hot Encoding</a></li>
</ul></li>
<li class="chapter" data-level="11.9" data-path="data-pre-processing.html"><a href="data-pre-processing.html#binning"><i class="fa fa-check"></i><b>11.9</b> Binning</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html"><i class="fa fa-check"></i><b>12</b> Using External ML Frameworks</a><ul>
<li class="chapter" data-level="12.1" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#using-h2o"><i class="fa fa-check"></i><b>12.1</b> Using h2o</a></li>
<li class="chapter" data-level="12.2" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#create-some-h20-models"><i class="fa fa-check"></i><b>12.2</b> Create Some h20 Models</a></li>
<li class="chapter" data-level="12.3" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#saving-a-model"><i class="fa fa-check"></i><b>12.3</b> Saving A Model</a></li>
<li class="chapter" data-level="12.4" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#using-the-h2o-auto-ml-feature"><i class="fa fa-check"></i><b>12.4</b> Using The h2o Auto ML Feature</a></li>
<li class="chapter" data-level="12.5" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#launching-a-job"><i class="fa fa-check"></i><b>12.5</b> Launching a Job</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Predictive Learning in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="using-methods-other-than-lm" class="section level1">
<h1><span class="header-section-number">Chapter 9</span> Using Methods Other Than lm</h1>
<p>So when considering our inital example of using the <strong>lm</strong> function I think (hope) that I have convinced you that using caret to call lm is the way to go since it generalizes the process of cross fold validation and the presentation of results. It’s also quite convenient in that we can use other methods than lm to do some preductions.</p>
<p>To that end, let’s look at random forests to see if it improves the situation. Note that we aren’t, at least at this point, trying to understand the underlying details and subtleties of any of the alternative functions we might use although that is ultimately very important. However, we’ll defer the conversation until later. Many people are surprised to learn that random Forests (or even a single Decision Tree) can be used to predict a numeric outcome, but they can be. The advantages of using random Forests include the following:</p>
<pre><code>- easy to use
- resistant to overfitting
- accurate use for non linear models</code></pre>
<p>Some problems include:</p>
<pre><code>- the rd function requires setting hyperparameters 
- adjustment of hyperparameters can be specific to the data set
- default vlaues will requie adjustment or &quot;tuning&quot;</code></pre>
<p>For now, let’s set up a “shoot out” between the <strong>lm</strong> function and the <strong>rf</strong> function to see if the latter yields a lower RMSE than the former (or vice versa).</p>
<div class="sourceCode" id="cb372"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb372-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">124</span>)</a>
<a class="sourceLine" id="cb372-2" data-line-number="2"></a>
<a class="sourceLine" id="cb372-3" data-line-number="3">new_idx &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(mtcars<span class="op">$</span>mpg,<span class="dt">p=</span>.<span class="dv">80</span>,<span class="dt">list=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb372-4" data-line-number="4">new_train &lt;-<span class="st"> </span>mtcars[new_idx,]</a>
<a class="sourceLine" id="cb372-5" data-line-number="5">new_test  &lt;-<span class="st"> </span>mtcars[<span class="op">-</span>new_idx,]</a>
<a class="sourceLine" id="cb372-6" data-line-number="6"></a>
<a class="sourceLine" id="cb372-7" data-line-number="7"></a>
<a class="sourceLine" id="cb372-8" data-line-number="8">caret_lm &lt;-<span class="st"> </span><span class="kw">train</span>(mpg <span class="op">~</span><span class="st"> </span>.,</a>
<a class="sourceLine" id="cb372-9" data-line-number="9">                  <span class="dt">data =</span> new_train,</a>
<a class="sourceLine" id="cb372-10" data-line-number="10">                  <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>)</a>
<a class="sourceLine" id="cb372-11" data-line-number="11"></a>
<a class="sourceLine" id="cb372-12" data-line-number="12">caret_lm<span class="op">$</span>results</a></code></pre></div>
<pre><code>##   intercept     RMSE  Rsquared      MAE   RMSESD RsquaredSD    MAESD
## 1      TRUE 4.937432 0.5675623 4.014161 1.333183  0.1958169 1.249055</code></pre>
<div class="sourceCode" id="cb374"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb374-1" data-line-number="1">Metrics<span class="op">::</span><span class="kw">rmse</span>(new_test<span class="op">$</span>mpg, <span class="kw">predict</span>(caret_lm,new_test))</a></code></pre></div>
<pre><code>## [1] 5.087608</code></pre>
<p>So let’s see what the random forest function will give us with the same data. Because we are using caret, all we have to is sub in the desired method which is “rf”.</p>
<div class="sourceCode" id="cb376"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb376-1" data-line-number="1">caret_rf &lt;-<span class="st"> </span><span class="kw">train</span>(mpg <span class="op">~</span><span class="st"> </span>.,</a>
<a class="sourceLine" id="cb376-2" data-line-number="2">                  <span class="dt">data =</span> new_train,</a>
<a class="sourceLine" id="cb376-3" data-line-number="3">                  <span class="dt">method =</span> <span class="st">&quot;rf&quot;</span>)</a>
<a class="sourceLine" id="cb376-4" data-line-number="4"></a>
<a class="sourceLine" id="cb376-5" data-line-number="5">caret_rf<span class="op">$</span>results</a></code></pre></div>
<pre><code>##   mtry     RMSE  Rsquared      MAE   RMSESD RsquaredSD     MAESD
## 1    2 2.767412 0.8678962 2.238987 1.086212 0.06200765 0.7913614
## 2    6 2.673043 0.8756824 2.162140 1.005430 0.06150165 0.7275747
## 3   10 2.803810 0.8587926 2.288413 1.013513 0.06827013 0.7410137</code></pre>
<div class="sourceCode" id="cb378"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb378-1" data-line-number="1">Metrics<span class="op">::</span><span class="kw">rmse</span>(new_test<span class="op">$</span>mpg, <span class="kw">predict</span>(caret_rf,new_test))</a></code></pre></div>
<pre><code>## [1] 2.145429</code></pre>
<p>So the random Forest approach, in this case, produces a much lower RMSE for the test data frame. The larger question relates to why there is more information in the output for the rf model. What is the <strong>mtry</strong> argument and why does it take on three different values during the execution of the function ?</p>
<div class="sourceCode" id="cb380"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb380-1" data-line-number="1">caret_rf</a></code></pre></div>
<pre><code>## Random Forest 
## 
## 28 samples
## 10 predictors
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 28, 28, 28, 28, 28, 28, ... 
## Resampling results across tuning parameters:
## 
##   mtry  RMSE      Rsquared   MAE     
##    2    2.767412  0.8678962  2.238987
##    6    2.673043  0.8756824  2.162140
##   10    2.803810  0.8587926  2.288413
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was mtry = 6.</code></pre>
<p>The <strong>mtry</strong> argument is a <strong>hyperparameter</strong> which represents information that is supplied in the form of an argument prior to the call to the. These parameters might not be something one can intelligently set without some experimentation.</p>
<div id="parameters-vs-hyperparameters-1" class="section level2">
<h2><span class="header-section-number">9.1</span> Parameters vs Hyperparameters</h2>
<p><strong>Model parameters</strong> are things that are generated as part of the modeling process. They are the product or result of model fitting. These might be things like slope and intercept from a linear model - or coefficients.</p>
<p><strong>Hyperparameters</strong> have default values for various arguments but this does not mean that the defaults are appropriate for all cases.</p>
<p>So with <strong>rf</strong> there is a hyperarameter called <strong>mtry</strong> that influences the outcome but is not necessarily something that we know how to optimally set. The <strong>mtry</strong> value is the number of variables that are randomly sampled at each tree split.</p>
<p>If we had called the random Forest function without using caret we could supply a number of values for mtry to try to arrive at the “best” value to produce the lowest RMSE.</p>
<div class="sourceCode" id="cb382"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb382-1" data-line-number="1"><span class="kw">library</span>(randomForest)</a>
<a class="sourceLine" id="cb382-2" data-line-number="2"></a>
<a class="sourceLine" id="cb382-3" data-line-number="3">non_caret_rf &lt;-<span class="st"> </span><span class="kw">randomForest</span>(mpg<span class="op">~</span>.,</a>
<a class="sourceLine" id="cb382-4" data-line-number="4">                             <span class="dt">data=</span> new_train,</a>
<a class="sourceLine" id="cb382-5" data-line-number="5">                             <span class="dt">mtry=</span><span class="dv">3</span>,      <span class="co"># This is the default </span></a>
<a class="sourceLine" id="cb382-6" data-line-number="6">                             <span class="dt">importance=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb382-7" data-line-number="7"></a>
<a class="sourceLine" id="cb382-8" data-line-number="8"><span class="co"># Check out the RMSE of the preditcions </span></a>
<a class="sourceLine" id="cb382-9" data-line-number="9">Metrics<span class="op">::</span><span class="kw">rmse</span>(new_test<span class="op">$</span>mpg,<span class="kw">predict</span>(non_caret_rf,new_test))</a></code></pre></div>
<pre><code>## [1] 1.903625</code></pre>
<p>We could also write a loop to do this for multiple values of mtry.</p>
<div class="sourceCode" id="cb384"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb384-1" data-line-number="1">make_mtcars_rf &lt;-<span class="st"> </span><span class="cf">function</span>(<span class="dt">mtry=</span><span class="dv">3</span>) {</a>
<a class="sourceLine" id="cb384-2" data-line-number="2">    my_rf &lt;-<span class="st"> </span><span class="kw">randomForest</span>(mpg<span class="op">~</span>.,</a>
<a class="sourceLine" id="cb384-3" data-line-number="3">                          <span class="dt">data =</span> new_train,</a>
<a class="sourceLine" id="cb384-4" data-line-number="4">                          <span class="dt">mtry =</span> mtry,</a>
<a class="sourceLine" id="cb384-5" data-line-number="5">                          <span class="dt">importance =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb384-6" data-line-number="6"></a>
<a class="sourceLine" id="cb384-7" data-line-number="7"><span class="co"># Check out the predictions</span></a>
<a class="sourceLine" id="cb384-8" data-line-number="8">    rmse_rf &lt;-<span class="st"> </span>Metrics<span class="op">::</span><span class="kw">rmse</span>(new_test<span class="op">$</span>mpg,<span class="kw">predict</span>(my_rf,new_test))</a>
<a class="sourceLine" id="cb384-9" data-line-number="9"></a>
<a class="sourceLine" id="cb384-10" data-line-number="10">    <span class="kw">return</span>(rmse_rf)</a>
<a class="sourceLine" id="cb384-11" data-line-number="11">}</a></code></pre></div>
<p>The following will call the <strong>randomForest</strong> package 5 times. Starting with the first iteration, the value of <strong>mtry</strong> will be 3, the next time it will be 4, and so on until the last iteration where it will be 8. This is just an experiment to see if varying <strong>mtry</strong> will help minimize the RMSE of our model. We also have to be careful not to pick incorrect values for <strong>mtry</strong> so reading the help page for the <strong>randomForest</strong> package would be helpful. For now, let’s assume that what we are doing is okay.</p>
<div class="sourceCode" id="cb385"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb385-1" data-line-number="1"><span class="kw">sapply</span>(<span class="dv">3</span><span class="op">:</span><span class="dv">8</span>,make_mtcars_rf)</a></code></pre></div>
<pre><code>## [1] 1.982152 2.084413 2.101195 2.124388 2.051716 2.184972</code></pre>
<p>While this is fine, it would be nice if there were an easier way to handle this process. Besides, if we pick another method (e.g. the <strong>ranger</strong> function) then we have to deal with whatever arguments and hyperparameters that method requires.</p>
<p>The above is an example of what we would have to do if we didn’t have someting like caret to help us try out different values of the <strong>mtry</strong> hyperparameter. As we saw from our earlier work, the model <strong>caret_rf</strong> tried three different values of the <strong>mtry</strong> hyperparameter:</p>
<div class="sourceCode" id="cb387"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb387-1" data-line-number="1">caret_rf</a></code></pre></div>
<pre><code>## Random Forest 
## 
## 28 samples
## 10 predictors
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 28, 28, 28, 28, 28, 28, ... 
## Resampling results across tuning parameters:
## 
##   mtry  RMSE      Rsquared   MAE     
##    2    2.767412  0.8678962  2.238987
##    6    2.673043  0.8756824  2.162140
##   10    2.803810  0.8587926  2.288413
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was mtry = 6.</code></pre>
<p>As a matter of fact we can plot this model and it will show us something interesting. In particular, we now see that using values less than 6 predictors / columns / features results in lower RMSE. If we use more, than the RMSE goes up. This is the power of methods that use hyperparamters. If we move through a number of values for the <strong>mtry</strong> then perhaps we can find the best value to get the best result.</p>
<div class="sourceCode" id="cb389"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb389-1" data-line-number="1"><span class="kw">plot</span>(caret_rf)</a></code></pre></div>
<p><img src="biosml_files/figure-html/unnamed-chunk-70-1.png" width="672" /></p>
<p>Telling <strong>caret</strong> to use more values of <strong>mtry</strong> is possible. This will explicitly try all values from 1 - 10 inclusive. Note that the model gets built for each value of <strong>mtry</strong>. This causes the model building process to take longer than it normally would.</p>
<div class="sourceCode" id="cb390"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb390-1" data-line-number="1">caret_rf &lt;-<span class="st"> </span><span class="kw">train</span>(mpg <span class="op">~</span><span class="st"> </span>.,</a>
<a class="sourceLine" id="cb390-2" data-line-number="2">                  <span class="dt">data =</span> new_train,</a>
<a class="sourceLine" id="cb390-3" data-line-number="3">                  <span class="dt">method =</span> <span class="st">&quot;rf&quot;</span>,</a>
<a class="sourceLine" id="cb390-4" data-line-number="4">                  <span class="dt">tuneLength =</span> <span class="dv">10</span>)</a></code></pre></div>
<pre><code>## note: only 9 unique complexity parameters in default grid. Truncating the grid to 9 .</code></pre>
<div class="sourceCode" id="cb392"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb392-1" data-line-number="1">caret_rf<span class="op">$</span>results</a></code></pre></div>
<pre><code>##   mtry     RMSE  Rsquared      MAE    RMSESD RsquaredSD     MAESD
## 1    2 2.654331 0.8487161 2.177264 1.0179793 0.09043523 0.7344729
## 2    3 2.592350 0.8515155 2.112745 1.0215747 0.09291606 0.7504804
## 3    4 2.546599 0.8536404 2.063396 0.9918813 0.08917800 0.7268040
## 4    5 2.522124 0.8549801 2.040184 0.9875859 0.09233036 0.7462059
## 5    6 2.560619 0.8462381 2.078882 0.9751577 0.09803020 0.7460211
## 6    7 2.546294 0.8443172 2.065910 0.9712595 0.09897416 0.7483275
## 7    8 2.584603 0.8390093 2.097366 0.9849969 0.10136125 0.7640514
## 8    9 2.592787 0.8366038 2.109758 0.9985790 0.10237274 0.7735958
## 9   10 2.610255 0.8337551 2.118641 1.0035417 0.10468172 0.7828991</code></pre>
<div class="sourceCode" id="cb394"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb394-1" data-line-number="1">Metrics<span class="op">::</span><span class="kw">rmse</span>(new_test<span class="op">$</span>mpg, <span class="kw">predict</span>(caret_rf,new_test))</a></code></pre></div>
<pre><code>## [1] 2.071873</code></pre>
<p>It looks like a value of 4 will produce the lowest value for RMSE.</p>
<div class="sourceCode" id="cb396"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb396-1" data-line-number="1"><span class="kw">plot</span>(caret_rf)</a></code></pre></div>
<p><img src="biosml_files/figure-html/plotcaretrf-1.png" width="672" /></p>
</div>
<div id="hyperparameter-tuning" class="section level2">
<h2><span class="header-section-number">9.2</span> Hyperparameter Tuning</h2>
<p>The process of finding the “right” values for these parameters is generally referred to as “hypermarket tuning”. Different values are supplied for each invocation of a method (as we did in the above example) to see the effect on the model. We might do this many times to arrive at the optimal parameter set to produce a model that offers the “best” explanatory and predictive power.</p>
<p>Just to review - things like coefficients and residuals are parameters that are generated by a call to the <strong>lm</strong> function. They don’t actually exist until the function does some work. The <strong>hyperparameters</strong> are specific to whatever algorithm (and supporting R function) you are using. Concepts such as coefficients and intercept, however, are parameters that would be generated in this case by lm.</p>
<p>More generally, what if we wanted to use other functions to do some predicting ? You can check this page for a list of caret supported methods along with any hyperparamters available for tuning. Obviously, if you know the underlying method you can refer to the help page for it to see what paramters exist.</p>
<div id="multiple-hyperparameters" class="section level3">
<h3><span class="header-section-number">9.2.1</span> Multiple Hyperparameters ?</h3>
<p>Let’s look at another method for random forests such as the <strong>ranger</strong> function which allages to build trees very rapidly. We can easily call it via the <strong>train</strong> function. If you consult the reference manual for the caret implementation of <strong>ranger</strong> you will see that it supports three hyperparameters: <strong>mtry, splitrule</strong>, and <strong>min.node.size</strong>.</p>
<div class="sourceCode" id="cb397"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb397-1" data-line-number="1">caret_ranger &lt;-<span class="st"> </span><span class="kw">train</span>(mpg <span class="op">~</span><span class="st"> </span>.,</a>
<a class="sourceLine" id="cb397-2" data-line-number="2">                      <span class="dt">data =</span> new_train,</a>
<a class="sourceLine" id="cb397-3" data-line-number="3">                      <span class="dt">method =</span> <span class="st">&quot;ranger&quot;</span>)</a>
<a class="sourceLine" id="cb397-4" data-line-number="4"></a>
<a class="sourceLine" id="cb397-5" data-line-number="5">caret_ranger<span class="op">$</span>results</a></code></pre></div>
<pre><code>##   mtry min.node.size  splitrule     RMSE  Rsquared      MAE    RMSESD
## 1    2             5   variance 2.593815 0.8466324 2.066727 0.6209003
## 2    2             5 extratrees 2.766129 0.8264357 2.242318 0.6277065
## 3    6             5   variance 2.483004 0.8656660 1.919371 0.6014174
## 4    6             5 extratrees 2.654512 0.8372522 2.111165 0.6095019
## 5   10             5   variance 2.578429 0.8524675 1.990380 0.6556106
## 6   10             5 extratrees 2.645518 0.8399955 2.080374 0.6240255
##   RsquaredSD     MAESD
## 1 0.04837170 0.4988512
## 2 0.05777494 0.5171023
## 3 0.04497003 0.4986984
## 4 0.05395181 0.5167884
## 5 0.05489347 0.5321517
## 6 0.05368885 0.5299911</code></pre>
<div class="sourceCode" id="cb399"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb399-1" data-line-number="1">Metrics<span class="op">::</span><span class="kw">rmse</span>(new_test<span class="op">$</span>mpg, <span class="kw">predict</span>(caret_ranger,new_test))</a></code></pre></div>
<pre><code>## [1] 2.22648</code></pre>
<p>And then we can plot these results to get a more intutive view of the output.</p>
<div class="sourceCode" id="cb401"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb401-1" data-line-number="1"><span class="kw">plot</span>(caret_ranger)</a></code></pre></div>
<p><img src="biosml_files/figure-html/plotrangerinvoke-1.png" width="672" /></p>
<div class="sourceCode" id="cb402"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb402-1" data-line-number="1">caret_ranger &lt;-<span class="st"> </span><span class="kw">train</span>(mpg <span class="op">~</span><span class="st"> </span>.,</a>
<a class="sourceLine" id="cb402-2" data-line-number="2">                      <span class="dt">data =</span> new_train,</a>
<a class="sourceLine" id="cb402-3" data-line-number="3">                      <span class="dt">method =</span> <span class="st">&quot;ranger&quot;</span>,</a>
<a class="sourceLine" id="cb402-4" data-line-number="4">                      <span class="dt">tuneLength =</span> <span class="dv">10</span>)</a></code></pre></div>
<pre><code>## note: only 9 unique complexity parameters in default grid. Truncating the grid to 9 .</code></pre>
<div class="sourceCode" id="cb404"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb404-1" data-line-number="1">caret_ranger<span class="op">$</span>results</a></code></pre></div>
<pre><code>##    mtry min.node.size  splitrule     RMSE  Rsquared      MAE    RMSESD
## 1     2             5   variance 2.535142 0.8705777 2.050717 0.7213109
## 2     2             5 extratrees 2.733620 0.8449910 2.229738 0.7697078
## 3     3             5   variance 2.459643 0.8792912 1.974891 0.7102015
## 4     3             5 extratrees 2.697460 0.8492856 2.205555 0.7174634
## 5     4             5   variance 2.413853 0.8811367 1.928837 0.6799630
## 6     4             5 extratrees 2.664466 0.8534908 2.171931 0.7522685
## 7     5             5   variance 2.396935 0.8826333 1.902248 0.6642884
## 8     5             5 extratrees 2.646204 0.8562650 2.158095 0.6929833
## 9     6             5   variance 2.403365 0.8830201 1.898374 0.6594777
## 10    6             5 extratrees 2.614236 0.8584534 2.132242 0.7040789
## 11    7             5   variance 2.414677 0.8798765 1.904939 0.6577313
## 12    7             5 extratrees 2.632442 0.8594405 2.140217 0.7004326
## 13    8             5   variance 2.430565 0.8782065 1.923389 0.6579862
## 14    8             5 extratrees 2.618759 0.8590369 2.135648 0.6871433
## 15    9             5   variance 2.449120 0.8742522 1.934580 0.6459582
## 16    9             5 extratrees 2.604208 0.8594866 2.117187 0.6775269
## 17   10             5   variance 2.487449 0.8686236 1.969850 0.6753278
## 18   10             5 extratrees 2.604154 0.8607695 2.122085 0.6736732
##    RsquaredSD     MAESD
## 1  0.08406100 0.5480455
## 2  0.09607066 0.6150415
## 3  0.08193415 0.5414353
## 4  0.09468130 0.5737425
## 5  0.08712652 0.5282255
## 6  0.09407159 0.6117592
## 7  0.08740512 0.5140223
## 8  0.09171305 0.5710811
## 9  0.08517967 0.5101976
## 10 0.09267746 0.5748181
## 11 0.08974818 0.5246354
## 12 0.09158392 0.5756883
## 13 0.09169315 0.5161000
## 14 0.09196265 0.5715926
## 15 0.08864581 0.5073912
## 16 0.09265309 0.5709619
## 17 0.09618672 0.5192270
## 18 0.08758355 0.5584563</code></pre>
<div class="sourceCode" id="cb406"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb406-1" data-line-number="1">Metrics<span class="op">::</span><span class="kw">rmse</span>(new_test<span class="op">$</span>mpg, <span class="kw">predict</span>(caret_ranger,new_test))</a></code></pre></div>
<pre><code>## [1] 2.085479</code></pre>
<div class="sourceCode" id="cb408"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb408-1" data-line-number="1"><span class="kw">plot</span>(caret_ranger)</a></code></pre></div>
<p><img src="biosml_files/figure-html/plotnewranger-1.png" width="672" /></p>
</div>
<div id="custom-tuning-grid" class="section level3">
<h3><span class="header-section-number">9.2.2</span> Custom Tuning Grid</h3>
<p>So it’s possible to go even deeper when tuning hyperparameters. We can create a custom tuning grid instead of letting caret pick the values. Of course, using <strong>tuneLength</strong> is fine but the custom tuning grid allows for finer grained control. It’s easy, all we need to do is to specify our own “tuneGrid” and then pass it to <strong>train</strong>.</p>
<div class="sourceCode" id="cb409"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb409-1" data-line-number="1">MyGrid &lt;-<span class="st"> </span><span class="kw">data.frame</span>(</a>
<a class="sourceLine" id="cb409-2" data-line-number="2">   <span class="dt">mtry =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">6</span>),</a>
<a class="sourceLine" id="cb409-3" data-line-number="3">   <span class="dt">splitrule =</span> <span class="st">&quot;variance&quot;</span>,</a>
<a class="sourceLine" id="cb409-4" data-line-number="4">   <span class="dt">min.node.size =</span> <span class="dv">4</span></a>
<a class="sourceLine" id="cb409-5" data-line-number="5">)</a>
<a class="sourceLine" id="cb409-6" data-line-number="6"></a>
<a class="sourceLine" id="cb409-7" data-line-number="7"><span class="co"># Now we supply it when calling train</span></a>
<a class="sourceLine" id="cb409-8" data-line-number="8"></a>
<a class="sourceLine" id="cb409-9" data-line-number="9">caret_ranger &lt;-<span class="st"> </span><span class="kw">train</span>(mpg <span class="op">~</span><span class="st"> </span>.,</a>
<a class="sourceLine" id="cb409-10" data-line-number="10">                  <span class="dt">data =</span> new_train,</a>
<a class="sourceLine" id="cb409-11" data-line-number="11">                  <span class="dt">method =</span> <span class="st">&quot;ranger&quot;</span>,</a>
<a class="sourceLine" id="cb409-12" data-line-number="12">                  <span class="dt">tuneGrid =</span> MyGrid)</a>
<a class="sourceLine" id="cb409-13" data-line-number="13"></a>
<a class="sourceLine" id="cb409-14" data-line-number="14">caret_ranger<span class="op">$</span>results</a></code></pre></div>
<pre><code>##   mtry splitrule min.node.size     RMSE  Rsquared      MAE    RMSESD
## 1    2  variance             4 2.644805 0.8612962 2.187830 0.5518203
## 2    4  variance             4 2.551675 0.8629956 2.056296 0.5058089
## 3    6  variance             4 2.554660 0.8598010 2.049961 0.5235632
##   RsquaredSD     MAESD
## 1 0.06821914 0.4520530
## 2 0.07012752 0.4366614
## 3 0.07389630 0.4463890</code></pre>
<div class="sourceCode" id="cb411"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb411-1" data-line-number="1">Metrics<span class="op">::</span><span class="kw">rmse</span>(new_test<span class="op">$</span>mpg, <span class="kw">predict</span>(caret_ranger,new_test))</a></code></pre></div>
<pre><code>## [1] 1.964188</code></pre>
</div>
</div>
<div id="using-validation-data-sets" class="section level2">
<h2><span class="header-section-number">9.3</span> Using Validation Data Sets</h2>
<p>When building a model, we generate a training and test data set. We use the former to build a model and, if we are using something like the caret package, that process involves cross fold validation or bootstrap sampling to generate a good estimate for out-of-sample error. We then apply the model to the test data frame.</p>
<p>If we are using a method that has hyperparamters then maybe we want an intermediate data set to help validate our ultimate choice of hyperparameters. By taking this approach we can still keep our test data set off to the side for later use with the trained and validated model. Using this idea doesn’t require us to do much beyond generating a third data set.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="decision-trees.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="picking-the-best-model.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
