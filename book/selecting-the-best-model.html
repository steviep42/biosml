<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 10 Selecting The Best Model | Predictive Learning in R</title>
  <meta name="description" content="Chapter 10 Selecting The Best Model | Predictive Learning in R" />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 10 Selecting The Best Model | Predictive Learning in R" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 10 Selecting The Best Model | Predictive Learning in R" />
  
  
  

<meta name="author" content="Steve Pittard - wsp@emory.edu" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="using-methods-other-than-lm.html"/>
<link rel="next" href="data-pre-processing.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#machine-learning"><i class="fa fa-check"></i><b>1.1</b> Machine Learning</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#predictive-modeling"><i class="fa fa-check"></i><b>1.2</b> Predictive Modeling</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#in-sample-vs-out-of-sample-error"><i class="fa fa-check"></i><b>1.3</b> In-Sample vs Out-Of-Sample Error</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#performance-metrics"><i class="fa fa-check"></i><b>1.4</b> Performance Metrics</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#black-box"><i class="fa fa-check"></i><b>1.5</b> Black Box</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html"><i class="fa fa-check"></i><b>2</b> Predictive / Supervised Learning</a><ul>
<li class="chapter" data-level="2.1" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#explanation-vs-prediction"><i class="fa fa-check"></i><b>2.1</b> Explanation vs Prediction</a><ul>
<li class="chapter" data-level="2.1.1" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#titanic-data"><i class="fa fa-check"></i><b>2.1.1</b> Titanic Data</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#bias-vs-variance"><i class="fa fa-check"></i><b>2.2</b> Bias vs Variance</a><ul>
<li class="chapter" data-level="2.2.1" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#bias"><i class="fa fa-check"></i><b>2.2.1</b> Bias</a></li>
<li class="chapter" data-level="2.2.2" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#variance"><i class="fa fa-check"></i><b>2.2.2</b> Variance</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#overfitting-and-underfitting"><i class="fa fa-check"></i><b>2.3</b> Overfitting and Underfitting</a></li>
<li class="chapter" data-level="2.4" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#some-important-terminology"><i class="fa fa-check"></i><b>2.4</b> Some Important Terminology</a></li>
<li class="chapter" data-level="2.5" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#levels-of-measurement"><i class="fa fa-check"></i><b>2.5</b> Levels of Measurement</a><ul>
<li class="chapter" data-level="2.5.1" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#nominal"><i class="fa fa-check"></i><b>2.5.1</b> Nominal</a></li>
<li class="chapter" data-level="2.5.2" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#ordinal"><i class="fa fa-check"></i><b>2.5.2</b> Ordinal</a></li>
<li class="chapter" data-level="2.5.3" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#interval"><i class="fa fa-check"></i><b>2.5.3</b> Interval</a></li>
<li class="chapter" data-level="2.5.4" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#ratio"><i class="fa fa-check"></i><b>2.5.4</b> Ratio</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="a-motivating-example.html"><a href="a-motivating-example.html"><i class="fa fa-check"></i><b>3</b> A Motivating Example</a><ul>
<li class="chapter" data-level="3.1" data-path="a-motivating-example.html"><a href="a-motivating-example.html#a-more-detailed-workflow"><i class="fa fa-check"></i><b>3.1</b> A More Detailed Workflow</a></li>
<li class="chapter" data-level="3.2" data-path="a-motivating-example.html"><a href="a-motivating-example.html#visualizations"><i class="fa fa-check"></i><b>3.2</b> Visualizations</a><ul>
<li class="chapter" data-level="3.2.1" data-path="a-motivating-example.html"><a href="a-motivating-example.html#scatterplots"><i class="fa fa-check"></i><b>3.2.1</b> Scatterplots</a></li>
<li class="chapter" data-level="3.2.2" data-path="a-motivating-example.html"><a href="a-motivating-example.html#boxplots"><i class="fa fa-check"></i><b>3.2.2</b> Boxplots</a></li>
<li class="chapter" data-level="3.2.3" data-path="a-motivating-example.html"><a href="a-motivating-example.html#histograms"><i class="fa fa-check"></i><b>3.2.3</b> Histograms</a></li>
<li class="chapter" data-level="3.2.4" data-path="a-motivating-example.html"><a href="a-motivating-example.html#tables"><i class="fa fa-check"></i><b>3.2.4</b> Tables</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="a-motivating-example.html"><a href="a-motivating-example.html#correlations"><i class="fa fa-check"></i><b>3.3</b> Correlations</a></li>
<li class="chapter" data-level="3.4" data-path="a-motivating-example.html"><a href="a-motivating-example.html#building-a-model---in-sample-error"><i class="fa fa-check"></i><b>3.4</b> Building A Model - In Sample Error</a></li>
<li class="chapter" data-level="3.5" data-path="a-motivating-example.html"><a href="a-motivating-example.html#out-of-sample-data"><i class="fa fa-check"></i><b>3.5</b> Out Of Sample Data</a></li>
<li class="chapter" data-level="3.6" data-path="a-motivating-example.html"><a href="a-motivating-example.html#some-additional-considerations"><i class="fa fa-check"></i><b>3.6</b> Some Additional Considerations</a></li>
<li class="chapter" data-level="3.7" data-path="a-motivating-example.html"><a href="a-motivating-example.html#other-methods"><i class="fa fa-check"></i><b>3.7</b> Other Methods ?</a></li>
<li class="chapter" data-level="3.8" data-path="a-motivating-example.html"><a href="a-motivating-example.html#summary"><i class="fa fa-check"></i><b>3.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="training-test-data.html"><a href="training-test-data.html"><i class="fa fa-check"></i><b>4</b> Training / Test Data</a><ul>
<li class="chapter" data-level="4.1" data-path="training-test-data.html"><a href="training-test-data.html#cross-fold-validation"><i class="fa fa-check"></i><b>4.1</b> Cross Fold Validation</a></li>
<li class="chapter" data-level="4.2" data-path="training-test-data.html"><a href="training-test-data.html#create-a-function-to-automate-things"><i class="fa fa-check"></i><b>4.2</b> Create A Function To Automate Things</a></li>
<li class="chapter" data-level="4.3" data-path="training-test-data.html"><a href="training-test-data.html#repeated-cross-validation"><i class="fa fa-check"></i><b>4.3</b> Repeated Cross Validation</a></li>
<li class="chapter" data-level="4.4" data-path="training-test-data.html"><a href="training-test-data.html#bootstrap"><i class="fa fa-check"></i><b>4.4</b> Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="caret-package.html"><a href="caret-package.html"><i class="fa fa-check"></i><b>5</b> Caret Package</a><ul>
<li class="chapter" data-level="5.1" data-path="caret-package.html"><a href="caret-package.html#putting-caret-to-work"><i class="fa fa-check"></i><b>5.1</b> Putting caret To Work</a></li>
<li class="chapter" data-level="5.2" data-path="caret-package.html"><a href="caret-package.html#back-to-the-beginning"><i class="fa fa-check"></i><b>5.2</b> Back To The Beginning</a></li>
<li class="chapter" data-level="5.3" data-path="caret-package.html"><a href="caret-package.html#splitting"><i class="fa fa-check"></i><b>5.3</b> Splitting</a></li>
<li class="chapter" data-level="5.4" data-path="caret-package.html"><a href="caret-package.html#calling-the-train-function"><i class="fa fa-check"></i><b>5.4</b> Calling The train() Function</a></li>
<li class="chapter" data-level="5.5" data-path="caret-package.html"><a href="caret-package.html#reproducible-results"><i class="fa fa-check"></i><b>5.5</b> Reproducible Results</a></li>
<li class="chapter" data-level="5.6" data-path="caret-package.html"><a href="caret-package.html#one-size-fits-all"><i class="fa fa-check"></i><b>5.6</b> One Size Fits All</a></li>
<li class="chapter" data-level="5.7" data-path="caret-package.html"><a href="caret-package.html#alternative-calling-sequence"><i class="fa fa-check"></i><b>5.7</b> Alternative Calling Sequence</a></li>
<li class="chapter" data-level="5.8" data-path="caret-package.html"><a href="caret-package.html#hyperparameters"><i class="fa fa-check"></i><b>5.8</b> Hyperparameters</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="classification-problems.html"><a href="classification-problems.html"><i class="fa fa-check"></i><b>6</b> Classification Problems</a><ul>
<li class="chapter" data-level="6.1" data-path="classification-problems.html"><a href="classification-problems.html#performance-measures"><i class="fa fa-check"></i><b>6.1</b> Performance Measures</a></li>
<li class="chapter" data-level="6.2" data-path="classification-problems.html"><a href="classification-problems.html#important-terminology"><i class="fa fa-check"></i><b>6.2</b> Important Terminology</a></li>
<li class="chapter" data-level="6.3" data-path="classification-problems.html"><a href="classification-problems.html#a-basic-model"><i class="fa fa-check"></i><b>6.3</b> A Basic Model</a></li>
<li class="chapter" data-level="6.4" data-path="classification-problems.html"><a href="classification-problems.html#selecting-the-correct-threshold-alpha"><i class="fa fa-check"></i><b>6.4</b> Selecting The Correct Threshold / Alpha</a><ul>
<li class="chapter" data-level="6.4.1" data-path="classification-problems.html"><a href="classification-problems.html#moving-the-threshold"><i class="fa fa-check"></i><b>6.4.1</b> Moving The Threshold</a></li>
<li class="chapter" data-level="6.4.2" data-path="classification-problems.html"><a href="classification-problems.html#distribution-of-predicted-probabilities"><i class="fa fa-check"></i><b>6.4.2</b> Distribution of Predicted Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="classification-problems.html"><a href="classification-problems.html#hypothesis-testing"><i class="fa fa-check"></i><b>6.5</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="6.6" data-path="classification-problems.html"><a href="classification-problems.html#confusion-matrix"><i class="fa fa-check"></i><b>6.6</b> Confusion Matrix</a><ul>
<li class="chapter" data-level="6.6.1" data-path="classification-problems.html"><a href="classification-problems.html#computing-performance-metrics"><i class="fa fa-check"></i><b>6.6.1</b> Computing Performance Metrics</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="classification-problems.html"><a href="classification-problems.html#picking-the-right-metric"><i class="fa fa-check"></i><b>6.7</b> Picking the Right Metric</a></li>
<li class="chapter" data-level="6.8" data-path="classification-problems.html"><a href="classification-problems.html#wait.-where-are-we"><i class="fa fa-check"></i><b>6.8</b> Wait. Where Are We ?</a></li>
<li class="chapter" data-level="6.9" data-path="classification-problems.html"><a href="classification-problems.html#other-ways-to-compute-the-roc-curve"><i class="fa fa-check"></i><b>6.9</b> Other Ways To Compute The ROC Curve</a></li>
<li class="chapter" data-level="6.10" data-path="classification-problems.html"><a href="classification-problems.html#roc-curve-summary"><i class="fa fa-check"></i><b>6.10</b> ROC Curve Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="classification-example.html"><a href="classification-example.html"><i class="fa fa-check"></i><b>7</b> Classification Example</a><ul>
<li class="chapter" data-level="7.1" data-path="classification-example.html"><a href="classification-example.html#exploratory-plots"><i class="fa fa-check"></i><b>7.1</b> Exploratory Plots</a></li>
<li class="chapter" data-level="7.2" data-path="classification-example.html"><a href="classification-example.html#generalized-linear-models"><i class="fa fa-check"></i><b>7.2</b> Generalized Linear Models</a></li>
<li class="chapter" data-level="7.3" data-path="classification-example.html"><a href="classification-example.html#random-forests"><i class="fa fa-check"></i><b>7.3</b> Random Forests</a></li>
<li class="chapter" data-level="7.4" data-path="classification-example.html"><a href="classification-example.html#target-variable-format"><i class="fa fa-check"></i><b>7.4</b> Target Variable Format</a></li>
<li class="chapter" data-level="7.5" data-path="classification-example.html"><a href="classification-example.html#addressing-class-imbalance"><i class="fa fa-check"></i><b>7.5</b> Addressing Class Imbalance</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>8</b> Decision Trees</a><ul>
<li class="chapter" data-level="8.1" data-path="decision-trees.html"><a href="decision-trees.html#advantages"><i class="fa fa-check"></i><b>8.1</b> Advantages</a></li>
<li class="chapter" data-level="8.2" data-path="decision-trees.html"><a href="decision-trees.html#a-classification-example"><i class="fa fa-check"></i><b>8.2</b> A Classification Example</a></li>
<li class="chapter" data-level="8.3" data-path="decision-trees.html"><a href="decision-trees.html#digging-deeper"><i class="fa fa-check"></i><b>8.3</b> Digging Deeper</a><ul>
<li class="chapter" data-level="8.3.1" data-path="decision-trees.html"><a href="decision-trees.html#evaluating-performance"><i class="fa fa-check"></i><b>8.3.1</b> Evaluating performance</a></li>
<li class="chapter" data-level="8.3.2" data-path="decision-trees.html"><a href="decision-trees.html#tree-splitting"><i class="fa fa-check"></i><b>8.3.2</b> Tree Splitting</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="decision-trees.html"><a href="decision-trees.html#gini-index"><i class="fa fa-check"></i><b>8.4</b> Gini Index</a></li>
<li class="chapter" data-level="8.5" data-path="decision-trees.html"><a href="decision-trees.html#regression-trees"><i class="fa fa-check"></i><b>8.5</b> Regression Trees</a><ul>
<li class="chapter" data-level="8.5.1" data-path="decision-trees.html"><a href="decision-trees.html#performance-measure"><i class="fa fa-check"></i><b>8.5.1</b> Performance Measure</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="decision-trees.html"><a href="decision-trees.html#parameters-vs-hyperparameters"><i class="fa fa-check"></i><b>8.6</b> Parameters vs Hyperparameters</a></li>
<li class="chapter" data-level="8.7" data-path="decision-trees.html"><a href="decision-trees.html#grid-searching"><i class="fa fa-check"></i><b>8.7</b> Grid Searching</a></li>
<li class="chapter" data-level="8.8" data-path="decision-trees.html"><a href="decision-trees.html#bagged-trees"><i class="fa fa-check"></i><b>8.8</b> Bagged Trees</a></li>
<li class="chapter" data-level="8.9" data-path="decision-trees.html"><a href="decision-trees.html#random-forests-1"><i class="fa fa-check"></i><b>8.9</b> Random Forests</a></li>
<li class="chapter" data-level="8.10" data-path="decision-trees.html"><a href="decision-trees.html#boosted-trees"><i class="fa fa-check"></i><b>8.10</b> Boosted Trees</a></li>
<li class="chapter" data-level="8.11" data-path="decision-trees.html"><a href="decision-trees.html#using-caret"><i class="fa fa-check"></i><b>8.11</b> Using caret</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="using-methods-other-than-lm.html"><a href="using-methods-other-than-lm.html"><i class="fa fa-check"></i><b>9</b> Using Methods Other Than lm</a><ul>
<li class="chapter" data-level="9.1" data-path="using-methods-other-than-lm.html"><a href="using-methods-other-than-lm.html#parameters-vs-hyperparameters-1"><i class="fa fa-check"></i><b>9.1</b> Parameters vs Hyperparameters</a></li>
<li class="chapter" data-level="9.2" data-path="using-methods-other-than-lm.html"><a href="using-methods-other-than-lm.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>9.2</b> Hyperparameter Tuning</a><ul>
<li class="chapter" data-level="9.2.1" data-path="using-methods-other-than-lm.html"><a href="using-methods-other-than-lm.html#multiple-hyperparameters"><i class="fa fa-check"></i><b>9.2.1</b> Multiple Hyperparameters ?</a></li>
<li class="chapter" data-level="9.2.2" data-path="using-methods-other-than-lm.html"><a href="using-methods-other-than-lm.html#custom-tuning-grid"><i class="fa fa-check"></i><b>9.2.2</b> Custom Tuning Grid</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="using-methods-other-than-lm.html"><a href="using-methods-other-than-lm.html#using-validation-data-sets"><i class="fa fa-check"></i><b>9.3</b> Using Validation Data Sets</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="selecting-the-best-model.html"><a href="selecting-the-best-model.html"><i class="fa fa-check"></i><b>10</b> Selecting The Best Model</a><ul>
<li class="chapter" data-level="10.1" data-path="selecting-the-best-model.html"><a href="selecting-the-best-model.html#an-example"><i class="fa fa-check"></i><b>10.1</b> An Example</a></li>
<li class="chapter" data-level="10.2" data-path="selecting-the-best-model.html"><a href="selecting-the-best-model.html#more-comparisons"><i class="fa fa-check"></i><b>10.2</b> More Comparisons</a></li>
<li class="chapter" data-level="10.3" data-path="selecting-the-best-model.html"><a href="selecting-the-best-model.html#using-the-resamples-function"><i class="fa fa-check"></i><b>10.3</b> Using the resamples() function</a></li>
<li class="chapter" data-level="10.4" data-path="selecting-the-best-model.html"><a href="selecting-the-best-model.html#model-performance"><i class="fa fa-check"></i><b>10.4</b> Model Performance</a></li>
<li class="chapter" data-level="10.5" data-path="selecting-the-best-model.html"><a href="selecting-the-best-model.html#feature-evaluation"><i class="fa fa-check"></i><b>10.5</b> Feature Evaluation</a><ul>
<li class="chapter" data-level="10.5.1" data-path="selecting-the-best-model.html"><a href="selecting-the-best-model.html#identifying-redundant-features"><i class="fa fa-check"></i><b>10.5.1</b> Identifying Redundant Features</a></li>
<li class="chapter" data-level="10.5.2" data-path="selecting-the-best-model.html"><a href="selecting-the-best-model.html#highly-correlated-variables"><i class="fa fa-check"></i><b>10.5.2</b> Highly Correlated Variables</a></li>
<li class="chapter" data-level="10.5.3" data-path="selecting-the-best-model.html"><a href="selecting-the-best-model.html#ranking-features"><i class="fa fa-check"></i><b>10.5.3</b> Ranking Features</a></li>
<li class="chapter" data-level="10.5.4" data-path="selecting-the-best-model.html"><a href="selecting-the-best-model.html#feature-selection"><i class="fa fa-check"></i><b>10.5.4</b> Feature Selection</a></li>
<li class="chapter" data-level="10.5.5" data-path="selecting-the-best-model.html"><a href="selecting-the-best-model.html#recursive-feature-elimination"><i class="fa fa-check"></i><b>10.5.5</b> Recursive Feature Elimination</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="data-pre-processing.html"><a href="data-pre-processing.html"><i class="fa fa-check"></i><b>11</b> Data Pre Processing</a><ul>
<li class="chapter" data-level="11.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#types-of-pre-processing"><i class="fa fa-check"></i><b>11.1</b> Types of Pre Processing</a></li>
<li class="chapter" data-level="11.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#missing-values"><i class="fa fa-check"></i><b>11.2</b> Missing Values</a><ul>
<li class="chapter" data-level="11.2.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#finding-rows-with-missing-data"><i class="fa fa-check"></i><b>11.2.1</b> Finding Rows with Missing Data</a></li>
<li class="chapter" data-level="11.2.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#finding-columns-with-missing-data"><i class="fa fa-check"></i><b>11.2.2</b> Finding Columns With Missing Data</a></li>
<li class="chapter" data-level="11.2.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#use-the-median-approach"><i class="fa fa-check"></i><b>11.2.3</b> Use the Median Approach</a></li>
<li class="chapter" data-level="11.2.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#package-based-approach"><i class="fa fa-check"></i><b>11.2.4</b> Package-based Approach</a></li>
<li class="chapter" data-level="11.2.5" data-path="data-pre-processing.html"><a href="data-pre-processing.html#using-caret-1"><i class="fa fa-check"></i><b>11.2.5</b> Using caret</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#scaling"><i class="fa fa-check"></i><b>11.3</b> Scaling</a><ul>
<li class="chapter" data-level="11.3.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#methods-that-benefit-from-scaling"><i class="fa fa-check"></i><b>11.3.1</b> Methods That Benefit From Scaling</a></li>
<li class="chapter" data-level="11.3.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#methods-that-do-not-require-scaling"><i class="fa fa-check"></i><b>11.3.2</b> Methods That Do Not Require Scaling</a></li>
<li class="chapter" data-level="11.3.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#how-to-scale"><i class="fa fa-check"></i><b>11.3.3</b> How To Scale</a></li>
<li class="chapter" data-level="11.3.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#order-of-processing"><i class="fa fa-check"></i><b>11.3.4</b> Order of Processing</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#low-variance-variables"><i class="fa fa-check"></i><b>11.4</b> Low Variance Variables</a></li>
<li class="chapter" data-level="11.5" data-path="data-pre-processing.html"><a href="data-pre-processing.html#pca---principal-components-analysis"><i class="fa fa-check"></i><b>11.5</b> PCA - Principal Components Analysis</a><ul>
<li class="chapter" data-level="11.5.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#identify-the-factors"><i class="fa fa-check"></i><b>11.5.1</b> Identify The Factors</a></li>
<li class="chapter" data-level="11.5.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#check-for-high-correlations"><i class="fa fa-check"></i><b>11.5.2</b> Check For High Correlations</a></li>
<li class="chapter" data-level="11.5.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#so-why-use-pca"><i class="fa fa-check"></i><b>11.5.3</b> So Why Use PCA ?</a></li>
<li class="chapter" data-level="11.5.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#check-the-biplot"><i class="fa fa-check"></i><b>11.5.4</b> Check The BiPlot</a></li>
<li class="chapter" data-level="11.5.5" data-path="data-pre-processing.html"><a href="data-pre-processing.html#check-the-screeplot"><i class="fa fa-check"></i><b>11.5.5</b> Check The ScreePlot</a></li>
<li class="chapter" data-level="11.5.6" data-path="data-pre-processing.html"><a href="data-pre-processing.html#use-the-transformed-data"><i class="fa fa-check"></i><b>11.5.6</b> Use The Transformed Data</a></li>
<li class="chapter" data-level="11.5.7" data-path="data-pre-processing.html"><a href="data-pre-processing.html#pls"><i class="fa fa-check"></i><b>11.5.7</b> PLS</a></li>
<li class="chapter" data-level="11.5.8" data-path="data-pre-processing.html"><a href="data-pre-processing.html#summary-1"><i class="fa fa-check"></i><b>11.5.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="data-pre-processing.html"><a href="data-pre-processing.html#order-of-pre-processing"><i class="fa fa-check"></i><b>11.6</b> Order of Pre-Processing</a></li>
<li class="chapter" data-level="11.7" data-path="data-pre-processing.html"><a href="data-pre-processing.html#handling-categories"><i class="fa fa-check"></i><b>11.7</b> Handling Categories</a><ul>
<li class="chapter" data-level="11.7.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#examples"><i class="fa fa-check"></i><b>11.7.1</b> Examples</a></li>
<li class="chapter" data-level="11.7.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#admissions-data"><i class="fa fa-check"></i><b>11.7.2</b> Admissions Data</a></li>
<li class="chapter" data-level="11.7.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#is-rank-a-category"><i class="fa fa-check"></i><b>11.7.3</b> Is Rank A Category ?</a></li>
<li class="chapter" data-level="11.7.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#relationship-to-one-hot-encoding"><i class="fa fa-check"></i><b>11.7.4</b> Relationship To One Hot Encoding</a></li>
</ul></li>
<li class="chapter" data-level="11.8" data-path="data-pre-processing.html"><a href="data-pre-processing.html#binning"><i class="fa fa-check"></i><b>11.8</b> Binning</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html"><i class="fa fa-check"></i><b>12</b> Using External ML Frameworks</a><ul>
<li class="chapter" data-level="12.1" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#using-h2o"><i class="fa fa-check"></i><b>12.1</b> Using h2o</a></li>
<li class="chapter" data-level="12.2" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#create-some-h20-models"><i class="fa fa-check"></i><b>12.2</b> Create Some h20 Models</a></li>
<li class="chapter" data-level="12.3" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#saving-a-model"><i class="fa fa-check"></i><b>12.3</b> Saving A Model</a></li>
<li class="chapter" data-level="12.4" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#using-the-h2o-auto-ml-feature"><i class="fa fa-check"></i><b>12.4</b> Using The h2o Auto ML Feature</a></li>
<li class="chapter" data-level="12.5" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#launching-a-job"><i class="fa fa-check"></i><b>12.5</b> Launching a Job</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Predictive Learning in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="selecting-the-best-model" class="section level1">
<h1><span class="header-section-number">Chapter 10</span> Selecting The Best Model</h1>
<p>It’s always of interest to compare the results of one model to another (or even more) to determine the “best” model to share with some else. On the other hand, it’s easy to get carried away with trying out different models in an attempt to make performance improvements especially when the might only be marginally better.</p>
<p>It’s also wise to pick methods that you know how to reasonably defend over those that you can’t. For example, picking a Neural Net model might result in better accuracy although if you are challenged on the results in some way, would you be able to address all concerns ? If a logistic regression model resulted in a comparable result then you might should stick with that result since it’s a well-known method that few would question.</p>
<div id="an-example" class="section level2">
<h2><span class="header-section-number">10.1</span> An Example</h2>
<p>As an example, let’s go back to the linear modeling example back in the section wherein we introduced the <strong>caret</strong> package.</p>
<div class="sourceCode" id="cb432"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb432-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">123</span>) <span class="co"># Make this example reproducible</span></a>
<a class="sourceLine" id="cb432-2" data-line-number="2">idx &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(mtcars<span class="op">$</span>mpg, <span class="dt">p =</span> <span class="fl">.8</span>, </a>
<a class="sourceLine" id="cb432-3" data-line-number="3">                           <span class="dt">list =</span> <span class="ot">FALSE</span>, </a>
<a class="sourceLine" id="cb432-4" data-line-number="4">                           <span class="dt">times =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb432-5" data-line-number="5"><span class="kw">head</span>(idx)</a></code></pre></div>
<pre><code>##      Resample1
## [1,]         1
## [2,]         3
## [3,]         4
## [4,]         5
## [5,]         6
## [6,]         8</code></pre>
<div class="sourceCode" id="cb434"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb434-1" data-line-number="1">example_Train &lt;-<span class="st"> </span>mtcars[ idx,]</a>
<a class="sourceLine" id="cb434-2" data-line-number="2">example_Test  &lt;-<span class="st"> </span>mtcars[<span class="op">-</span>idx,]</a>
<a class="sourceLine" id="cb434-3" data-line-number="3"></a>
<a class="sourceLine" id="cb434-4" data-line-number="4"><span class="co">#</span></a>
<a class="sourceLine" id="cb434-5" data-line-number="5">control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>,       <span class="co"># Cross Fold</span></a>
<a class="sourceLine" id="cb434-6" data-line-number="6">                        <span class="dt">number =</span> <span class="dv">5</span>,          <span class="co"># 5 Folds</span></a>
<a class="sourceLine" id="cb434-7" data-line-number="7">                        <span class="dt">verboseIter =</span> <span class="ot">FALSE</span>)  <span class="co"># Verbose</span></a>
<a class="sourceLine" id="cb434-8" data-line-number="8"><span class="co"># Train the model</span></a>
<a class="sourceLine" id="cb434-9" data-line-number="9"></a>
<a class="sourceLine" id="cb434-10" data-line-number="10"><span class="kw">set.seed</span>(<span class="dv">123</span>) <span class="co"># Make this example reproducible</span></a>
<a class="sourceLine" id="cb434-11" data-line-number="11">my_lm &lt;-<span class="st"> </span><span class="kw">train</span>(mpg <span class="op">~</span><span class="st"> </span>wt, </a>
<a class="sourceLine" id="cb434-12" data-line-number="12">               example_Train,</a>
<a class="sourceLine" id="cb434-13" data-line-number="13">               <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>,</a>
<a class="sourceLine" id="cb434-14" data-line-number="14">               <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>,<span class="st">&quot;scale&quot;</span>),</a>
<a class="sourceLine" id="cb434-15" data-line-number="15">               <span class="dt">trControl =</span> control)</a>
<a class="sourceLine" id="cb434-16" data-line-number="16"></a>
<a class="sourceLine" id="cb434-17" data-line-number="17">my_lm</a></code></pre></div>
<pre><code>## Linear Regression 
## 
## 28 samples
##  1 predictor
## 
## Pre-processing: centered (1), scaled (1) 
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 22, 22, 23, 22, 23 
## Resampling results:
## 
##   RMSE  Rsquared  MAE 
##   2.67  0.813     2.26
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<div class="sourceCode" id="cb436"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb436-1" data-line-number="1"><span class="kw">plot</span>(mpg<span class="op">~</span>wt,<span class="dt">data=</span>example_Train,<span class="dt">main=</span><span class="st">&quot;Train Data&quot;</span>)</a>
<a class="sourceLine" id="cb436-2" data-line-number="2"><span class="kw">abline</span>(lm_fit<span class="op">$</span>finalModel,<span class="dt">lty=</span><span class="dv">2</span>)</a></code></pre></div>
<p><img src="biosml_files/figure-html/pickthe1-1.png" width="672" /></p>
<p>So does this model perform better than a model built using a support vector machine ? It’s easy to generate such a model by reusing much of the same information from above.</p>
<p>This is arguably one of the best features of the <strong>caret</strong> package as it helps us execute any number of models and then assess their performance on new data. Let’s look at our models thus far. In fact, it’s so easy to generate them with caret, we’ll just make them here again. Let’s set a common <strong>trainControl</strong> list. We’ll use the <strong>Train</strong> and <strong>Test</strong> sets from above.</p>
<div class="sourceCode" id="cb437"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb437-1" data-line-number="1"><span class="co">#set.seed(123) # Make this example reproducible</span></a>
<a class="sourceLine" id="cb437-2" data-line-number="2">my_svm &lt;-<span class="st"> </span><span class="kw">train</span>(</a>
<a class="sourceLine" id="cb437-3" data-line-number="3">  mpg <span class="op">~</span><span class="st"> </span>wt, </a>
<a class="sourceLine" id="cb437-4" data-line-number="4">  example_Train,</a>
<a class="sourceLine" id="cb437-5" data-line-number="5">  <span class="dt">method =</span> <span class="st">&quot;svmRadial&quot;</span>,</a>
<a class="sourceLine" id="cb437-6" data-line-number="6">  <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>,<span class="st">&quot;scale&quot;</span>),</a>
<a class="sourceLine" id="cb437-7" data-line-number="7">  <span class="dt">trControl =</span> control</a>
<a class="sourceLine" id="cb437-8" data-line-number="8">)</a>
<a class="sourceLine" id="cb437-9" data-line-number="9"></a>
<a class="sourceLine" id="cb437-10" data-line-number="10">my_svm</a></code></pre></div>
<pre><code>## Support Vector Machines with Radial Basis Function Kernel 
## 
## 28 samples
##  1 predictor
## 
## Pre-processing: centered (1), scaled (1) 
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 23, 21, 23, 22, 23 
## Resampling results across tuning parameters:
## 
##   C     RMSE  Rsquared  MAE 
##   0.25  3.83  0.770     2.91
##   0.50  3.13  0.804     2.42
##   1.00  2.96  0.774     2.44
## 
## Tuning parameter &#39;sigma&#39; was held constant at a value of 4.94
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were sigma = 4.94 and C = 1.</code></pre>
<p>So let’s plot the training data as well as the resulting regression line (in the color black) coming from the lm object. We’ll also plot the results from the Support Vector Machine predictions on the same graph (in red). From this plot it appears that the Support Vector Machine does a better job of “following” the actual data - at least for the training data. We might even be able to improve the SVM performance if we tune the hyperparameters but the default without tuning seems better than the lm. It’s this type of observation that leads one to consider if there are yet other methods that would result in even better results.</p>
<div class="sourceCode" id="cb439"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb439-1" data-line-number="1"><span class="kw">plot</span>(mpg<span class="op">~</span>wt,<span class="dt">data=</span>example_Train,<span class="dt">main=</span><span class="st">&quot;Train Data&quot;</span>)</a>
<a class="sourceLine" id="cb439-2" data-line-number="2"><span class="kw">abline</span>(lm_fit<span class="op">$</span>finalModel)</a>
<a class="sourceLine" id="cb439-3" data-line-number="3"><span class="kw">grid</span>()</a>
<a class="sourceLine" id="cb439-4" data-line-number="4">svm_preds &lt;-<span class="st"> </span><span class="kw">predict</span>(my_svm,example_Train)</a>
<a class="sourceLine" id="cb439-5" data-line-number="5"></a>
<a class="sourceLine" id="cb439-6" data-line-number="6">pdf &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">cbind</span>(Train<span class="op">$</span>wt,svm_preds)) </a>
<a class="sourceLine" id="cb439-7" data-line-number="7"><span class="kw">points</span>(svm_preds<span class="op">~</span>V1,<span class="dt">data=</span>pdf[<span class="kw">order</span>(pdf<span class="op">$</span>V1),],<span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">pch=</span><span class="dv">4</span>,<span class="dt">type=</span><span class="st">&quot;l&quot;</span>)</a>
<a class="sourceLine" id="cb439-8" data-line-number="8"><span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>,<span class="kw">c</span>(<span class="st">&quot;lm&quot;</span>,<span class="st">&quot;svm&quot;</span>),<span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;black&quot;</span>,<span class="st">&quot;red&quot;</span>),<span class="dt">lty=</span><span class="dv">1</span>,<span class="dt">cex=</span><span class="dv">1</span>)</a></code></pre></div>
<p><img src="biosml_files/figure-html/pickthe3-1.png" width="672" /></p>
<p>In terms of the RMSE for the Test set predictions, which method is better (i.e. the lowest RMSE) ?</p>
<div class="sourceCode" id="cb440"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb440-1" data-line-number="1">lm_preds &lt;-<span class="st"> </span><span class="kw">predict</span>(my_lm,example_Train)</a>
<a class="sourceLine" id="cb440-2" data-line-number="2">svm_preds &lt;-<span class="st"> </span><span class="kw">predict</span>(my_svm,example_Train)</a>
<a class="sourceLine" id="cb440-3" data-line-number="3"><span class="co">#</span></a>
<a class="sourceLine" id="cb440-4" data-line-number="4"><span class="kw">cat</span>(<span class="st">&quot;RMSE for lm is: &quot;</span>,Metrics<span class="op">::</span><span class="kw">rmse</span>(example_Test<span class="op">$</span>mpg,lm_preds),<span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</a></code></pre></div>
<pre><code>## RMSE for lm is:  9.66</code></pre>
<div class="sourceCode" id="cb442"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb442-1" data-line-number="1"><span class="kw">cat</span>(<span class="st">&quot;RMSE for smv is: &quot;</span>,Metrics<span class="op">::</span><span class="kw">rmse</span>(example_Test<span class="op">$</span>mpg,svm_preds),<span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</a></code></pre></div>
<pre><code>## RMSE for smv is:  9.15</code></pre>
</div>
<div id="more-comparisons" class="section level2">
<h2><span class="header-section-number">10.2</span> More Comparisons</h2>
<p>Let’s keep going. We’ll use some more methods to see how they perform on the same data. The cool thing about caret is that we can reuse the same control object and seeds to facilitate reproducibility. We’ll pick two other methods in addition to the ones we have to see how we can compare them all. In reality, this would be just the beginning of the process, not the end since if we pick a method with hyperparameters then we would want to tune those.</p>
<div class="sourceCode" id="cb444"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb444-1" data-line-number="1">control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>,       <span class="co"># Cross Fold</span></a>
<a class="sourceLine" id="cb444-2" data-line-number="2">                        <span class="dt">number =</span> <span class="dv">5</span>,          <span class="co"># 5 Folds</span></a>
<a class="sourceLine" id="cb444-3" data-line-number="3">                        <span class="dt">verboseIter =</span> <span class="ot">FALSE</span>)  <span class="co"># Verbose</span></a></code></pre></div>
<p>Note that we will be predicting the MPG value as a function of all variables in the data frame.</p>
<div class="sourceCode" id="cb445"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb445-1" data-line-number="1"><span class="co"># Train the lm model</span></a>
<a class="sourceLine" id="cb445-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb445-3" data-line-number="3">comp_mod_lm &lt;-<span class="st"> </span><span class="kw">train</span>(mpg <span class="op">~</span><span class="st"> </span>.,</a>
<a class="sourceLine" id="cb445-4" data-line-number="4">                     example_Train,</a>
<a class="sourceLine" id="cb445-5" data-line-number="5">                     <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>,</a>
<a class="sourceLine" id="cb445-6" data-line-number="6">                     <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>,<span class="st">&quot;scale&quot;</span>),</a>
<a class="sourceLine" id="cb445-7" data-line-number="7">                     <span class="dt">trControl =</span> control)</a>
<a class="sourceLine" id="cb445-8" data-line-number="8"></a>
<a class="sourceLine" id="cb445-9" data-line-number="9"></a>
<a class="sourceLine" id="cb445-10" data-line-number="10"><span class="co"># Train the SVM Radial Model</span></a>
<a class="sourceLine" id="cb445-11" data-line-number="11"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb445-12" data-line-number="12">comp_mod_svm &lt;-<span class="st"> </span><span class="kw">train</span>(mpg <span class="op">~</span><span class="st"> </span>.,</a>
<a class="sourceLine" id="cb445-13" data-line-number="13">                     example_Train,</a>
<a class="sourceLine" id="cb445-14" data-line-number="14">                     <span class="dt">method =</span> <span class="st">&quot;svmRadial&quot;</span>,</a>
<a class="sourceLine" id="cb445-15" data-line-number="15">                     <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>,<span class="st">&quot;scale&quot;</span>),</a>
<a class="sourceLine" id="cb445-16" data-line-number="16">                     <span class="dt">trControl =</span> control)</a>
<a class="sourceLine" id="cb445-17" data-line-number="17"></a>
<a class="sourceLine" id="cb445-18" data-line-number="18"><span class="co"># Train the Lasso and Elastic-Net Regularized Generalized Linear Models</span></a>
<a class="sourceLine" id="cb445-19" data-line-number="19"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb445-20" data-line-number="20">comp_mod_glmnet &lt;-<span class="st"> </span><span class="kw">train</span>(mpg <span class="op">~</span><span class="st"> </span>.,</a>
<a class="sourceLine" id="cb445-21" data-line-number="21">                     example_Train,</a>
<a class="sourceLine" id="cb445-22" data-line-number="22">                     <span class="dt">method =</span> <span class="st">&quot;glmnet&quot;</span>,</a>
<a class="sourceLine" id="cb445-23" data-line-number="23">                     <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>,<span class="st">&quot;scale&quot;</span>),</a>
<a class="sourceLine" id="cb445-24" data-line-number="24">                     <span class="dt">trControl =</span> control)</a>
<a class="sourceLine" id="cb445-25" data-line-number="25"></a>
<a class="sourceLine" id="cb445-26" data-line-number="26"><span class="co"># Train the Random Forest Model</span></a>
<a class="sourceLine" id="cb445-27" data-line-number="27"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb445-28" data-line-number="28">comp_mod_ranger &lt;-<span class="st"> </span><span class="kw">train</span>(mpg <span class="op">~</span><span class="st"> </span>.,</a>
<a class="sourceLine" id="cb445-29" data-line-number="29">                     example_Train,</a>
<a class="sourceLine" id="cb445-30" data-line-number="30">                     <span class="dt">method =</span> <span class="st">&quot;ranger&quot;</span>,</a>
<a class="sourceLine" id="cb445-31" data-line-number="31">                     <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>,<span class="st">&quot;scale&quot;</span>),</a>
<a class="sourceLine" id="cb445-32" data-line-number="32">                     <span class="dt">trControl =</span> control)</a></code></pre></div>
</div>
<div id="using-the-resamples-function" class="section level2">
<h2><span class="header-section-number">10.3</span> Using the resamples() function</h2>
<p>Now, here comes the “magic”. Because we built four different modeling objects on the same data set and seeds we can now use the <strong>resamples</strong> function to collect, analyze, and visualize a set of results. This is pretty powerful.</p>
<div class="sourceCode" id="cb446"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb446-1" data-line-number="1">results &lt;-<span class="st"> </span><span class="kw">resamples</span>(<span class="kw">list</span>(<span class="dt">LM     =</span> comp_mod_lm, </a>
<a class="sourceLine" id="cb446-2" data-line-number="2">                          <span class="dt">SVM    =</span> comp_mod_svm, </a>
<a class="sourceLine" id="cb446-3" data-line-number="3">                          <span class="dt">GLMNET =</span> comp_mod_glmnet,</a>
<a class="sourceLine" id="cb446-4" data-line-number="4">                          <span class="dt">RANGER =</span> comp_mod_ranger))</a>
<a class="sourceLine" id="cb446-5" data-line-number="5"></a>
<a class="sourceLine" id="cb446-6" data-line-number="6"><span class="kw">summary</span>(results)</a></code></pre></div>
<pre><code>## 
## Call:
## summary.resamples(object = results)
## 
## Models: LM, SVM, GLMNET, RANGER 
## Number of resamples: 5 
## 
## MAE 
##        Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s
## LM     1.81    1.84   2.51 2.53    2.87 3.60    0
## SVM    2.30    2.46   3.18 3.05    3.45 3.85    0
## GLMNET 1.86    1.94   2.21 2.12    2.29 2.31    0
## RANGER 1.74    1.84   1.93 2.16    2.51 2.77    0
## 
## RMSE 
##        Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s
## LM     2.17    2.36   2.86 2.96    3.10 4.29    0
## SVM    2.54    3.11   3.54 3.47    3.76 4.41    0
## GLMNET 2.16    2.27   2.31 2.51    2.66 3.14    0
## RANGER 1.84    1.90   2.28 2.51    3.20 3.35    0
## 
## Rsquared 
##         Min. 1st Qu. Median  Mean 3rd Qu.  Max. NA&#39;s
## LM     0.514   0.641  0.830 0.749   0.859 0.900    0
## SVM    0.664   0.674  0.737 0.777   0.875 0.936    0
## GLMNET 0.808   0.853  0.882 0.882   0.885 0.981    0
## RANGER 0.858   0.878  0.893 0.907   0.923 0.981    0</code></pre>
<p>We can visualize the results. For whatever reason, the caret package favors use of the <strong>lattice</strong> graphics package. Most of the work here is in telling the dotplot function that we need to plot each of the metrics on its own respective scale to improve viewabilty.</p>
<div class="sourceCode" id="cb448"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb448-1" data-line-number="1">scales &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">x=</span><span class="kw">list</span>(<span class="dt">relation=</span><span class="st">&quot;free&quot;</span>), <span class="dt">y=</span><span class="kw">list</span>(<span class="dt">relation=</span><span class="st">&quot;free&quot;</span>)) </a>
<a class="sourceLine" id="cb448-2" data-line-number="2"><span class="kw">dotplot</span>(results,<span class="dt">scales=</span>scales)</a></code></pre></div>
<p><img src="biosml_files/figure-html/dpres-1.png" width="672" /></p>
<div class="sourceCode" id="cb449"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb449-1" data-line-number="1">scales &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">x=</span><span class="kw">list</span>(<span class="dt">relation=</span><span class="st">&quot;free&quot;</span>), <span class="dt">y=</span><span class="kw">list</span>(<span class="dt">relation=</span><span class="st">&quot;free&quot;</span>)) </a>
<a class="sourceLine" id="cb449-2" data-line-number="2"><span class="kw">bwplot</span>(results, <span class="dt">scales=</span>scales)</a></code></pre></div>
<p><img src="biosml_files/figure-html/bwscale-1.png" width="672" /></p>
<div class="sourceCode" id="cb450"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb450-1" data-line-number="1"><span class="co">#bwplot(results)</span></a></code></pre></div>
</div>
<div id="model-performance" class="section level2">
<h2><span class="header-section-number">10.4</span> Model Performance</h2>
<p>Of course, we can now use the Test data frame to see how the RMSE looks on the holdout data frame.</p>
<div class="sourceCode" id="cb451"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb451-1" data-line-number="1">Metrics<span class="op">::</span><span class="kw">rmse</span>(example_Test<span class="op">$</span>mpg,<span class="kw">predict</span>(comp_mod_lm,example_Test))</a></code></pre></div>
<pre><code>## [1] 4.81</code></pre>
<div class="sourceCode" id="cb453"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb453-1" data-line-number="1">Metrics<span class="op">::</span><span class="kw">rmse</span>(example_Test<span class="op">$</span>mpg,<span class="kw">predict</span>(comp_mod_svm,example_Test))</a></code></pre></div>
<pre><code>## [1] 3.66</code></pre>
<div class="sourceCode" id="cb455"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb455-1" data-line-number="1">Metrics<span class="op">::</span><span class="kw">rmse</span>(example_Test<span class="op">$</span>mpg,<span class="kw">predict</span>(comp_mod_svm,example_Test))</a></code></pre></div>
<pre><code>## [1] 3.66</code></pre>
<div class="sourceCode" id="cb457"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb457-1" data-line-number="1">Metrics<span class="op">::</span><span class="kw">rmse</span>(example_Test<span class="op">$</span>mpg,<span class="kw">predict</span>(comp_mod_ranger,example_Test))</a></code></pre></div>
<pre><code>## [1] 2.29</code></pre>
</div>
<div id="feature-evaluation" class="section level2">
<h2><span class="header-section-number">10.5</span> Feature Evaluation</h2>
<p>Features are the columns in your data set. Up until now we have not been concerned with the formula being specified choosing rather to focus on how to run models with the caret package. However, knowing how to select the “best” subset of features is important since an overspcified formula might result in very long training times and, even then, it might not be that good of a model for predicting out of sample error. Of course, various methods have ways to deal with this problem.</p>
<p>For example, Stepwise regression is one way to look at combinations of predictor variables to arrive at the optimal feature set according to some score (e.g. AIC, BIC). This process is implemented recursively. However, none of this should be a substitute for solid intuition about the data or knowing how features vary with each other (if at all).
Still, packages such as caret have ways to assist with feature selection.</p>
<div id="identifying-redundant-features" class="section level3">
<h3><span class="header-section-number">10.5.1</span> Identifying Redundant Features</h3>
<p>Lots of Data Scientists and model builders would like to know which features are important and which are redundant - BEFORE building the model. Many times, people just include all features and rely upon post model statistics to diagnose the model. This is fine though there are automated methods to help. Some statisticians do not like this approach (e.g. Step Wise Regression) because it emphasizes a scoring metric that is computed as a function of other measures though that process itself might be incomplete in some way. Ideally, you would have some up front idea about the data.</p>
</div>
<div id="highly-correlated-variables" class="section level3">
<h3><span class="header-section-number">10.5.2</span> Highly Correlated Variables</h3>
<p>In an earlier section we looked at the correlations between the variables in the mtcars data frame. When there are a number of strongly correlated variables the issue of multicollinerarity might exist. One variable might be sufficient to represent the information of one or more other variables which is useful when building models because it would allow us to leave out comparatively low information variables.</p>
<p>Techniques like PCA Principal Components Analysis can be used to recombine features in a way that explains most of the variation of data set. A simpler way might be to look for highly correlated sets of variables and remove those over a certain threshold of correlation from the data frame.</p>
<p>Let’s look at the mtcars data frame again to see what variables are correlated.</p>
<div class="sourceCode" id="cb459"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb459-1" data-line-number="1"><span class="kw">data</span>(mtcars)</a>
<a class="sourceLine" id="cb459-2" data-line-number="2">correlations &lt;-<span class="st"> </span><span class="kw">cor</span>(mtcars)</a>
<a class="sourceLine" id="cb459-3" data-line-number="3">correlations[<span class="dv">1</span><span class="op">:</span><span class="dv">6</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">6</span>]</a></code></pre></div>
<pre><code>##         mpg    cyl   disp     hp   drat     wt
## mpg   1.000 -0.852 -0.848 -0.776  0.681 -0.868
## cyl  -0.852  1.000  0.902  0.832 -0.700  0.782
## disp -0.848  0.902  1.000  0.791 -0.710  0.888
## hp   -0.776  0.832  0.791  1.000 -0.449  0.659
## drat  0.681 -0.700 -0.710 -0.449  1.000 -0.712
## wt   -0.868  0.782  0.888  0.659 -0.712  1.000</code></pre>
<p>It turns out that there are lots of strong correlations going on here. The darker the circle the stronger the correlation.</p>
<div class="sourceCode" id="cb461"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb461-1" data-line-number="1"><span class="kw">suppressMessages</span>(<span class="kw">library</span>(corrplot))</a>
<a class="sourceLine" id="cb461-2" data-line-number="2"><span class="kw">corrplot</span>(correlations, <span class="dt">order=</span><span class="st">&quot;hclust&quot;</span>)</a></code></pre></div>
<p><img src="biosml_files/figure-html/unnamed-chunk-63-1.png" width="672" /></p>
<p>The <strong>caret</strong> package has some functions that can help us identify highly correlated variables that might be a candidates for removal prior to use in building a model. One of the variables that is highly correlated with others is <strong>mpg</strong> Since that is the one we are trying to predict, we’ll keep it around. The following columns from mtcars have a correlation of .75 (or higher) with some other variable(s)</p>
<div class="sourceCode" id="cb462"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb462-1" data-line-number="1">(highcorr &lt;-<span class="st"> </span><span class="kw">findCorrelation</span>(correlations, <span class="dt">cutoff=</span>.<span class="dv">75</span>))</a></code></pre></div>
<pre><code>## [1]  2  3  1 10</code></pre>
<div class="sourceCode" id="cb464"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb464-1" data-line-number="1"><span class="kw">names</span>(mtcars[,highcorr])</a></code></pre></div>
<pre><code>## [1] &quot;cyl&quot;  &quot;disp&quot; &quot;mpg&quot;  &quot;gear&quot;</code></pre>
<p>So let’s remove those variables from consideration although we’ll keep the <strong>mpg</strong> variable since that is what we will be predicting.</p>
<div class="sourceCode" id="cb466"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb466-1" data-line-number="1"><span class="kw">data</span>(mtcars)</a>
<a class="sourceLine" id="cb466-2" data-line-number="2">decorrel_mtcars &lt;-<span class="st"> </span>mtcars[,<span class="op">-</span>highcorr[highcorr <span class="op">!=</span><span class="st"> </span><span class="dv">1</span>]]</a></code></pre></div>
<p>This might yield a better model though it’s tough to tell.</p>
<div class="sourceCode" id="cb467"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb467-1" data-line-number="1">idx &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(decorrel_mtcars<span class="op">$</span>mpg, <span class="dt">p =</span> <span class="fl">.8</span>, </a>
<a class="sourceLine" id="cb467-2" data-line-number="2">                                  <span class="dt">list =</span> <span class="ot">FALSE</span>, </a>
<a class="sourceLine" id="cb467-3" data-line-number="3">                                  <span class="dt">times =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb467-4" data-line-number="4">train &lt;-<span class="st"> </span>decorrel_mtcars[ idx,]</a>
<a class="sourceLine" id="cb467-5" data-line-number="5">test  &lt;-<span class="st"> </span>decorrel_mtcars[<span class="op">-</span>idx,]</a>
<a class="sourceLine" id="cb467-6" data-line-number="6"></a>
<a class="sourceLine" id="cb467-7" data-line-number="7">decorrel_lm &lt;-<span class="st"> </span><span class="kw">train</span>(mpg<span class="op">~</span>.,</a>
<a class="sourceLine" id="cb467-8" data-line-number="8">                     <span class="dt">data =</span> train,</a>
<a class="sourceLine" id="cb467-9" data-line-number="9">                     <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>,</a>
<a class="sourceLine" id="cb467-10" data-line-number="10">                     <span class="dt">metric =</span> <span class="st">&quot;RMSE&quot;</span>)</a></code></pre></div>
</div>
<div id="ranking-features" class="section level3">
<h3><span class="header-section-number">10.5.3</span> Ranking Features</h3>
<p>Another approach would be to identify the important variables as determined by a rigorous, reliable and statistically aware process. Some models will report this information as part of the computation. The <strong>caret</strong> package has a function called <strong>varImp</strong> which helps to calculate relative variable importance within an object produced by the <strong>train</strong> function. What this means for us is that we can take most models built with <strong>train</strong> and pass it to the <strong>varImp</strong> function.</p>
<div class="sourceCode" id="cb468"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb468-1" data-line-number="1">idx &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(mtcars<span class="op">$</span>mpg, <span class="dt">p =</span> <span class="fl">.8</span>, </a>
<a class="sourceLine" id="cb468-2" data-line-number="2">                                  <span class="dt">list =</span> <span class="ot">FALSE</span>, </a>
<a class="sourceLine" id="cb468-3" data-line-number="3">                                  <span class="dt">times =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb468-4" data-line-number="4">train &lt;-<span class="st"> </span>mtcars[ idx,]</a>
<a class="sourceLine" id="cb468-5" data-line-number="5">test  &lt;-<span class="st"> </span>mtcars[<span class="op">-</span>idx,]</a>
<a class="sourceLine" id="cb468-6" data-line-number="6"></a>
<a class="sourceLine" id="cb468-7" data-line-number="7">myLm &lt;-<span class="st"> </span><span class="kw">train</span>(mpg<span class="op">~</span>.,</a>
<a class="sourceLine" id="cb468-8" data-line-number="8">                     <span class="dt">data =</span> train,</a>
<a class="sourceLine" id="cb468-9" data-line-number="9">                     <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>,</a>
<a class="sourceLine" id="cb468-10" data-line-number="10">                     <span class="dt">metric =</span> <span class="st">&quot;RMSE&quot;</span>)</a>
<a class="sourceLine" id="cb468-11" data-line-number="11"></a>
<a class="sourceLine" id="cb468-12" data-line-number="12"><span class="kw">summary</span>(myLm)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = .outcome ~ ., data = dat)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -3.721 -1.312 -0.397  1.671  4.128 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -15.3244    24.2510   -0.63     0.54
## cyl           1.2384     1.3024    0.95     0.35
## disp          0.0112     0.0207    0.54     0.59
## hp           -0.0100     0.0243   -0.41     0.69
## drat          2.7566     2.2031    1.25     0.23
## wt           -2.9298     2.2589   -1.30     0.21
## qsec          0.9587     0.7400    1.30     0.21
## vs            1.0514     2.3031    0.46     0.65
## am            2.7462     2.1074    1.30     0.21
## gear          3.1311     2.0798    1.51     0.15
## carb         -1.3704     1.0927   -1.25     0.23
## 
## Residual standard error: 2.64 on 17 degrees of freedom
## Multiple R-squared:  0.877,  Adjusted R-squared:  0.805 
## F-statistic: 12.1 on 10 and 17 DF,  p-value: 6.93e-06</code></pre>
<p>If you look at the summary of the model it looks pretty bleak because mot of the coefficients aren’t significant. You wouldn’t want to take your career on this model.</p>
<p>Now let’s see what variables are considered to be important by using <strong>varImp</strong>. Remember that this might lead us to use a subset of the more prominent variables in the formation of a new model in case, for example, the coefficients in the existing model weren’t significant.</p>
<div class="sourceCode" id="cb470"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb470-1" data-line-number="1"><span class="kw">plot</span>(<span class="kw">varImp</span>(myLm))</a></code></pre></div>
<p><img src="biosml_files/figure-html/unnamed-chunk-68-1.png" width="672" /></p>
<p>How do things look in the model if we limit the formula to some of the “important” (allegedly) features ? Well, it explains more of the variance with fewer predictors (the R^2 value). Both <strong>wt</strong> and <strong>hp</strong> are significant. That’s a start.</p>
<div class="sourceCode" id="cb471"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb471-1" data-line-number="1">myLm &lt;-<span class="st"> </span><span class="kw">train</span>(mpg<span class="op">~</span>wt<span class="op">+</span>disp<span class="op">+</span>hp,</a>
<a class="sourceLine" id="cb471-2" data-line-number="2">                     <span class="dt">data =</span> train,</a>
<a class="sourceLine" id="cb471-3" data-line-number="3">                     <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>,</a>
<a class="sourceLine" id="cb471-4" data-line-number="4">                     <span class="dt">metric =</span> <span class="st">&quot;RMSE&quot;</span>)</a>
<a class="sourceLine" id="cb471-5" data-line-number="5"></a>
<a class="sourceLine" id="cb471-6" data-line-number="6"><span class="kw">summary</span>(myLm)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = .outcome ~ ., data = dat)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -3.976 -1.859 -0.106  1.093  5.998 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  3.70e+01   2.52e+00   14.65  1.8e-13 ***
## wt          -3.97e+00   1.30e+00   -3.04   0.0056 ** 
## disp        -4.25e-05   1.20e-02    0.00   0.9972    
## hp          -2.78e-02   1.33e-02   -2.09   0.0475 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.82 on 24 degrees of freedom
## Multiple R-squared:  0.803,  Adjusted R-squared:  0.778 
## F-statistic: 32.6 on 3 and 24 DF,  p-value: 1.26e-08</code></pre>
</div>
<div id="feature-selection" class="section level3">
<h3><span class="header-section-number">10.5.4</span> Feature Selection</h3>
<p>Being able to automatically include only the most important features is desirable (again an example might be Step Wise Regression). Caret provides its own function to do a simple backwards selection to recursively eliminate features based on how they contribute (or not) to the model. You could do all of this by hand yourself of course.</p>
<p>Automatic feature selection methods can be used to build many models with different subsets of a dataset and identify those attributes that are and are not required to build an accurate model. The <strong>caret</strong> package has a function called *rfe** to implement this. However, it involves some additional setup:</p>
<div class="sourceCode" id="cb473"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb473-1" data-line-number="1">rfeControl &lt;-<span class="st"> </span><span class="kw">rfeControl</span>(<span class="dt">functions=</span>lmFuncs, <span class="dt">method=</span><span class="st">&quot;cv&quot;</span>, <span class="dt">number=</span><span class="dv">10</span>)</a>
<a class="sourceLine" id="cb473-2" data-line-number="2"></a>
<a class="sourceLine" id="cb473-3" data-line-number="3">results &lt;-<span class="st"> </span><span class="kw">rfe</span>(mtcars[,<span class="dv">2</span><span class="op">:</span><span class="dv">11</span>], </a>
<a class="sourceLine" id="cb473-4" data-line-number="4">               mtcars[,<span class="dv">1</span>], </a>
<a class="sourceLine" id="cb473-5" data-line-number="5">               <span class="dt">sizes=</span><span class="kw">c</span>(<span class="dv">2</span><span class="op">:</span><span class="dv">11</span>), </a>
<a class="sourceLine" id="cb473-6" data-line-number="6">               <span class="dt">rfeControl=</span>rfeControl)</a>
<a class="sourceLine" id="cb473-7" data-line-number="7"></a>
<a class="sourceLine" id="cb473-8" data-line-number="8"><span class="kw">print</span>(results)</a></code></pre></div>
<pre><code>## 
## Recursive feature selection
## 
## Outer resampling method: Cross-Validated (10 fold) 
## 
## Resampling performance over subset size:
## 
##  Variables RMSE Rsquared  MAE RMSESD RsquaredSD MAESD Selected
##          2 3.13    0.925 2.74   1.31     0.0792 1.204         
##          3 3.13    0.850 2.76   1.18     0.2360 1.131         
##          4 3.13    0.863 2.76   1.20     0.2048 1.161         
##          5 2.91    0.855 2.50   1.03     0.2298 0.867        *
##          6 3.34    0.859 2.88   1.41     0.2241 1.274         
##          7 3.48    0.862 3.02   1.71     0.2212 1.573         
##          8 3.51    0.864 3.07   1.81     0.2079 1.730         
##          9 3.61    0.865 3.16   2.00     0.1935 1.901         
##         10 3.51    0.874 3.06   1.94     0.1836 1.859         
## 
## The top 5 variables (out of 5):
##    wt, am, drat, gear, qsec</code></pre>
<div class="sourceCode" id="cb475"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb475-1" data-line-number="1"><span class="kw">plot</span>(results,<span class="dt">type=</span><span class="kw">c</span>(<span class="st">&quot;g&quot;</span>,<span class="st">&quot;o&quot;</span>))</a></code></pre></div>
<p><img src="biosml_files/figure-html/rfectrl11-1.png" width="672" /></p>
<div class="sourceCode" id="cb476"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb476-1" data-line-number="1">myLm &lt;-<span class="st"> </span><span class="kw">train</span>(mpg<span class="op">~</span>wt<span class="op">+</span>am<span class="op">+</span>qsec,</a>
<a class="sourceLine" id="cb476-2" data-line-number="2">                     <span class="dt">data =</span> train,</a>
<a class="sourceLine" id="cb476-3" data-line-number="3">                     <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>,</a>
<a class="sourceLine" id="cb476-4" data-line-number="4">                     <span class="dt">metric =</span> <span class="st">&quot;RMSE&quot;</span>)</a>
<a class="sourceLine" id="cb476-5" data-line-number="5"></a>
<a class="sourceLine" id="cb476-6" data-line-number="6"><span class="kw">summary</span>(myLm)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = .outcome ~ ., data = dat)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -3.403 -1.779 -0.781  1.335  4.785 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   11.464      7.875    1.46   0.1584    
## wt            -3.833      0.779   -4.92  5.1e-05 ***
## am             3.013      1.577    1.91   0.0681 .  
## qsec           1.108      0.331    3.35   0.0027 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.58 on 24 degrees of freedom
## Multiple R-squared:  0.834,  Adjusted R-squared:  0.813 
## F-statistic: 40.2 on 3 and 24 DF,  p-value: 1.62e-09</code></pre>
</div>
<div id="recursive-feature-elimination" class="section level3">
<h3><span class="header-section-number">10.5.5</span> Recursive Feature Elimination</h3>
<p>The general idea with this approach is to build models using combinations of predictors to arrive at the best model according to some metric such as RMSE. Some predictors might be discarded along the way resulting in a “leaner” feature set that would then hopefully be easier to defend than a more complex or fully specified feature set.</p>
<p>There is no free lunch here in that blindly accepting the features handed to you by a recursive or automatic method should not be considered authoritative especially if you have a reason to believe that some key feature has been excluded. Many people, however, like to use these methods as a starting point. You would still need to review the diagnostics associated with a final model to determine if it is statistically sound.</p>
<p>According to the <a href="https://topepo.github.io/caret/feature-selection-overview.html#models-with-built-in-feature-selection"><strong>caret</strong> documentation</a> there are a large number of supported models that contain some form of embedded or built-in feature selection. Such functions are doing you a favor (or not) by showing you the importance of contributing features.</p>
<p><img src="pics/feature_selection.png" /></p>
<div id="an-example-1" class="section level4">
<h4><span class="header-section-number">10.5.5.1</span> An Example</h4>
<p>Let’s work with the <strong>lm</strong> function again to see if we can find some interesting features using some caret functions. The main function for Recursive Feature Elimination is <strong>rfe</strong> which, like the <strong>train</strong> function, accepts a control object to help guide the process. Here we are telling the <strong>rfe</strong> function to use some helper functions to assess predictors. We don’t need to pass it a model - it handles these things under the hood. In this case we’ll use 10 Fold Cross Validation.</p>
<div class="sourceCode" id="cb478"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb478-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb478-2" data-line-number="2">control &lt;-<span class="st"> </span><span class="kw">rfeControl</span>(<span class="dt">functions=</span>lmFuncs, <span class="dt">method=</span><span class="st">&quot;cv&quot;</span>,<span class="dt">number=</span><span class="dv">10</span>)</a>
<a class="sourceLine" id="cb478-3" data-line-number="3"></a>
<a class="sourceLine" id="cb478-4" data-line-number="4">results &lt;-<span class="st"> </span><span class="kw">rfe</span>(mtcars[,<span class="dv">2</span><span class="op">:</span><span class="dv">11</span>],     <span class="co"># Predictor features</span></a>
<a class="sourceLine" id="cb478-5" data-line-number="5">               mtcars[,<span class="dv">1</span>],        <span class="co"># Predicted features - mpg</span></a>
<a class="sourceLine" id="cb478-6" data-line-number="6">               <span class="dt">sizes=</span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>),      <span class="co"># pick groups of predictors 1-5 </span></a>
<a class="sourceLine" id="cb478-7" data-line-number="7">               <span class="dt">rfeControl=</span>control)</a>
<a class="sourceLine" id="cb478-8" data-line-number="8">results</a></code></pre></div>
<pre><code>## 
## Recursive feature selection
## 
## Outer resampling method: Cross-Validated (10 fold) 
## 
## Resampling performance over subset size:
## 
##  Variables RMSE Rsquared  MAE RMSESD RsquaredSD MAESD Selected
##          1 3.11    0.849 2.76   1.88      0.240  1.81         
##          2 3.13    0.839 2.80   1.65      0.269  1.57         
##          3 2.70    0.887 2.42   1.37      0.154  1.20        *
##          4 2.92    0.866 2.51   1.44      0.166  1.24         
##          5 2.99    0.888 2.63   1.44      0.146  1.25         
##         10 3.39    0.886 3.06   1.45      0.161  1.28         
## 
## The top 3 variables (out of 3):
##    wt, am, drat</code></pre>
<div class="sourceCode" id="cb480"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb480-1" data-line-number="1"><span class="kw">plot</span>(results,<span class="dt">type=</span><span class="kw">c</span>(<span class="st">&quot;o&quot;</span>,<span class="st">&quot;g&quot;</span>))</a></code></pre></div>
<p><img src="biosml_files/figure-html/unnamed-chunk-70-1.png" width="672" /></p>
<p>What we get back is some idea about the important features. We could then build a model with caret that uses only these features to see if the suggested RMSE value mentioned in the rfe process matches.</p>
</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="using-methods-other-than-lm.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="data-pre-processing.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
