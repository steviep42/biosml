<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Caret Package | Predictive Learning in R - BIOS 534</title>
  <meta name="description" content="Chapter 6 Caret Package | Predictive Learning in R - BIOS 534" />
  <meta name="generator" content="bookdown 0.14 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Caret Package | Predictive Learning in R - BIOS 534" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Caret Package | Predictive Learning in R - BIOS 534" />
  
  
  

<meta name="author" content="Steve Pittard" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="using-methods-other-than-lm.html"/>
<link rel="next" href="picking-the-best-model.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a></li>
<li class="chapter" data-level="2" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html"><i class="fa fa-check"></i><b>2</b> Predictive / Supervised Learning</a><ul>
<li class="chapter" data-level="2.1" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#explanation-vs-prediction"><i class="fa fa-check"></i><b>2.1</b> Explanation vs Prediction</a></li>
<li class="chapter" data-level="2.2" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#two-types-of-predictive-models"><i class="fa fa-check"></i><b>2.2</b> Two Types of Predictive Models:</a></li>
<li class="chapter" data-level="2.3" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#bias-vs-variance"><i class="fa fa-check"></i><b>2.3</b> Bias vs Variance</a></li>
<li class="chapter" data-level="2.4" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#overfitting-and-underfitting"><i class="fa fa-check"></i><b>2.4</b> Overfitting and Underfitting</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="a-motivating-example-linear-regression.html"><a href="a-motivating-example-linear-regression.html"><i class="fa fa-check"></i><b>3</b> A Motivating Example - Linear Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="a-motivating-example-linear-regression.html"><a href="a-motivating-example-linear-regression.html#scatterplot"><i class="fa fa-check"></i><b>3.1</b> Scatterplot</a></li>
<li class="chapter" data-level="3.2" data-path="a-motivating-example-linear-regression.html"><a href="a-motivating-example-linear-regression.html#correlations"><i class="fa fa-check"></i><b>3.2</b> Correlations</a></li>
<li class="chapter" data-level="3.3" data-path="a-motivating-example-linear-regression.html"><a href="a-motivating-example-linear-regression.html#building-a-model---in-sample-error"><i class="fa fa-check"></i><b>3.3</b> Building A Model - In Sample Error</a></li>
<li class="chapter" data-level="3.4" data-path="a-motivating-example-linear-regression.html"><a href="a-motivating-example-linear-regression.html#out-of-sample-data"><i class="fa fa-check"></i><b>3.4</b> Out Of Sample Data</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="training-test-data.html"><a href="training-test-data.html"><i class="fa fa-check"></i><b>4</b> Training / Test Data</a><ul>
<li class="chapter" data-level="4.1" data-path="training-test-data.html"><a href="training-test-data.html#cross-fold-validation-continued"><i class="fa fa-check"></i><b>4.1</b> Cross Fold Validation Continued</a></li>
<li class="chapter" data-level="4.2" data-path="training-test-data.html"><a href="training-test-data.html#create-a-function-to-automate-things"><i class="fa fa-check"></i><b>4.2</b> Create A Function To Automate Things</a></li>
<li class="chapter" data-level="4.3" data-path="training-test-data.html"><a href="training-test-data.html#repeated-cross-validation"><i class="fa fa-check"></i><b>4.3</b> Repeated Cross Validation</a></li>
<li class="chapter" data-level="4.4" data-path="training-test-data.html"><a href="training-test-data.html#bootstrap"><i class="fa fa-check"></i><b>4.4</b> Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="using-methods-other-than-lm.html"><a href="using-methods-other-than-lm.html"><i class="fa fa-check"></i><b>5</b> Using Methods Other Than lm</a><ul>
<li class="chapter" data-level="5.1" data-path="using-methods-other-than-lm.html"><a href="using-methods-other-than-lm.html#parameters-vs-hyperparameters"><i class="fa fa-check"></i><b>5.1</b> Parameters vs Hyperparameters</a></li>
<li class="chapter" data-level="5.2" data-path="using-methods-other-than-lm.html"><a href="using-methods-other-than-lm.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>5.2</b> Hyperparameter Tuning</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="caret-package.html"><a href="caret-package.html"><i class="fa fa-check"></i><b>6</b> Caret Package</a><ul>
<li class="chapter" data-level="6.1" data-path="caret-package.html"><a href="caret-package.html#putting-caret-to-work"><i class="fa fa-check"></i><b>6.1</b> Putting caret To Work</a></li>
<li class="chapter" data-level="6.2" data-path="caret-package.html"><a href="caret-package.html#back-to-the-beginning"><i class="fa fa-check"></i><b>6.2</b> Back To The Beginning</a></li>
<li class="chapter" data-level="6.3" data-path="caret-package.html"><a href="caret-package.html#splitting"><i class="fa fa-check"></i><b>6.3</b> Splitting</a></li>
<li class="chapter" data-level="6.4" data-path="caret-package.html"><a href="caret-package.html#one-size-fits-all"><i class="fa fa-check"></i><b>6.4</b> One Size Fits All</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html"><i class="fa fa-check"></i><b>7</b> Picking The Best Model</a><ul>
<li class="chapter" data-level="7.1" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#using-the-resamples-function"><i class="fa fa-check"></i><b>7.1</b> Using the resamples() function</a></li>
<li class="chapter" data-level="7.2" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#model-performance"><i class="fa fa-check"></i><b>7.2</b> Model Performance</a></li>
<li class="chapter" data-level="7.3" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#feature-selection"><i class="fa fa-check"></i><b>7.3</b> Feature Selection</a><ul>
<li class="chapter" data-level="7.3.1" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#recursive-feature-elimination"><i class="fa fa-check"></i><b>7.3.1</b> Recursive Feature Elimination</a></li>
<li class="chapter" data-level="7.3.2" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#redundant-feature-removal"><i class="fa fa-check"></i><b>7.3.2</b> Redundant Feature Removal</a></li>
<li class="chapter" data-level="7.3.3" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#feature-importance"><i class="fa fa-check"></i><b>7.3.3</b> Feature Importance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="data-pre-processing.html"><a href="data-pre-processing.html"><i class="fa fa-check"></i><b>8</b> Data Pre Processing</a><ul>
<li class="chapter" data-level="8.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#look-for-highly-correlated-variables"><i class="fa fa-check"></i><b>8.1</b> Look for Highly Correlated Variables</a></li>
<li class="chapter" data-level="8.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#scaling-considerations"><i class="fa fa-check"></i><b>8.2</b> Scaling Considerations</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="classification-problems.html"><a href="classification-problems.html"><i class="fa fa-check"></i><b>9</b> Classification Problems</a><ul>
<li class="chapter" data-level="9.1" data-path="classification-problems.html"><a href="classification-problems.html#hypothesis-testing"><i class="fa fa-check"></i><b>9.1</b> Hypothesis Testing</a><ul>
<li class="chapter" data-level="9.1.1" data-path="classification-problems.html"><a href="classification-problems.html#type-i-and-ii-errors"><i class="fa fa-check"></i><b>9.1.1</b> Type I and II Errors</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="classification-problems.html"><a href="classification-problems.html#performance-measures"><i class="fa fa-check"></i><b>9.2</b> Performance Measures</a></li>
<li class="chapter" data-level="9.3" data-path="classification-problems.html"><a href="classification-problems.html#table-of-outcomes"><i class="fa fa-check"></i><b>9.3</b> Table of Outcomes</a><ul>
<li class="chapter" data-level="9.3.1" data-path="classification-problems.html"><a href="classification-problems.html#computing-performance-metrics"><i class="fa fa-check"></i><b>9.3.1</b> Computing Performance Metrics</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="classification-problems.html"><a href="classification-problems.html#picking-the-right-metric"><i class="fa fa-check"></i><b>9.4</b> Picking the Right Metric</a><ul>
<li class="chapter" data-level="9.4.1" data-path="classification-problems.html"><a href="classification-problems.html#working-with-prediction-probabilities"><i class="fa fa-check"></i><b>9.4.1</b> Working With Prediction Probabilities</a></li>
<li class="chapter" data-level="9.4.2" data-path="classification-problems.html"><a href="classification-problems.html#creating-a-roc-curve"><i class="fa fa-check"></i><b>9.4.2</b> Creating a ROC Curve</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="classification-example.html"><a href="classification-example.html"><i class="fa fa-check"></i><b>10</b> Classification Example</a><ul>
<li class="chapter" data-level="10.1" data-path="classification-example.html"><a href="classification-example.html#boxplots-and-densities"><i class="fa fa-check"></i><b>10.1</b> Boxplots And Densities</a></li>
<li class="chapter" data-level="10.2" data-path="classification-example.html"><a href="classification-example.html#generalized-linear-models"><i class="fa fa-check"></i><b>10.2</b> Generalized Linear Models</a></li>
<li class="chapter" data-level="10.3" data-path="classification-example.html"><a href="classification-example.html#random-forests"><i class="fa fa-check"></i><b>10.3</b> Random Forests</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html"><i class="fa fa-check"></i><b>11</b> Using External ML Frameworks</a><ul>
<li class="chapter" data-level="11.1" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#using-h2o"><i class="fa fa-check"></i><b>11.1</b> Using h2o</a></li>
<li class="chapter" data-level="11.2" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#create-some-h20-models"><i class="fa fa-check"></i><b>11.2</b> Create Some h20 Models</a></li>
<li class="chapter" data-level="11.3" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#saving-a-model"><i class="fa fa-check"></i><b>11.3</b> Saving A Model</a></li>
<li class="chapter" data-level="11.4" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#using-the-h2o-auto-ml-feature"><i class="fa fa-check"></i><b>11.4</b> Using The h2o Auto ML Feature</a></li>
<li class="chapter" data-level="11.5" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#launching-a-job"><i class="fa fa-check"></i><b>11.5</b> Launching a Job</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Predictive Learning in R - BIOS 534</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="caret-package" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> Caret Package</h1>
<p>By now you are probably fatigued with understanding the details of writing the code to split data, doing Cross Validation, storing the results, and looking at descriptive stats associated with the resulting RMSE. And this is all before considering the various parameters associated with whatever method we wish to implement. Each function has its own set of requirements which may not extend to other functions. What we need (well, what we would like) is a framework to streamline this process and automate it as much as possible but not at the expense of understanding the results.</p>
<p>The <a href="https://topepo.github.io/caret/index.html"><strong>caret</strong></a> (<strong>C</strong>lassification <strong>A</strong>nd <strong>R</strong>egression <strong>T</strong>raining) package provides a uniform interface for calling different algorithms while simplifying the data splitting and RMSE calculation. It supports many different model types and also provides the ability to tune hyper parameters.</p>
<p>While being able to understand how to work with each specific function individually is a worthy goal, you might consider using caret to help you get up and running quickly. It is still your responsibility to know what the results are telling you - there is no magic here though the process of running different models and managing data is greatly simplified by using this package (at least I think so).</p>
<p>Note that <strong>caret</strong> provides a nice wrapper around the various modeling functions. Since each underlying model is itself a standalone R package and associated set of functions you can always call them directly should you prefer that approach. That’s what we have been doing in the earlier part of this text.</p>
<div id="putting-caret-to-work" class="section level2">
<h2><span class="header-section-number">6.1</span> Putting caret To Work</h2>
<p>It’s easy to get lost in all that we have been doing so let’s review what the typical predictive modeling workflow will look like:</p>
<ol style="list-style-type: decimal">
<li>Data Import (.csv., extraction from a database, etc)</li>
<li>Data Visualization</li>
<li><p>Data Prep - Missing, imputation - Scaling - Create dummy variables / one hot encoding - Dimensionality Reduction</p></li>
<li><p>Data Splitting (training / test) - Determine split ration - K-Fold Cross Validation (repeated)</p></li>
<li><p>Modeling</p></li>
<li><p>Model Evaluation</p></li>
</ol>
<p>To do step 5 requires some predefined idea of a performance metric. We have been using RMSE and will continue to do so as we rework some of the previous examples using the caret package.</p>
</div>
<div id="back-to-the-beginning" class="section level2">
<h2><span class="header-section-number">6.2</span> Back To The Beginning</h2>
<p>It is implied that in predictive modeling the ultimate goal is to generate a model that could be reasonably applied to new data. As we have learned, it is best to train any model on a data set that has been (re)sampled in some way (e.g. K Fold CV) which should help provide a more realistic estimate of “out of sample” error.</p>
<p>In our earliest example we tried to predict the MPG from mtcars using a basic linear modeling function. The caret package provides a uniform way to do this which allows us to easily substitute in alternative functions without having to majorly change our code. We can call the <strong>train</strong> function in such a way as to pass in any arguments that are specific to a method though in a way that allows for minimal alteration. We can also tell the <strong>train</strong> function that we want to evaluate RMSE as a performance measure. That is, it will “know” that our primary performance measure for a model is RMSE. Before we do that, however, we’ll make a test / train pair. The <strong>caret</strong> package provides ways to do that.</p>
</div>
<div id="splitting" class="section level2">
<h2><span class="header-section-number">6.3</span> Splitting</h2>
<p><strong>createDataPartition</strong> can be used to create test and train data splits according to some proportion. There is a function called <strong>createFolds</strong> can be used to generate balanced cross–validation groupings from a set of data. <strong>createResample</strong> can be used to make simple bootstrap samples. For now, we’ll just stick with <strong>createDataPartition</strong> for creating a test/train pair.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">idx &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(mtcars<span class="op">$</span>mpg, <span class="dt">p =</span> .<span class="dv">8</span>, 
                                  <span class="dt">list =</span> <span class="ot">FALSE</span>, 
                                  <span class="dt">times =</span> <span class="dv">1</span>)
<span class="kw">head</span>(idx)</code></pre></div>
<pre><code>##      Resample1
## [1,]         1
## [2,]         3
## [3,]         4
## [4,]         5
## [5,]         6
## [6,]         7</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Train &lt;-<span class="st"> </span>mtcars[ idx,]
Test  &lt;-<span class="st"> </span>mtcars[<span class="op">-</span>idx,]

<span class="co">#</span>
<span class="kw">nrow</span>(Train)</code></pre></div>
<pre><code>## [1] 28</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">nrow</span>(Test)</code></pre></div>
<pre><code>## [1] 4</code></pre>
<p>To actually create a model involves use of the <strong>train</strong> function which is the premier function in the <strong>caret</strong> package. It does what it name suggests - train models. Note that we tell it:</p>
<ol style="list-style-type: decimal">
<li>What we are trying to predict (a formula)</li>
<li>What our data set is (e.g. Train)</li>
<li>The desired method (“ml”)
<ul>
<li>Note that this method name MUST match an existing R modeling function</li>
</ul></li>
<li>A desired scoring metric. In this case we seek to minimize RMSE on future predictions</li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lm_fit &lt;-<span class="st"> </span><span class="kw">train</span>(mpg<span class="op">~</span>wt,
                <span class="dt">data=</span>Train,
                <span class="dt">method=</span><span class="st">&quot;lm&quot;</span>,
                <span class="dt">metric=</span><span class="st">&quot;RMSE&quot;</span>)</code></pre></div>
<p>We get back a single object that contains a lot of information that could help us figure out if the model is worth anything. But first, just type the name of fit object to see what you can see. This shows us information that has been derived from some re sampling activity across a number of bootstrapped samples.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lm_fit</code></pre></div>
<pre><code>## Linear Regression 
## 
## 28 samples
##  1 predictor
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 28, 28, 28, 28, 28, 28, ... 
## Resampling results:
## 
##   RMSE    Rsquared   MAE     
##   3.2103  0.7658741  2.560222
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(lm_fit)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = .outcome ~ ., data = dat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.2896 -2.2951 -0.2399  1.7935  6.8982 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  36.5028     1.9889   18.35  &lt; 2e-16 ***
## wt           -5.1776     0.6013   -8.61 4.34e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.951 on 26 degrees of freedom
## Multiple R-squared:  0.7404, Adjusted R-squared:  0.7304 
## F-statistic: 74.14 on 1 and 26 DF,  p-value: 4.337e-09</code></pre>
<p>So let’s apply this model to the test data frame</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">compute_rmse</span>(<span class="kw">predict</span>(lm_fit,Test),Test<span class="op">$</span>mpg)</code></pre></div>
<pre><code>## [1] 3.697121</code></pre>
<p>There is more here than meets the eye.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">names</span>(lm_fit)</code></pre></div>
<pre><code>##  [1] &quot;method&quot;       &quot;modelInfo&quot;    &quot;modelType&quot;    &quot;results&quot;     
##  [5] &quot;pred&quot;         &quot;bestTune&quot;     &quot;call&quot;         &quot;dots&quot;        
##  [9] &quot;metric&quot;       &quot;control&quot;      &quot;finalModel&quot;   &quot;preProcess&quot;  
## [13] &quot;trainingData&quot; &quot;resample&quot;     &quot;resampledCM&quot;  &quot;perfNames&quot;   
## [17] &quot;maximize&quot;     &quot;yLimits&quot;      &quot;times&quot;        &quot;levels&quot;      
## [21] &quot;terms&quot;        &quot;coefnames&quot;    &quot;xlevels&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">str</span>(lm_fit<span class="op">$</span>control,<span class="dv">1</span>)</code></pre></div>
<pre><code>## List of 28
##  $ method           : chr &quot;boot&quot;
##  $ number           : num 25
##  $ repeats          : logi NA
##  $ search           : chr &quot;grid&quot;
##  $ p                : num 0.75
##  $ initialWindow    : NULL
##  $ horizon          : num 1
##  $ fixedWindow      : logi TRUE
##  $ skip             : num 0
##  $ verboseIter      : logi FALSE
##  $ returnData       : logi TRUE
##  $ returnResamp     : chr &quot;final&quot;
##  $ savePredictions  : chr &quot;none&quot;
##  $ classProbs       : logi FALSE
##  $ summaryFunction  :function (data, lev = NULL, model = NULL)  
##  $ selectionFunction: chr &quot;best&quot;
##  $ preProcOptions   :List of 6
##  $ sampling         : NULL
##  $ index            :List of 25
##  $ indexOut         :List of 25
##  $ indexFinal       : NULL
##  $ timingSamps      : num 0
##  $ predictionBounds : logi [1:2] FALSE FALSE
##  $ seeds            :List of 26
##  $ adaptive         :List of 4
##  $ trim             : logi FALSE
##  $ allowParallel    : logi TRUE
##  $ yLimits          : num [1:2] 9.22 35.07</code></pre>
<p>Check out the some of the model characteristics</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(lm_fit)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = .outcome ~ ., data = dat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.2896 -2.2951 -0.2399  1.7935  6.8982 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  36.5028     1.9889   18.35  &lt; 2e-16 ***
## wt           -5.1776     0.6013   -8.61 4.34e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.951 on 26 degrees of freedom
## Multiple R-squared:  0.7404, Adjusted R-squared:  0.7304 
## F-statistic: 74.14 on 1 and 26 DF,  p-value: 4.337e-09</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(lm_fit<span class="op">$</span>finalModel)<span class="op">$</span>r.squared</code></pre></div>
<pre><code>## [1] 0.7403576</code></pre>
<p>We can go right to the final Model which contains the information for the</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lm_fit<span class="op">$</span>finalModel</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = .outcome ~ ., data = dat)
## 
## Coefficients:
## (Intercept)           wt  
##      36.503       -5.178</code></pre>
<p>So it looks like caret did some re sampling for us by default. However, we can specify cross fold validation if we wanted to. This requires a more involved form of the <strong>train</strong> function.</p>
<p>You can influence the <strong>train</strong> function by passing a “special” list to it via the <strong>trControl</strong> argument. This gets a bit confusing because the primary function to train models is called <strong>train</strong> and the command used to create the special is called <strong>trainControl</strong> and the argument in the <strong>train</strong> function is called <strong>trControl</strong>. With use, it becomes easier to remember the difference though at first it’s confusing.</p>
<p>Here we train the model as before but specifically requesting a Cross Fold Validation method of 10 times. We are requesting verbose output.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">my_lm &lt;-<span class="st"> </span><span class="kw">train</span>(
  mpg <span class="op">~</span><span class="st"> </span>., 
  Train,
  <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>,
  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(
    <span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, 
    <span class="dt">number =</span> <span class="dv">5</span>,
    <span class="dt">verboseIter =</span> <span class="ot">TRUE</span>
  )
)</code></pre></div>
<pre><code>## + Fold1: intercept=TRUE 
## - Fold1: intercept=TRUE 
## + Fold2: intercept=TRUE 
## - Fold2: intercept=TRUE 
## + Fold3: intercept=TRUE 
## - Fold3: intercept=TRUE 
## + Fold4: intercept=TRUE 
## - Fold4: intercept=TRUE 
## + Fold5: intercept=TRUE 
## - Fold5: intercept=TRUE 
## Aggregating results
## Fitting final model on full training set</code></pre>
<p>So the object returned from caret gives us an estimate of how well the model will perform (based on RMSE) for out of sample data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">my_lm</code></pre></div>
<pre><code>## Linear Regression 
## 
## 28 samples
## 10 predictors
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 22, 23, 20, 23, 24 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   3.982511  0.7472726  3.417154
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">compute_rmse</span>(<span class="kw">predict</span>(my_lm,Test),Test<span class="op">$</span>mpg)</code></pre></div>
<pre><code>## [1] 3.605451</code></pre>
<p>We could also repeat the 5 times CV validation an arbitrary number of times to generate greater confidence in the RMSE estimates returned by the model. Remember, a major reason for using K Fold validation is to better estimate the out of sample error by holding out a portion of the data frame being trained upon.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">my_lm &lt;-<span class="st"> </span><span class="kw">train</span>(
  mpg <span class="op">~</span><span class="st"> </span>., 
  Train,
  <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>,
  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(
    <span class="dt">method =</span> <span class="st">&quot;repeatedcv&quot;</span>,    <span class="co"># Note &quot;rpeatedcv&quot;</span>
    <span class="dt">number =</span> <span class="dv">5</span>,
    <span class="dt">repeats =</span> <span class="dv">5</span>
  )
)

<span class="kw">compute_rmse</span>(<span class="kw">predict</span>(my_lm,Test),Test<span class="op">$</span>mpg)</code></pre></div>
<pre><code>## [1] 3.605451</code></pre>
<p>As a matter of convenience, you can create a variable to hold the content of <strong>trainControl</strong> and then pass that to the function. This is subjective and either approach is fine.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">my_ctrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(
    <span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, 
    <span class="dt">number =</span> <span class="dv">5</span>,
    <span class="dt">verboseIter =</span> <span class="ot">TRUE</span>
  )

my_lm &lt;-<span class="st"> </span><span class="kw">train</span>(
  mpg <span class="op">~</span><span class="st"> </span>., 
  Train,
  <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>,
  <span class="dt">trControl =</span> my_ctrl
)</code></pre></div>
<pre><code>## + Fold1: intercept=TRUE 
## - Fold1: intercept=TRUE 
## + Fold2: intercept=TRUE 
## - Fold2: intercept=TRUE 
## + Fold3: intercept=TRUE 
## - Fold3: intercept=TRUE 
## + Fold4: intercept=TRUE 
## - Fold4: intercept=TRUE 
## + Fold5: intercept=TRUE 
## - Fold5: intercept=TRUE 
## Aggregating results
## Fitting final model on full training set</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">compute_rmse</span>(<span class="kw">predict</span>(my_lm,Test),Test<span class="op">$</span>mpg)</code></pre></div>
<pre><code>## [1] 3.605451</code></pre>
</div>
<div id="one-size-fits-all" class="section level2">
<h2><span class="header-section-number">6.4</span> One Size Fits All</h2>
<p>So this is where things get interesting. If we wanted to use another method such as Random Forests, we do NOT have to change much at all. We just provide the name of the desired method which in this case, is <strong>rf</strong>. Of course, there might be parameters specific to the new method but we’ll hold off that for now.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">my_ctrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(
    <span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, 
    <span class="dt">number =</span> <span class="dv">5</span>,
    <span class="dt">verboseIter =</span> <span class="ot">TRUE</span>
  )

my_rf &lt;-<span class="st"> </span><span class="kw">train</span>(
  mpg <span class="op">~</span><span class="st"> </span>., 
  Train,
  <span class="dt">method =</span> <span class="st">&quot;rf&quot;</span>,
  <span class="dt">trControl =</span> my_ctrl
)</code></pre></div>
<pre><code>## + Fold1: mtry= 2 
## - Fold1: mtry= 2 
## + Fold1: mtry= 6 
## - Fold1: mtry= 6 
## + Fold1: mtry=10 
## - Fold1: mtry=10 
## + Fold2: mtry= 2 
## - Fold2: mtry= 2 
## + Fold2: mtry= 6 
## - Fold2: mtry= 6 
## + Fold2: mtry=10 
## - Fold2: mtry=10 
## + Fold3: mtry= 2 
## - Fold3: mtry= 2 
## + Fold3: mtry= 6 
## - Fold3: mtry= 6 
## + Fold3: mtry=10 
## - Fold3: mtry=10 
## + Fold4: mtry= 2 
## - Fold4: mtry= 2 
## + Fold4: mtry= 6 
## - Fold4: mtry= 6 
## + Fold4: mtry=10 
## - Fold4: mtry=10 
## + Fold5: mtry= 2 
## - Fold5: mtry= 2 
## + Fold5: mtry= 6 
## - Fold5: mtry= 6 
## + Fold5: mtry=10 
## - Fold5: mtry=10 
## Aggregating results
## Selecting tuning parameters
## Fitting mtry = 6 on full training set</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">my_rf</code></pre></div>
<pre><code>## Random Forest 
## 
## 28 samples
## 10 predictors
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 21, 23, 23, 22, 23 
## Resampling results across tuning parameters:
## 
##   mtry  RMSE      Rsquared   MAE     
##    2    2.544211  0.8960265  2.050086
##    6    2.483917  0.9165483  2.009985
##   10    2.509347  0.9079007  2.040340
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was mtry = 6.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">compute_rmse</span>(<span class="kw">predict</span>(my_rf,Test),Test<span class="op">$</span>mpg)</code></pre></div>
<pre><code>## [1] 2.767514</code></pre>
<p>We can get a plot of how the RMSE and R squared value varied with different values of <strong>mtry</strong>. Here we see that an mtry value of 6 randomly selected columns / variables provides the lowest RMSE.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(my_rf) <span class="op">+</span><span class="st"> </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;top&quot;</span>)</code></pre></div>
<p><img src="biosml_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>We can use the <strong>tuneLength</strong> argument to tell the <strong>train</strong> function to use N different values of <strong>mtry</strong> which is a <strong>hyperparameter</strong> to the randomForest package. The value relates to the number of columns in the data frame. We have 11 total and we are trying to predict one of them (mpg). So we can tell the <strong>train</strong> function to randomly select N variables (up to 10) when a tree is split.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">my_ctrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(
    <span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, 
    <span class="dt">number =</span> <span class="dv">5</span>,
    <span class="dt">verboseIter =</span> <span class="ot">TRUE</span>
  )

my_rf &lt;-<span class="st"> </span><span class="kw">train</span>(
  mpg <span class="op">~</span><span class="st"> </span>., 
  Train,
  <span class="dt">method =</span> <span class="st">&quot;rf&quot;</span>,
  <span class="dt">tuneLength =</span> <span class="dv">9</span>,    <span class="co"># We&#39;ll use 9 different values</span>
  <span class="dt">trControl =</span> my_ctrl
)</code></pre></div>
<pre><code>## + Fold1: mtry= 2 
## - Fold1: mtry= 2 
## + Fold1: mtry= 3 
## - Fold1: mtry= 3 
## + Fold1: mtry= 4 
## - Fold1: mtry= 4 
## + Fold1: mtry= 5 
## - Fold1: mtry= 5 
## + Fold1: mtry= 6 
## - Fold1: mtry= 6 
## + Fold1: mtry= 7 
## - Fold1: mtry= 7 
## + Fold1: mtry= 8 
## - Fold1: mtry= 8 
## + Fold1: mtry= 9 
## - Fold1: mtry= 9 
## + Fold1: mtry=10 
## - Fold1: mtry=10 
## + Fold2: mtry= 2 
## - Fold2: mtry= 2 
## + Fold2: mtry= 3 
## - Fold2: mtry= 3 
## + Fold2: mtry= 4 
## - Fold2: mtry= 4 
## + Fold2: mtry= 5 
## - Fold2: mtry= 5 
## + Fold2: mtry= 6 
## - Fold2: mtry= 6 
## + Fold2: mtry= 7 
## - Fold2: mtry= 7 
## + Fold2: mtry= 8 
## - Fold2: mtry= 8 
## + Fold2: mtry= 9 
## - Fold2: mtry= 9 
## + Fold2: mtry=10 
## - Fold2: mtry=10 
## + Fold3: mtry= 2 
## - Fold3: mtry= 2 
## + Fold3: mtry= 3 
## - Fold3: mtry= 3 
## + Fold3: mtry= 4 
## - Fold3: mtry= 4 
## + Fold3: mtry= 5 
## - Fold3: mtry= 5 
## + Fold3: mtry= 6 
## - Fold3: mtry= 6 
## + Fold3: mtry= 7 
## - Fold3: mtry= 7 
## + Fold3: mtry= 8 
## - Fold3: mtry= 8 
## + Fold3: mtry= 9 
## - Fold3: mtry= 9 
## + Fold3: mtry=10 
## - Fold3: mtry=10 
## + Fold4: mtry= 2 
## - Fold4: mtry= 2 
## + Fold4: mtry= 3 
## - Fold4: mtry= 3 
## + Fold4: mtry= 4 
## - Fold4: mtry= 4 
## + Fold4: mtry= 5 
## - Fold4: mtry= 5 
## + Fold4: mtry= 6 
## - Fold4: mtry= 6 
## + Fold4: mtry= 7 
## - Fold4: mtry= 7 
## + Fold4: mtry= 8 
## - Fold4: mtry= 8 
## + Fold4: mtry= 9 
## - Fold4: mtry= 9 
## + Fold4: mtry=10 
## - Fold4: mtry=10 
## + Fold5: mtry= 2 
## - Fold5: mtry= 2 
## + Fold5: mtry= 3 
## - Fold5: mtry= 3 
## + Fold5: mtry= 4 
## - Fold5: mtry= 4 
## + Fold5: mtry= 5 
## - Fold5: mtry= 5 
## + Fold5: mtry= 6 
## - Fold5: mtry= 6 
## + Fold5: mtry= 7 
## - Fold5: mtry= 7 
## + Fold5: mtry= 8 
## - Fold5: mtry= 8 
## + Fold5: mtry= 9 
## - Fold5: mtry= 9 
## + Fold5: mtry=10 
## - Fold5: mtry=10 
## Aggregating results
## Selecting tuning parameters
## Fitting mtry = 7 on full training set</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">my_rf</code></pre></div>
<pre><code>## Random Forest 
## 
## 28 samples
## 10 predictors
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 23, 22, 23, 22, 22 
## Resampling results across tuning parameters:
## 
##   mtry  RMSE      Rsquared   MAE     
##    2    2.330078  0.8652131  2.026366
##    3    2.293669  0.8674504  2.003557
##    4    2.263289  0.8751798  1.998438
##    5    2.246241  0.8787279  1.978190
##    6    2.212955  0.8811688  1.970637
##    7    2.209497  0.8826643  1.955071
##    8    2.214264  0.8847512  1.964696
##    9    2.269204  0.8751851  2.007807
##   10    2.253680  0.8792453  2.001920
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was mtry = 7.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(my_rf)</code></pre></div>
<p><img src="biosml_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">compute_rmse</span>(<span class="kw">predict</span>(my_rf,Test),Test<span class="op">$</span>mpg)</code></pre></div>
<pre><code>## [1] 2.813082</code></pre>
<p>If you have a questions about what hyper parameters can be tuned for a given method then you can refer to the online <a href="https://topepo.github.io/caret/available-models.html">caret documentation</a> Here is a screenshot of the table of supported models and associated tuning parameters.</p>
<div class="figure">
<img src="pics/avmodels.png" />

</div>
<p>Another way to do this within the caret package itself is that if you already know the abbreviation for the specific method you wish to use (e.g. “rf”) then you can use some built in functions to help you. Remember that <strong>caret</strong> does not replace or rewrite functions, it merely provides a nice wrapper around them. Since each underlying model is it a standalone R package and associated set of functions you can always call them directly.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">modelLookup</span>(<span class="st">&quot;rf&quot;</span>)</code></pre></div>
<pre><code>##   model parameter                         label forReg forClass probModel
## 1    rf      mtry #Randomly Selected Predictors   TRUE     TRUE      TRUE</code></pre>
<p>Here we get the hyper parameters for the <strong>ranger</strong> function. We see that it has three hyper parameters that could be varied in some way to influence a final model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">modelLookup</span>(<span class="st">&quot;ranger&quot;</span>)</code></pre></div>
<pre><code>##    model     parameter                         label forReg forClass
## 1 ranger          mtry #Randomly Selected Predictors   TRUE     TRUE
## 2 ranger     splitrule                Splitting Rule   TRUE     TRUE
## 3 ranger min.node.size             Minimal Node Size   TRUE     TRUE
##   probModel
## 1      TRUE
## 2      TRUE
## 3      TRUE</code></pre>
<p>If you just want a list of all the models supported by caret then do something like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">models &lt;-<span class="st"> </span><span class="kw">modelLookup</span>()[,<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>]
<span class="kw">nrow</span>(models)</code></pre></div>
<pre><code>## [1] 502</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Search for a Model</span>
models[models<span class="op">$</span>model<span class="op">==</span><span class="st">&quot;rf&quot;</span>,]</code></pre></div>
<pre><code>##     model parameter                         label
## 365    rf      mtry #Randomly Selected Predictors</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">models[models<span class="op">$</span>model<span class="op">==</span><span class="st">&quot;ranger&quot;</span>,]</code></pre></div>
<pre><code>##      model     parameter                         label
## 351 ranger          mtry #Randomly Selected Predictors
## 352 ranger     splitrule                Splitting Rule
## 353 ranger min.node.size             Minimal Node Size</code></pre>
<p>So in the case of the <strong>ranger</strong> function there are actually three hyper parameters that could be tuned.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">my_ctrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(
    <span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, 
    <span class="dt">number =</span> <span class="dv">3</span>,
    <span class="dt">verboseIter =</span> <span class="ot">FALSE</span> 
  )

my_ranger &lt;-<span class="st"> </span><span class="kw">train</span>(
  mpg <span class="op">~</span><span class="st"> </span>., 
  Train,
  <span class="dt">method =</span> <span class="st">&quot;ranger&quot;</span>,
  <span class="dt">tuneLength =</span> <span class="dv">6</span>,    
  <span class="dt">trControl =</span> my_ctrl
)

my_ranger</code></pre></div>
<pre><code>## Random Forest 
## 
## 28 samples
## 10 predictors
## 
## No pre-processing
## Resampling: Cross-Validated (3 fold) 
## Summary of sample sizes: 20, 19, 17 
## Resampling results across tuning parameters:
## 
##   mtry  splitrule   RMSE      Rsquared   MAE     
##    2    variance    2.757550  0.8074667  2.297045
##    2    extratrees  2.726511  0.8237626  2.151578
##    3    variance    2.656569  0.8185486  2.189522
##    3    extratrees  2.634597  0.8364472  2.178665
##    5    variance    2.628752  0.8221877  2.204136
##    5    extratrees  2.595241  0.8317185  2.142563
##    6    variance    2.613666  0.8176006  2.178122
##    6    extratrees  2.568329  0.8350533  2.125458
##    8    variance    2.613480  0.8208897  2.179424
##    8    extratrees  2.429722  0.8525664  2.020205
##   10    variance    2.691867  0.8056569  2.195034
##   10    extratrees  2.500379  0.8492944  2.087951
## 
## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 5
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were mtry = 8, splitrule =
##  extratrees and min.node.size = 5.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">compute_rmse</span>(<span class="kw">predict</span>(my_ranger,Test),Test<span class="op">$</span>mpg)</code></pre></div>
<pre><code>## [1] 2.993649</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(my_ranger)</code></pre></div>
<p><img src="biosml_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="using-methods-other-than-lm.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="picking-the-best-model.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
