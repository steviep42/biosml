<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Caret Package | Predictive Learning in R</title>
  <meta name="description" content="Chapter 5 Caret Package | Predictive Learning in R" />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Caret Package | Predictive Learning in R" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Caret Package | Predictive Learning in R" />
  
  
  

<meta name="author" content="Steve Pittard - wsp@emory.edu" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="training-test-data.html"/>
<link rel="next" href="classification-problems.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#machine-learning"><i class="fa fa-check"></i><b>1.1</b> Machine Learning</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#predictive-modeling"><i class="fa fa-check"></i><b>1.2</b> Predictive Modeling</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#in-sample-vs-out-of-sample-error"><i class="fa fa-check"></i><b>1.3</b> In-Sample vs Out-Of-Sample Error</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#performance-metrics"><i class="fa fa-check"></i><b>1.4</b> Performance Metrics</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#black-box"><i class="fa fa-check"></i><b>1.5</b> Black Box</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html"><i class="fa fa-check"></i><b>2</b> Predictive / Supervised Learning</a><ul>
<li class="chapter" data-level="2.1" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#explanation-vs-prediction"><i class="fa fa-check"></i><b>2.1</b> Explanation vs Prediction</a><ul>
<li class="chapter" data-level="2.1.1" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#titanic-data"><i class="fa fa-check"></i><b>2.1.1</b> Titanic Data</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#bias-vs-variance"><i class="fa fa-check"></i><b>2.2</b> Bias vs Variance</a><ul>
<li class="chapter" data-level="2.2.1" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#bias"><i class="fa fa-check"></i><b>2.2.1</b> Bias</a></li>
<li class="chapter" data-level="2.2.2" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#variance"><i class="fa fa-check"></i><b>2.2.2</b> Variance</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#overfitting-and-underfitting"><i class="fa fa-check"></i><b>2.3</b> Overfitting and Underfitting</a></li>
<li class="chapter" data-level="2.4" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#some-important-terminology"><i class="fa fa-check"></i><b>2.4</b> Some Important Terminology</a></li>
<li class="chapter" data-level="2.5" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#levels-of-measurement"><i class="fa fa-check"></i><b>2.5</b> Levels of Measurement</a><ul>
<li class="chapter" data-level="2.5.1" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#nominal"><i class="fa fa-check"></i><b>2.5.1</b> Nominal</a></li>
<li class="chapter" data-level="2.5.2" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#ordinal"><i class="fa fa-check"></i><b>2.5.2</b> Ordinal</a></li>
<li class="chapter" data-level="2.5.3" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#interval"><i class="fa fa-check"></i><b>2.5.3</b> Interval</a></li>
<li class="chapter" data-level="2.5.4" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#ratio"><i class="fa fa-check"></i><b>2.5.4</b> Ratio</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="a-motivating-example.html"><a href="a-motivating-example.html"><i class="fa fa-check"></i><b>3</b> A Motivating Example</a><ul>
<li class="chapter" data-level="3.1" data-path="a-motivating-example.html"><a href="a-motivating-example.html#a-more-detailed-workflow"><i class="fa fa-check"></i><b>3.1</b> A More Detailed Workflow</a></li>
<li class="chapter" data-level="3.2" data-path="a-motivating-example.html"><a href="a-motivating-example.html#visualizations"><i class="fa fa-check"></i><b>3.2</b> Visualizations</a><ul>
<li class="chapter" data-level="3.2.1" data-path="a-motivating-example.html"><a href="a-motivating-example.html#scatterplots"><i class="fa fa-check"></i><b>3.2.1</b> Scatterplots</a></li>
<li class="chapter" data-level="3.2.2" data-path="a-motivating-example.html"><a href="a-motivating-example.html#boxplots"><i class="fa fa-check"></i><b>3.2.2</b> Boxplots</a></li>
<li class="chapter" data-level="3.2.3" data-path="a-motivating-example.html"><a href="a-motivating-example.html#histograms"><i class="fa fa-check"></i><b>3.2.3</b> Histograms</a></li>
<li class="chapter" data-level="3.2.4" data-path="a-motivating-example.html"><a href="a-motivating-example.html#tables"><i class="fa fa-check"></i><b>3.2.4</b> Tables</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="a-motivating-example.html"><a href="a-motivating-example.html#correlations"><i class="fa fa-check"></i><b>3.3</b> Correlations</a></li>
<li class="chapter" data-level="3.4" data-path="a-motivating-example.html"><a href="a-motivating-example.html#building-a-model---in-sample-error"><i class="fa fa-check"></i><b>3.4</b> Building A Model - In Sample Error</a></li>
<li class="chapter" data-level="3.5" data-path="a-motivating-example.html"><a href="a-motivating-example.html#out-of-sample-data"><i class="fa fa-check"></i><b>3.5</b> Out Of Sample Data</a></li>
<li class="chapter" data-level="3.6" data-path="a-motivating-example.html"><a href="a-motivating-example.html#some-additional-considerations"><i class="fa fa-check"></i><b>3.6</b> Some Additional Considerations</a></li>
<li class="chapter" data-level="3.7" data-path="a-motivating-example.html"><a href="a-motivating-example.html#other-methods"><i class="fa fa-check"></i><b>3.7</b> Other Methods ?</a></li>
<li class="chapter" data-level="3.8" data-path="a-motivating-example.html"><a href="a-motivating-example.html#summary"><i class="fa fa-check"></i><b>3.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="training-test-data.html"><a href="training-test-data.html"><i class="fa fa-check"></i><b>4</b> Training / Test Data</a><ul>
<li class="chapter" data-level="4.1" data-path="training-test-data.html"><a href="training-test-data.html#cross-fold-validation"><i class="fa fa-check"></i><b>4.1</b> Cross Fold Validation</a></li>
<li class="chapter" data-level="4.2" data-path="training-test-data.html"><a href="training-test-data.html#create-a-function-to-automate-things"><i class="fa fa-check"></i><b>4.2</b> Create A Function To Automate Things</a></li>
<li class="chapter" data-level="4.3" data-path="training-test-data.html"><a href="training-test-data.html#repeated-cross-validation"><i class="fa fa-check"></i><b>4.3</b> Repeated Cross Validation</a></li>
<li class="chapter" data-level="4.4" data-path="training-test-data.html"><a href="training-test-data.html#bootstrap"><i class="fa fa-check"></i><b>4.4</b> Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="caret-package.html"><a href="caret-package.html"><i class="fa fa-check"></i><b>5</b> Caret Package</a><ul>
<li class="chapter" data-level="5.1" data-path="caret-package.html"><a href="caret-package.html#putting-caret-to-work"><i class="fa fa-check"></i><b>5.1</b> Putting caret To Work</a></li>
<li class="chapter" data-level="5.2" data-path="caret-package.html"><a href="caret-package.html#back-to-the-beginning"><i class="fa fa-check"></i><b>5.2</b> Back To The Beginning</a></li>
<li class="chapter" data-level="5.3" data-path="caret-package.html"><a href="caret-package.html#splitting"><i class="fa fa-check"></i><b>5.3</b> Splitting</a></li>
<li class="chapter" data-level="5.4" data-path="caret-package.html"><a href="caret-package.html#calling-the-train-function"><i class="fa fa-check"></i><b>5.4</b> Calling The train() Function</a></li>
<li class="chapter" data-level="5.5" data-path="caret-package.html"><a href="caret-package.html#reproducible-results"><i class="fa fa-check"></i><b>5.5</b> Reproducible Results</a></li>
<li class="chapter" data-level="5.6" data-path="caret-package.html"><a href="caret-package.html#one-size-fits-all"><i class="fa fa-check"></i><b>5.6</b> One Size Fits All</a></li>
<li class="chapter" data-level="5.7" data-path="caret-package.html"><a href="caret-package.html#alternative-calling-sequence"><i class="fa fa-check"></i><b>5.7</b> Alternative Calling Sequence</a></li>
<li class="chapter" data-level="5.8" data-path="caret-package.html"><a href="caret-package.html#hyperparameters"><i class="fa fa-check"></i><b>5.8</b> Hyperparameters</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="classification-problems.html"><a href="classification-problems.html"><i class="fa fa-check"></i><b>6</b> Classification Problems</a><ul>
<li class="chapter" data-level="6.1" data-path="classification-problems.html"><a href="classification-problems.html#performance-measures"><i class="fa fa-check"></i><b>6.1</b> Performance Measures</a></li>
<li class="chapter" data-level="6.2" data-path="classification-problems.html"><a href="classification-problems.html#important-terminology"><i class="fa fa-check"></i><b>6.2</b> Important Terminology</a></li>
<li class="chapter" data-level="6.3" data-path="classification-problems.html"><a href="classification-problems.html#a-basic-model"><i class="fa fa-check"></i><b>6.3</b> A Basic Model</a></li>
<li class="chapter" data-level="6.4" data-path="classification-problems.html"><a href="classification-problems.html#selecting-the-correct-threshold-alpha"><i class="fa fa-check"></i><b>6.4</b> Selecting The Correct Threshold / Alpha</a><ul>
<li class="chapter" data-level="6.4.1" data-path="classification-problems.html"><a href="classification-problems.html#moving-the-threshold"><i class="fa fa-check"></i><b>6.4.1</b> Moving The Threshold</a></li>
<li class="chapter" data-level="6.4.2" data-path="classification-problems.html"><a href="classification-problems.html#distribution-of-predicted-probabilities"><i class="fa fa-check"></i><b>6.4.2</b> Distribution of Predicted Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="classification-problems.html"><a href="classification-problems.html#hypothesis-testing"><i class="fa fa-check"></i><b>6.5</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="6.6" data-path="classification-problems.html"><a href="classification-problems.html#confusion-matrix"><i class="fa fa-check"></i><b>6.6</b> Confusion Matrix</a><ul>
<li class="chapter" data-level="6.6.1" data-path="classification-problems.html"><a href="classification-problems.html#computing-performance-metrics"><i class="fa fa-check"></i><b>6.6.1</b> Computing Performance Metrics</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="classification-problems.html"><a href="classification-problems.html#picking-the-right-metric"><i class="fa fa-check"></i><b>6.7</b> Picking the Right Metric</a></li>
<li class="chapter" data-level="6.8" data-path="classification-problems.html"><a href="classification-problems.html#wait.-where-are-we"><i class="fa fa-check"></i><b>6.8</b> Wait. Where Are We ?</a></li>
<li class="chapter" data-level="6.9" data-path="classification-problems.html"><a href="classification-problems.html#better-ways-to-compute-the-roc-curve"><i class="fa fa-check"></i><b>6.9</b> Better Ways To Compute The ROC Curve</a></li>
<li class="chapter" data-level="6.10" data-path="classification-problems.html"><a href="classification-problems.html#roc-curve-summary"><i class="fa fa-check"></i><b>6.10</b> ROC Curve Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="classification-example.html"><a href="classification-example.html"><i class="fa fa-check"></i><b>7</b> Classification Example</a><ul>
<li class="chapter" data-level="7.1" data-path="classification-example.html"><a href="classification-example.html#exploratory-plots"><i class="fa fa-check"></i><b>7.1</b> Exploratory Plots</a></li>
<li class="chapter" data-level="7.2" data-path="classification-example.html"><a href="classification-example.html#generalized-linear-models"><i class="fa fa-check"></i><b>7.2</b> Generalized Linear Models</a></li>
<li class="chapter" data-level="7.3" data-path="classification-example.html"><a href="classification-example.html#random-forests"><i class="fa fa-check"></i><b>7.3</b> Random Forests</a></li>
<li class="chapter" data-level="7.4" data-path="classification-example.html"><a href="classification-example.html#target-variable-format"><i class="fa fa-check"></i><b>7.4</b> Target Variable Format</a></li>
<li class="chapter" data-level="7.5" data-path="classification-example.html"><a href="classification-example.html#addressing-class-imbalance"><i class="fa fa-check"></i><b>7.5</b> Addressing Class Imbalance</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>8</b> Decision Trees</a><ul>
<li class="chapter" data-level="8.1" data-path="decision-trees.html"><a href="decision-trees.html#advantages"><i class="fa fa-check"></i><b>8.1</b> Advantages</a></li>
<li class="chapter" data-level="8.2" data-path="decision-trees.html"><a href="decision-trees.html#a-classification-example"><i class="fa fa-check"></i><b>8.2</b> A Classification Example</a></li>
<li class="chapter" data-level="8.3" data-path="decision-trees.html"><a href="decision-trees.html#digging-deeper"><i class="fa fa-check"></i><b>8.3</b> Digging Deeper</a><ul>
<li class="chapter" data-level="8.3.1" data-path="decision-trees.html"><a href="decision-trees.html#evaluating-performance"><i class="fa fa-check"></i><b>8.3.1</b> Evaluating performance</a></li>
<li class="chapter" data-level="8.3.2" data-path="decision-trees.html"><a href="decision-trees.html#tree-splitting"><i class="fa fa-check"></i><b>8.3.2</b> Tree Splitting</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="decision-trees.html"><a href="decision-trees.html#gini-index"><i class="fa fa-check"></i><b>8.4</b> Gini Index</a></li>
<li class="chapter" data-level="8.5" data-path="decision-trees.html"><a href="decision-trees.html#regression-trees"><i class="fa fa-check"></i><b>8.5</b> Regression Trees</a><ul>
<li class="chapter" data-level="8.5.1" data-path="decision-trees.html"><a href="decision-trees.html#performance-measure"><i class="fa fa-check"></i><b>8.5.1</b> Performance Measure</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="decision-trees.html"><a href="decision-trees.html#parameters-vs-hyperparameters"><i class="fa fa-check"></i><b>8.6</b> Parameters vs Hyperparameters</a></li>
<li class="chapter" data-level="8.7" data-path="decision-trees.html"><a href="decision-trees.html#grid-searching"><i class="fa fa-check"></i><b>8.7</b> Grid Searching</a></li>
<li class="chapter" data-level="8.8" data-path="decision-trees.html"><a href="decision-trees.html#bagged-trees"><i class="fa fa-check"></i><b>8.8</b> Bagged Trees</a></li>
<li class="chapter" data-level="8.9" data-path="decision-trees.html"><a href="decision-trees.html#random-forests-1"><i class="fa fa-check"></i><b>8.9</b> Random Forests</a></li>
<li class="chapter" data-level="8.10" data-path="decision-trees.html"><a href="decision-trees.html#boosted-trees"><i class="fa fa-check"></i><b>8.10</b> Boosted Trees</a></li>
<li class="chapter" data-level="8.11" data-path="decision-trees.html"><a href="decision-trees.html#using-caret"><i class="fa fa-check"></i><b>8.11</b> Using caret</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="using-methods-other-than-lm.html"><a href="using-methods-other-than-lm.html"><i class="fa fa-check"></i><b>9</b> Using Methods Other Than lm</a><ul>
<li class="chapter" data-level="9.1" data-path="using-methods-other-than-lm.html"><a href="using-methods-other-than-lm.html#parameters-vs-hyperparameters-1"><i class="fa fa-check"></i><b>9.1</b> Parameters vs Hyperparameters</a></li>
<li class="chapter" data-level="9.2" data-path="using-methods-other-than-lm.html"><a href="using-methods-other-than-lm.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>9.2</b> Hyperparameter Tuning</a><ul>
<li class="chapter" data-level="9.2.1" data-path="using-methods-other-than-lm.html"><a href="using-methods-other-than-lm.html#multiple-hyperparameters"><i class="fa fa-check"></i><b>9.2.1</b> Multiple Hyperparameters ?</a></li>
<li class="chapter" data-level="9.2.2" data-path="using-methods-other-than-lm.html"><a href="using-methods-other-than-lm.html#custom-tuning-grid"><i class="fa fa-check"></i><b>9.2.2</b> Custom Tuning Grid</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="using-methods-other-than-lm.html"><a href="using-methods-other-than-lm.html#using-validation-data-sets"><i class="fa fa-check"></i><b>9.3</b> Using Validation Data Sets</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html"><i class="fa fa-check"></i><b>10</b> Picking The Best Model</a><ul>
<li class="chapter" data-level="10.1" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#an-example"><i class="fa fa-check"></i><b>10.1</b> An Example</a></li>
<li class="chapter" data-level="10.2" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#more-comparisons"><i class="fa fa-check"></i><b>10.2</b> More Comparisons</a></li>
<li class="chapter" data-level="10.3" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#using-the-resamples-function"><i class="fa fa-check"></i><b>10.3</b> Using the resamples() function</a></li>
<li class="chapter" data-level="10.4" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#model-performance"><i class="fa fa-check"></i><b>10.4</b> Model Performance</a></li>
<li class="chapter" data-level="10.5" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#feature-evaluation"><i class="fa fa-check"></i><b>10.5</b> Feature Evaluation</a><ul>
<li class="chapter" data-level="10.5.1" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#identifying-redundant-features"><i class="fa fa-check"></i><b>10.5.1</b> Identifying Redundant Features</a></li>
<li class="chapter" data-level="10.5.2" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#highly-correlated-variables"><i class="fa fa-check"></i><b>10.5.2</b> Highly Correlated Variables</a></li>
<li class="chapter" data-level="10.5.3" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#ranking-features"><i class="fa fa-check"></i><b>10.5.3</b> Ranking Features</a></li>
<li class="chapter" data-level="10.5.4" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#feature-selection"><i class="fa fa-check"></i><b>10.5.4</b> Feature Selection</a></li>
<li class="chapter" data-level="10.5.5" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#recursive-feature-elimination"><i class="fa fa-check"></i><b>10.5.5</b> Recursive Feature Elimination</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="data-pre-processing.html"><a href="data-pre-processing.html"><i class="fa fa-check"></i><b>11</b> Data Pre Processing</a><ul>
<li class="chapter" data-level="11.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#types-of-pre-processing"><i class="fa fa-check"></i><b>11.1</b> Types of Pre Processing</a></li>
<li class="chapter" data-level="11.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#missing-values"><i class="fa fa-check"></i><b>11.2</b> Missing Values</a><ul>
<li class="chapter" data-level="11.2.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#finding-rows-with-missing-data"><i class="fa fa-check"></i><b>11.2.1</b> Finding Rows with Missing Data</a></li>
<li class="chapter" data-level="11.2.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#finding-columns-with-missing-data"><i class="fa fa-check"></i><b>11.2.2</b> Finding Columns With Missing Data</a></li>
<li class="chapter" data-level="11.2.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#use-the-median-approach"><i class="fa fa-check"></i><b>11.2.3</b> Use the Median Approach</a></li>
<li class="chapter" data-level="11.2.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#package-based-approach"><i class="fa fa-check"></i><b>11.2.4</b> Package-based Approach</a></li>
<li class="chapter" data-level="11.2.5" data-path="data-pre-processing.html"><a href="data-pre-processing.html#using-caret-1"><i class="fa fa-check"></i><b>11.2.5</b> Using caret</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#scaling"><i class="fa fa-check"></i><b>11.3</b> Scaling</a><ul>
<li class="chapter" data-level="11.3.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#methods-that-benefit-from-scaling"><i class="fa fa-check"></i><b>11.3.1</b> Methods That Benefit From Scaling</a></li>
<li class="chapter" data-level="11.3.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#methods-that-do-not-require-scaling"><i class="fa fa-check"></i><b>11.3.2</b> Methods That Do Not Require Scaling</a></li>
<li class="chapter" data-level="11.3.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#how-to-scale"><i class="fa fa-check"></i><b>11.3.3</b> How To Scale</a></li>
<li class="chapter" data-level="11.3.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#order-of-processing"><i class="fa fa-check"></i><b>11.3.4</b> Order of Processing</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#low-variance-variables"><i class="fa fa-check"></i><b>11.4</b> Low Variance Variables</a></li>
<li class="chapter" data-level="11.5" data-path="data-pre-processing.html"><a href="data-pre-processing.html#pca---principal-components-analysis"><i class="fa fa-check"></i><b>11.5</b> PCA - Principal Components Analysis</a><ul>
<li class="chapter" data-level="11.5.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#identify-the-factors"><i class="fa fa-check"></i><b>11.5.1</b> Identify The Factors</a></li>
<li class="chapter" data-level="11.5.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#check-for-high-correlations"><i class="fa fa-check"></i><b>11.5.2</b> Check For High Correlations</a></li>
<li class="chapter" data-level="11.5.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#so-why-use-pca"><i class="fa fa-check"></i><b>11.5.3</b> So Why Use PCA ?</a></li>
<li class="chapter" data-level="11.5.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#check-the-biplot"><i class="fa fa-check"></i><b>11.5.4</b> Check The BiPlot</a></li>
<li class="chapter" data-level="11.5.5" data-path="data-pre-processing.html"><a href="data-pre-processing.html#check-the-screeplot"><i class="fa fa-check"></i><b>11.5.5</b> Check The ScreePlot</a></li>
<li class="chapter" data-level="11.5.6" data-path="data-pre-processing.html"><a href="data-pre-processing.html#use-the-transformed-data"><i class="fa fa-check"></i><b>11.5.6</b> Use The Transformed Data</a></li>
<li class="chapter" data-level="11.5.7" data-path="data-pre-processing.html"><a href="data-pre-processing.html#pls"><i class="fa fa-check"></i><b>11.5.7</b> PLS</a></li>
<li class="chapter" data-level="11.5.8" data-path="data-pre-processing.html"><a href="data-pre-processing.html#summary-1"><i class="fa fa-check"></i><b>11.5.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="data-pre-processing.html"><a href="data-pre-processing.html#order-of-pre-processing"><i class="fa fa-check"></i><b>11.6</b> Order of Pre-Processing</a></li>
<li class="chapter" data-level="11.7" data-path="data-pre-processing.html"><a href="data-pre-processing.html#handling-categories"><i class="fa fa-check"></i><b>11.7</b> Handling Categories</a><ul>
<li class="chapter" data-level="11.7.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#examples"><i class="fa fa-check"></i><b>11.7.1</b> Examples</a></li>
<li class="chapter" data-level="11.7.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#admissions-data"><i class="fa fa-check"></i><b>11.7.2</b> Admissions Data</a></li>
<li class="chapter" data-level="11.7.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#is-rank-a-category"><i class="fa fa-check"></i><b>11.7.3</b> Is Rank A Category ?</a></li>
<li class="chapter" data-level="11.7.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#relationship-to-one-hot-encoding"><i class="fa fa-check"></i><b>11.7.4</b> Relationship To One Hot Encoding</a></li>
</ul></li>
<li class="chapter" data-level="11.8" data-path="data-pre-processing.html"><a href="data-pre-processing.html#binning"><i class="fa fa-check"></i><b>11.8</b> Binning</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html"><i class="fa fa-check"></i><b>12</b> Using External ML Frameworks</a><ul>
<li class="chapter" data-level="12.1" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#using-h2o"><i class="fa fa-check"></i><b>12.1</b> Using h2o</a></li>
<li class="chapter" data-level="12.2" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#create-some-h20-models"><i class="fa fa-check"></i><b>12.2</b> Create Some h20 Models</a></li>
<li class="chapter" data-level="12.3" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#saving-a-model"><i class="fa fa-check"></i><b>12.3</b> Saving A Model</a></li>
<li class="chapter" data-level="12.4" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#using-the-h2o-auto-ml-feature"><i class="fa fa-check"></i><b>12.4</b> Using The h2o Auto ML Feature</a></li>
<li class="chapter" data-level="12.5" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#launching-a-job"><i class="fa fa-check"></i><b>12.5</b> Launching a Job</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Predictive Learning in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="caret-package" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Caret Package</h1>
<p>By now you are probably fatigued with understanding the details of writing the code to split data, doing Cross Validation, storing the results, and looking at descriptive stats associated with the resulting RMSE. And this is all before considering the various parameters associated with whatever method we wish to implement.</p>
<p>Each function has its own set of requirements which may not extend to other functions. What we need (well, what we would like) is a framework to streamline this process and automate it as much as possible but not at the expense of understanding the results.</p>
<p>The <a href="https://topepo.github.io/caret/index.html"><strong>caret</strong></a> (<strong>C</strong>lassification <strong>A</strong>nd <strong>R</strong>egression <strong>T</strong>raining) package provides a uniform interface for calling different algorithms while simplifying the data splitting and RMSE calculation. It supports many different model types and also provides the ability to tune hyper parameters. Here are some of the features:</p>
<ul>
<li>Streamlined and consistent syntax to implement any of the 238 different methods using a single function</li>
<li>Easy data splitting to simplify the creation of train / test pairs</li>
<li>Realistic model estimates through built-in resampling</li>
<li>Convenient feature importance determination</li>
<li>Easy selection of different performance metrics (e.g. “ROC”,“Accuracy”, “Sensitivity”)</li>
<li>Automated and semi-automated parameter tuning</li>
<li>Simplified comparison of different models</li>
</ul>
<p>Note that <strong>caret</strong> provides a nice wrapper around the various modeling functions. Since each underlying model is itself a standalone R package and associated set of functions you can always call them directly should you prefer that approach. That’s what we have been doing in the earlier part of this text.</p>
<div id="putting-caret-to-work" class="section level2">
<h2><span class="header-section-number">5.1</span> Putting caret To Work</h2>
<p>It’s easy to get lost in all that we have been doing so let’s review what the typical predictive modeling workflow will look like:</p>
<ol style="list-style-type: decimal">
<li>Data Import (.csv., extraction from a database, etc)</li>
<li>Some Data Visualization</li>
<li><p>Data Prep (We haven’t done any of this yet)
- Missing, imputation
- Scaling
- Create dummy variables / one hot encoding
- Dimensionality Reduction</p></li>
<li><p>Data Splitting (training / test)
- Determine split ration
- K-Fold Cross Validation (repeated)</p></li>
<li><p>Modeling / Prediction</p></li>
<li><p>Evaluation</p></li>
</ol>
<p>To do step 5 requires some predefined idea of a performance metric. We have been using RMSE and will continue to do so as we rework some of the previous examples using the caret package.</p>
</div>
<div id="back-to-the-beginning" class="section level2">
<h2><span class="header-section-number">5.2</span> Back To The Beginning</h2>
<p>It is implied that in predictive modeling the ultimate goal is to generate a model that could be reasonably applied to new data. As we have learned, it is best to train any model on a data set that has been (re)sampled in some way (e.g. K Fold CV) which should help provide a more realistic estimate of “out of sample” error.</p>
<p>In our earliest example we tried to predict the MPG from mtcars using a basic linear modeling function. The caret package provides a uniform way to do this which allows us to easily substitute in alternative functions without having to majorly change our code.</p>
<p>We can call the <strong>train</strong> function in such a way as to pass in any arguments that are specific to a given method though in a way we could do for other methods. We can also tell the <strong>train</strong> function that we want to evaluate RMSE as a performance measure. That is, it will “know” that our primary performance measure for a model is RMSE. Before we do that, however, we’ll make a test / train pair. The <strong>caret</strong> package provides ways to do that.</p>
</div>
<div id="splitting" class="section level2">
<h2><span class="header-section-number">5.3</span> Splitting</h2>
<p><strong>createDataPartition</strong> can be used to create test and train data splits according to some proportion. There is a function called <strong>createFolds</strong> can be used to generate balanced cross–validation groupings from a set of data. <strong>createResample</strong> can be used to make simple bootstrap samples. For now, we’ll just stick with <strong>createDataPartition</strong> for creating a test/train pair.</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb111-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">123</span>) <span class="co"># Make this example reproducible</span></a>
<a class="sourceLine" id="cb111-2" data-line-number="2">idx &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(mtcars<span class="op">$</span>mpg, <span class="dt">p =</span> <span class="fl">.8</span>, </a>
<a class="sourceLine" id="cb111-3" data-line-number="3">                                  <span class="dt">list =</span> <span class="ot">FALSE</span>, </a>
<a class="sourceLine" id="cb111-4" data-line-number="4">                                  <span class="dt">times =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb111-5" data-line-number="5"><span class="kw">head</span>(idx)</a></code></pre></div>
<pre><code>##      Resample1
## [1,]         1
## [2,]         3
## [3,]         4
## [4,]         5
## [5,]         6
## [6,]         8</code></pre>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb113-1" data-line-number="1">Train &lt;-<span class="st"> </span>mtcars[ idx,]</a>
<a class="sourceLine" id="cb113-2" data-line-number="2">Test  &lt;-<span class="st"> </span>mtcars[<span class="op">-</span>idx,]</a>
<a class="sourceLine" id="cb113-3" data-line-number="3"></a>
<a class="sourceLine" id="cb113-4" data-line-number="4"><span class="co">#</span></a>
<a class="sourceLine" id="cb113-5" data-line-number="5"><span class="kw">nrow</span>(Train)</a></code></pre></div>
<pre><code>## [1] 28</code></pre>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb115-1" data-line-number="1"><span class="kw">nrow</span>(Test)</a></code></pre></div>
<pre><code>## [1] 4</code></pre>
</div>
<div id="calling-the-train-function" class="section level2">
<h2><span class="header-section-number">5.4</span> Calling The train() Function</h2>
<p>To actually create a model involves use of the <strong>train</strong> function which is the premier function in the <strong>caret</strong> package. It does what it name suggests - train models. Note that we tell it:</p>
<ol style="list-style-type: decimal">
<li>What we are trying to predict (a formula)</li>
<li>What our data set is (e.g. Train)</li>
<li>The desired method (“lm”)
<ul>
<li>Note that this method name MUST match an existing R modeling function</li>
</ul></li>
<li>A desired scoring metric. In this case we seek to minimize RMSE on
future predictions</li>
</ol>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb117-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">123</span>) <span class="co"># Make this example reproducible</span></a>
<a class="sourceLine" id="cb117-2" data-line-number="2">lm_fit &lt;-<span class="st"> </span><span class="kw">train</span>(mpg<span class="op">~</span>wt,</a>
<a class="sourceLine" id="cb117-3" data-line-number="3">                <span class="dt">data=</span>Train,</a>
<a class="sourceLine" id="cb117-4" data-line-number="4">                <span class="dt">method=</span><span class="st">&quot;lm&quot;</span>,</a>
<a class="sourceLine" id="cb117-5" data-line-number="5">                <span class="dt">metric=</span><span class="st">&quot;RMSE&quot;</span>)</a></code></pre></div>
<p>We get back a single object that contains a lot of information that could help us figure out if the model is worth anything. But first, just type the name of fit object to see what you can see. This shows us information that has been derived from some re sampling activity across a number of bootstrapped samples.</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb118-1" data-line-number="1">lm_fit</a></code></pre></div>
<pre><code>## Linear Regression 
## 
## 28 samples
##  1 predictor
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 28, 28, 28, 28, 28, 28, ... 
## Resampling results:
## 
##   RMSE      Rsquared  MAE     
##   2.676533  0.79883   2.138882
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb120-1" data-line-number="1"><span class="kw">summary</span>(lm_fit)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = .outcome ~ ., data = dat)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -3.890 -2.163 -0.091  1.361  7.140 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  36.2505     1.7925  20.223  &lt; 2e-16 ***
## wt           -4.9957     0.5249  -9.516 5.89e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.75 on 26 degrees of freedom
## Multiple R-squared:  0.7769, Adjusted R-squared:  0.7684 
## F-statistic: 90.56 on 1 and 26 DF,  p-value: 5.889e-10</code></pre>
<p>Note that the summary of the model “summary(lm_fit)” returns the same information that would be returned had we used the <strong>lm</strong> function directly as we did in the previous section. The point is that the <strong>train</strong> function doesn’t not seek to replace or obscure the resulting model in any way. We can always get whatever information we need from it. So let’s apply this model to the test data frame</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb122-1" data-line-number="1">Metrics<span class="op">::</span><span class="kw">rmse</span>(Test<span class="op">$</span>mpg,<span class="kw">predict</span>(lm_fit,Test))</a></code></pre></div>
<pre><code>## [1] 4.622936</code></pre>
<p>There is more here than meets the eye.</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb124-1" data-line-number="1"><span class="kw">names</span>(lm_fit)</a></code></pre></div>
<pre><code>##  [1] &quot;method&quot;       &quot;modelInfo&quot;    &quot;modelType&quot;    &quot;results&quot;     
##  [5] &quot;pred&quot;         &quot;bestTune&quot;     &quot;call&quot;         &quot;dots&quot;        
##  [9] &quot;metric&quot;       &quot;control&quot;      &quot;finalModel&quot;   &quot;preProcess&quot;  
## [13] &quot;trainingData&quot; &quot;resample&quot;     &quot;resampledCM&quot;  &quot;perfNames&quot;   
## [17] &quot;maximize&quot;     &quot;yLimits&quot;      &quot;times&quot;        &quot;levels&quot;      
## [21] &quot;terms&quot;        &quot;coefnames&quot;    &quot;xlevels&quot;</code></pre>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb126-1" data-line-number="1"><span class="kw">str</span>(lm_fit<span class="op">$</span>control,<span class="dv">1</span>)</a></code></pre></div>
<pre><code>## List of 28
##  $ method           : chr &quot;boot&quot;
##  $ number           : num 25
##  $ repeats          : logi NA
##  $ search           : chr &quot;grid&quot;
##  $ p                : num 0.75
##  $ initialWindow    : NULL
##  $ horizon          : num 1
##  $ fixedWindow      : logi TRUE
##  $ skip             : num 0
##  $ verboseIter      : logi FALSE
##  $ returnData       : logi TRUE
##  $ returnResamp     : chr &quot;final&quot;
##  $ savePredictions  : chr &quot;none&quot;
##  $ classProbs       : logi FALSE
##  $ summaryFunction  :function (data, lev = NULL, model = NULL)  
##  $ selectionFunction: chr &quot;best&quot;
##  $ preProcOptions   :List of 6
##  $ sampling         : NULL
##  $ index            :List of 25
##  $ indexOut         :List of 25
##  $ indexFinal       : NULL
##  $ timingSamps      : num 0
##  $ predictionBounds : logi [1:2] FALSE FALSE
##  $ seeds            :List of 26
##  $ adaptive         :List of 4
##  $ trim             : logi FALSE
##  $ allowParallel    : logi TRUE
##  $ yLimits          : num [1:2] 9.3 33.5</code></pre>
<p>Check out the some of the model characteristics</p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb128-1" data-line-number="1"><span class="kw">summary</span>(lm_fit)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = .outcome ~ ., data = dat)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -3.890 -2.163 -0.091  1.361  7.140 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  36.2505     1.7925  20.223  &lt; 2e-16 ***
## wt           -4.9957     0.5249  -9.516 5.89e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.75 on 26 degrees of freedom
## Multiple R-squared:  0.7769, Adjusted R-squared:  0.7684 
## F-statistic: 90.56 on 1 and 26 DF,  p-value: 5.889e-10</code></pre>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb130-1" data-line-number="1"><span class="kw">summary</span>(lm_fit<span class="op">$</span>finalModel)<span class="op">$</span>r.squared</a></code></pre></div>
<pre><code>## [1] 0.7769458</code></pre>
<p>We can go right to the final Model which contains the information for the</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb132-1" data-line-number="1">lm_fit<span class="op">$</span>finalModel</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = .outcome ~ ., data = dat)
## 
## Coefficients:
## (Intercept)           wt  
##      36.250       -4.996</code></pre>
<p>So it looks like caret did some re sampling for us by default. Actually, it was bootstrap sampling that we discussed earlier. However, we can specify cross fold validation if we wanted to. This requires a slightly more involved form of the <strong>train</strong> function.</p>
<p>You can influence the <strong>train</strong> function by passing a “special” list / object to it via the <strong>trControl</strong> argument. This gets a bit confusing because the primary function to train models is called <strong>train</strong> and the command used to create the special is called <strong>trainControl</strong> and the argument in the <strong>train</strong> function is called <strong>trControl</strong>. With use, it becomes easier to remember the difference though at first it’s confusing.</p>
<p>Here we train the model as before but specifically requesting a Cross Fold Validation method. We are requesting verbose output.</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb134-1" data-line-number="1">control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>,       <span class="co"># Cross Fold</span></a>
<a class="sourceLine" id="cb134-2" data-line-number="2">                        <span class="dt">number =</span> <span class="dv">5</span>,          <span class="co"># 5 Folds</span></a>
<a class="sourceLine" id="cb134-3" data-line-number="3">                        <span class="dt">verboseIter =</span> <span class="ot">TRUE</span>)  <span class="co"># Verbose</span></a>
<a class="sourceLine" id="cb134-4" data-line-number="4"><span class="co"># Train the model</span></a>
<a class="sourceLine" id="cb134-5" data-line-number="5"><span class="kw">set.seed</span>(<span class="dv">123</span>) <span class="co"># Make this example reproducible</span></a>
<a class="sourceLine" id="cb134-6" data-line-number="6">my_lm &lt;-<span class="st"> </span><span class="kw">train</span>(</a>
<a class="sourceLine" id="cb134-7" data-line-number="7">  mpg <span class="op">~</span><span class="st"> </span>., </a>
<a class="sourceLine" id="cb134-8" data-line-number="8">  Train,</a>
<a class="sourceLine" id="cb134-9" data-line-number="9">  <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>,</a>
<a class="sourceLine" id="cb134-10" data-line-number="10">  <span class="dt">trControl =</span> control</a>
<a class="sourceLine" id="cb134-11" data-line-number="11">)</a></code></pre></div>
<pre><code>## + Fold1: intercept=TRUE 
## - Fold1: intercept=TRUE 
## + Fold2: intercept=TRUE 
## - Fold2: intercept=TRUE 
## + Fold3: intercept=TRUE 
## - Fold3: intercept=TRUE 
## + Fold4: intercept=TRUE 
## - Fold4: intercept=TRUE 
## + Fold5: intercept=TRUE 
## - Fold5: intercept=TRUE 
## Aggregating results
## Fitting final model on full training set</code></pre>
<p>So the object returned from caret gives us an estimate of how well the model will perform (based on RMSE) for out of sample data.</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb136-1" data-line-number="1">my_lm</a></code></pre></div>
<pre><code>## Linear Regression 
## 
## 28 samples
## 10 predictors
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 22, 22, 23, 22, 23 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   2.955532  0.7487594  2.528231
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb138-1" data-line-number="1">Metrics<span class="op">::</span><span class="kw">rmse</span>(Test<span class="op">$</span>mpg,<span class="kw">predict</span>(my_lm,Test))</a></code></pre></div>
<pre><code>## [1] 4.808981</code></pre>
<p>We could also repeat the 5 times CV validation an arbitrary number of times to generate greater confidence in the RMSE estimates returned by the model. Remember, a major reason for using K Fold validation is to better estimate the out of sample error by holding out a portion of the data frame being trained upon.</p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb140-1" data-line-number="1">control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;repeatedcv&quot;</span>,  <span class="co"># Repeated Cross Fold</span></a>
<a class="sourceLine" id="cb140-2" data-line-number="2">                        <span class="dt">number  =</span> <span class="dv">3</span>,          <span class="co"># 3 Folds</span></a>
<a class="sourceLine" id="cb140-3" data-line-number="3">                        <span class="dt">repeats =</span> <span class="dv">3</span>,          <span class="co"># Repeats</span></a>
<a class="sourceLine" id="cb140-4" data-line-number="4">                        <span class="dt">verboseIter =</span> <span class="ot">FALSE</span>)  <span class="co"># Verbose</span></a>
<a class="sourceLine" id="cb140-5" data-line-number="5"></a>
<a class="sourceLine" id="cb140-6" data-line-number="6"><span class="kw">set.seed</span>(<span class="dv">123</span>) <span class="co"># Make this example reproducible</span></a>
<a class="sourceLine" id="cb140-7" data-line-number="7"></a>
<a class="sourceLine" id="cb140-8" data-line-number="8">my_lm &lt;-<span class="st"> </span><span class="kw">train</span>(</a>
<a class="sourceLine" id="cb140-9" data-line-number="9">  mpg <span class="op">~</span><span class="st"> </span>., </a>
<a class="sourceLine" id="cb140-10" data-line-number="10">  Train,</a>
<a class="sourceLine" id="cb140-11" data-line-number="11">  <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>,</a>
<a class="sourceLine" id="cb140-12" data-line-number="12">  <span class="dt">trControl =</span> control</a>
<a class="sourceLine" id="cb140-13" data-line-number="13">)</a>
<a class="sourceLine" id="cb140-14" data-line-number="14"></a>
<a class="sourceLine" id="cb140-15" data-line-number="15">Metrics<span class="op">::</span><span class="kw">rmse</span>(Test<span class="op">$</span>mpg,<span class="kw">predict</span>(my_lm,Test))</a></code></pre></div>
<pre><code>## [1] 4.808981</code></pre>
</div>
<div id="reproducible-results" class="section level2">
<h2><span class="header-section-number">5.5</span> Reproducible Results</h2>
<p>So this is where using a package like <strong>caret</strong> really pays off in that not only does it provide a nice, consistent front end for a large number of methods, it gives you back an object containing all of the results plus all thing used to provide the final estimate on out of band error.</p>
<p>While digging in deeper into the object can be scary, it does pay off to spend time looking at it. Think of it this way, if you don’t use something like caret then you will have to write your own loops to keep track of, for example, the RMSE resulting from each train/test pair in a cross fold validation. This of course is fine but caret makes it easier. Let’s study the previous example in greater detail.</p>
<p>To get the best result (e.g. RMSE, R-Squared, MAE) directly:</p>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb142-1" data-line-number="1">my_lm<span class="op">$</span>result</a></code></pre></div>
<pre><code>##   intercept     RMSE  Rsquared      MAE   RMSESD RsquaredSD    MAESD
## 1      TRUE 4.574962 0.6092634 3.584692 2.811159  0.2692945 1.881702</code></pre>
<p>To see the results coming from each train / test pair using to build the final Out of Sample Error estimate. Why are there 9 rows here ?</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb144-1" data-line-number="1">my_lm<span class="op">$</span>resample</a></code></pre></div>
<pre><code>##        RMSE   Rsquared      MAE   Resample
## 1  3.217212 0.68744503 2.744463 Fold1.Rep1
## 2  5.348479 0.27930338 3.923923 Fold2.Rep1
## 3  5.670200 0.48061619 4.860719 Fold3.Rep1
## 4  2.346548 0.83748766 1.989082 Fold1.Rep2
## 5  3.220792 0.76252308 2.511865 Fold2.Rep2
## 6  4.150818 0.74640561 3.858077 Fold3.Rep2
## 7  2.386493 0.86674237 1.915139 Fold1.Rep3
## 8  3.438640 0.73519906 2.592498 Fold2.Rep3
## 9 11.395476 0.08764857 7.866464 Fold3.Rep3</code></pre>
<p>Given that this is the underlying information that was used to build the ultimate out of sample error estimate, we should be able to take the average of the RMSE column which should match what we get in the final result.</p>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb146-1" data-line-number="1"><span class="co"># Access the RMSE column</span></a>
<a class="sourceLine" id="cb146-2" data-line-number="2">my_lm<span class="op">$</span>resample[,<span class="st">&#39;RMSE&#39;</span>]</a></code></pre></div>
<pre><code>## [1]  3.217212  5.348479  5.670200  2.346548  3.220792  4.150818  2.386493
## [8]  3.438640 11.395476</code></pre>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb148-1" data-line-number="1"><span class="co"># What is the average</span></a>
<a class="sourceLine" id="cb148-2" data-line-number="2">my_lm<span class="op">$</span>resample[,<span class="st">&#39;RMSE&#39;</span>] <span class="op">%&gt;%</span><span class="st"> </span>mean</a></code></pre></div>
<pre><code>## [1] 4.574962</code></pre>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb150-1" data-line-number="1"><span class="co"># Does it match what we get in the final result ?</span></a>
<a class="sourceLine" id="cb150-2" data-line-number="2">my_lm<span class="op">$</span>results[,<span class="st">&#39;RMSE&#39;</span>]</a></code></pre></div>
<pre><code>## [1] 4.574962</code></pre>
<p>The object is even more complete in that it provides you with access to the underlying sampled data used to generate the intermediate train / test and final results. The following will give us the row numbers of the training data used per iteration.</p>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb152-1" data-line-number="1">my_lm<span class="op">$</span>control<span class="op">$</span>index</a></code></pre></div>
<pre><code>## $Fold1.Rep1
##  [1]  1  3  7  8  9 10 11 12 13 14 16 17 18 20 21 22 24 25 28
## 
## $Fold2.Rep1
##  [1]  2  4  5  6  7  8 10 13 14 15 19 22 23 24 25 26 27 28
## 
## $Fold3.Rep1
##  [1]  1  2  3  4  5  6  9 11 12 15 16 17 18 19 20 21 23 26 27
## 
## $Fold1.Rep2
##  [1]  1  2  4  6  7  8  9 10 14 15 16 17 18 19 20 22 24 27
## 
## $Fold2.Rep2
##  [1]  1  3  5  6  7  9 10 11 12 13 15 17 20 21 22 23 25 26 28
## 
## $Fold3.Rep2
##  [1]  2  3  4  5  8 11 12 13 14 16 18 19 21 23 24 25 26 27 28
## 
## $Fold1.Rep3
##  [1]  1  4  6  7  8 11 13 15 16 19 20 21 22 24 25 26 27 28
## 
## $Fold2.Rep3
##  [1]  2  3  5  6  9 10 11 12 14 17 18 19 20 21 23 25 26 27 28
## 
## $Fold3.Rep3
##  [1]  1  2  3  4  5  7  8  9 10 12 13 14 15 16 17 18 22 23 24</code></pre>
<p>The following will give us the row numbers of the corresponding test data used per iteration. The general idea here is that you could recreate the final result by writing your own code if you were asked to do so.</p>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb154-1" data-line-number="1">my_lm<span class="op">$</span>control<span class="op">$</span>indexOut</a></code></pre></div>
<pre><code>## $Resample1
## [1]  2  4  5  6 15 19 23 26 27
## 
## $Resample2
##  [1]  1  3  9 11 12 16 17 18 20 21
## 
## $Resample3
## [1]  7  8 10 13 14 22 24 25 28
## 
## $Resample4
##  [1]  3  5 11 12 13 21 23 25 26 28
## 
## $Resample5
## [1]  2  4  8 14 16 18 19 24 27
## 
## $Resample6
## [1]  1  6  7  9 10 15 17 20 22
## 
## $Resample7
##  [1]  2  3  5  9 10 12 14 17 18 23
## 
## $Resample8
## [1]  1  4  7  8 13 15 16 22 24
## 
## $Resample9
## [1]  6 11 19 20 21 25 26 27 28</code></pre>
</div>
<div id="one-size-fits-all" class="section level2">
<h2><span class="header-section-number">5.6</span> One Size Fits All</h2>
<p>So this is where things get interesting. If we wanted to use another method such as Random Forests, we do NOT have to change much at all. We just provide the name of the desired method which in this case, is <strong>ranger</strong> which is a function to create Random Forests.</p>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb156-1" data-line-number="1">control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, </a>
<a class="sourceLine" id="cb156-2" data-line-number="2">                        <span class="dt">number =</span> <span class="dv">5</span>)</a>
<a class="sourceLine" id="cb156-3" data-line-number="3"></a>
<a class="sourceLine" id="cb156-4" data-line-number="4"><span class="kw">set.seed</span>(<span class="dv">123</span>) <span class="co"># Make this example reproducible</span></a>
<a class="sourceLine" id="cb156-5" data-line-number="5"></a>
<a class="sourceLine" id="cb156-6" data-line-number="6">my_ranger &lt;-<span class="st"> </span><span class="kw">train</span>(</a>
<a class="sourceLine" id="cb156-7" data-line-number="7">  mpg <span class="op">~</span><span class="st"> </span>., </a>
<a class="sourceLine" id="cb156-8" data-line-number="8">  Train,</a>
<a class="sourceLine" id="cb156-9" data-line-number="9">  <span class="dt">method =</span> <span class="st">&quot;ranger&quot;</span>,</a>
<a class="sourceLine" id="cb156-10" data-line-number="10">  <span class="dt">trControl =</span> control</a>
<a class="sourceLine" id="cb156-11" data-line-number="11">)</a>
<a class="sourceLine" id="cb156-12" data-line-number="12"></a>
<a class="sourceLine" id="cb156-13" data-line-number="13">my_ranger</a></code></pre></div>
<pre><code>## Random Forest 
## 
## 28 samples
## 10 predictors
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 22, 22, 23, 22, 23 
## Resampling results across tuning parameters:
## 
##   mtry  splitrule   RMSE      Rsquared   MAE     
##    2    variance    2.587753  0.9017899  2.180380
##    2    extratrees  2.694316  0.8805852  2.297226
##    6    variance    2.490290  0.9091406  2.147651
##    6    extratrees  2.608913  0.8938029  2.240797
##   10    variance    2.535569  0.9107767  2.186020
##   10    extratrees  2.523521  0.8992038  2.170427
## 
## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 5
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were mtry = 6, splitrule =
##  variance and min.node.size = 5.</code></pre>
<div class="sourceCode" id="cb158"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb158-1" data-line-number="1">Metrics<span class="op">::</span><span class="kw">rmse</span>(Test<span class="op">$</span>mpg,<span class="kw">predict</span>(my_ranger,Test))</a></code></pre></div>
<pre><code>## [1] 2.288412</code></pre>
</div>
<div id="alternative-calling-sequence" class="section level2">
<h2><span class="header-section-number">5.7</span> Alternative Calling Sequence</h2>
<p>It is native to R to want to use a formula when specifying the goal of the modeling process. We’ve been using something along the lines of “mpg ~ .” because R uses this format for many different statistical functions. However, it is also possible to specify the predictor variables as X and the predicted variable as Y. In the above example, this would look like:</p>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb160-1" data-line-number="1">my_ctrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(</a>
<a class="sourceLine" id="cb160-2" data-line-number="2">    <span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, </a>
<a class="sourceLine" id="cb160-3" data-line-number="3">    <span class="dt">number =</span> <span class="dv">3</span>,</a>
<a class="sourceLine" id="cb160-4" data-line-number="4">    <span class="dt">verboseIter =</span> <span class="ot">FALSE</span> </a>
<a class="sourceLine" id="cb160-5" data-line-number="5">  )</a>
<a class="sourceLine" id="cb160-6" data-line-number="6"></a>
<a class="sourceLine" id="cb160-7" data-line-number="7">my_ranger &lt;-<span class="st"> </span><span class="kw">train</span>(</a>
<a class="sourceLine" id="cb160-8" data-line-number="8">  <span class="dt">x =</span> Train[,<span class="op">-</span><span class="dv">1</span>],    <span class="co"># Everything BUT the MPG column</span></a>
<a class="sourceLine" id="cb160-9" data-line-number="9">  <span class="dt">y =</span> Train[,<span class="dv">1</span>],     <span class="co"># The MPG column</span></a>
<a class="sourceLine" id="cb160-10" data-line-number="10">  <span class="dt">method =</span> <span class="st">&quot;ranger&quot;</span>,</a>
<a class="sourceLine" id="cb160-11" data-line-number="11">  <span class="dt">tuneLength =</span> <span class="dv">6</span>,    </a>
<a class="sourceLine" id="cb160-12" data-line-number="12">  <span class="dt">trControl =</span> my_ctrl</a>
<a class="sourceLine" id="cb160-13" data-line-number="13">)</a>
<a class="sourceLine" id="cb160-14" data-line-number="14"></a>
<a class="sourceLine" id="cb160-15" data-line-number="15">my_ranger</a></code></pre></div>
<pre><code>## Random Forest 
## 
## 28 samples
## 10 predictors
## 
## No pre-processing
## Resampling: Cross-Validated (3 fold) 
## Summary of sample sizes: 19, 18, 19 
## Resampling results across tuning parameters:
## 
##   mtry  splitrule   RMSE      Rsquared   MAE     
##    2    variance    2.890146  0.8056424  2.266813
##    2    extratrees  2.926584  0.7983939  2.326078
##    3    variance    2.843515  0.8124454  2.219626
##    3    extratrees  2.884787  0.8097881  2.310120
##    5    variance    2.830262  0.8086326  2.217911
##    5    extratrees  2.887976  0.8122192  2.317901
##    6    variance    2.819049  0.8125409  2.190555
##    6    extratrees  2.972158  0.7987946  2.378725
##    8    variance    2.822495  0.8085079  2.214570
##    8    extratrees  2.944229  0.7993095  2.365574
##   10    variance    2.834211  0.8025443  2.247054
##   10    extratrees  2.951646  0.8008366  2.357687
## 
## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 5
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were mtry = 6, splitrule =
##  variance and min.node.size = 5.</code></pre>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb162-1" data-line-number="1">Metrics<span class="op">::</span><span class="kw">rmse</span>(Test<span class="op">$</span>mpg,<span class="kw">predict</span>(my_ranger,Test))</a></code></pre></div>
<pre><code>## [1] 2.338702</code></pre>
</div>
<div id="hyperparameters" class="section level2">
<h2><span class="header-section-number">5.8</span> Hyperparameters</h2>
<p>This model returns more information than say the <strong>lm</strong> function because this method uses something called “hyperparameters” which are arguments to a given method that gets set before you call the method. In this case there are two hyperparameters called <strong>mtry</strong> and <strong>splitrule</strong> that assume default variables if we don’t supply values.</p>
<p>We can get a plot of how the RMSE and R squared value varied with different values of <strong>mtry</strong> as well the <strong>splitrule</strong>. Here we see that an mtry value of 6 randomly selected columns / variables provides the lowest RMSE.</p>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb164-1" data-line-number="1"><span class="kw">ggplot</span>(my_ranger) <span class="op">+</span><span class="st"> </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;top&quot;</span>)</a></code></pre></div>
<p><img src="biosml_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>We can use the <strong>tuneLength</strong> argument to tell the <strong>train</strong> function to use N different values of <strong>mtry</strong> which is a <strong>hyperparameter</strong> to the randomForest package. The value relates to the number of columns in the data frame. We have 11 total and we are trying to predict one of them (mpg). So we can tell the <strong>train</strong> function to randomly select N variables (up to 10) when a tree is split.</p>
<div class="sourceCode" id="cb165"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb165-1" data-line-number="1">my_ctrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(</a>
<a class="sourceLine" id="cb165-2" data-line-number="2">    <span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, </a>
<a class="sourceLine" id="cb165-3" data-line-number="3">    <span class="dt">number =</span> <span class="dv">5</span>,</a>
<a class="sourceLine" id="cb165-4" data-line-number="4">    <span class="dt">verboseIter =</span> <span class="ot">TRUE</span></a>
<a class="sourceLine" id="cb165-5" data-line-number="5">  )</a>
<a class="sourceLine" id="cb165-6" data-line-number="6"></a>
<a class="sourceLine" id="cb165-7" data-line-number="7"><span class="kw">set.seed</span>(<span class="dv">123</span>) <span class="co"># Make this example reproducible</span></a>
<a class="sourceLine" id="cb165-8" data-line-number="8"></a>
<a class="sourceLine" id="cb165-9" data-line-number="9">my_rf &lt;-<span class="st"> </span><span class="kw">train</span>(</a>
<a class="sourceLine" id="cb165-10" data-line-number="10">  mpg <span class="op">~</span><span class="st"> </span>., </a>
<a class="sourceLine" id="cb165-11" data-line-number="11">  Train,</a>
<a class="sourceLine" id="cb165-12" data-line-number="12">  <span class="dt">method =</span> <span class="st">&quot;rf&quot;</span>,</a>
<a class="sourceLine" id="cb165-13" data-line-number="13">  <span class="dt">tuneLength =</span> <span class="dv">9</span>,    <span class="co"># We&#39;ll use 9 different values</span></a>
<a class="sourceLine" id="cb165-14" data-line-number="14">  <span class="dt">trControl =</span> my_ctrl</a>
<a class="sourceLine" id="cb165-15" data-line-number="15">)</a></code></pre></div>
<pre><code>## + Fold1: mtry= 2 
## - Fold1: mtry= 2 
## + Fold1: mtry= 3 
## - Fold1: mtry= 3 
## + Fold1: mtry= 4 
## - Fold1: mtry= 4 
## + Fold1: mtry= 5 
## - Fold1: mtry= 5 
## + Fold1: mtry= 6 
## - Fold1: mtry= 6 
## + Fold1: mtry= 7 
## - Fold1: mtry= 7 
## + Fold1: mtry= 8 
## - Fold1: mtry= 8 
## + Fold1: mtry= 9 
## - Fold1: mtry= 9 
## + Fold1: mtry=10 
## - Fold1: mtry=10 
## + Fold2: mtry= 2 
## - Fold2: mtry= 2 
## + Fold2: mtry= 3 
## - Fold2: mtry= 3 
## + Fold2: mtry= 4 
## - Fold2: mtry= 4 
## + Fold2: mtry= 5 
## - Fold2: mtry= 5 
## + Fold2: mtry= 6 
## - Fold2: mtry= 6 
## + Fold2: mtry= 7 
## - Fold2: mtry= 7 
## + Fold2: mtry= 8 
## - Fold2: mtry= 8 
## + Fold2: mtry= 9 
## - Fold2: mtry= 9 
## + Fold2: mtry=10 
## - Fold2: mtry=10 
## + Fold3: mtry= 2 
## - Fold3: mtry= 2 
## + Fold3: mtry= 3 
## - Fold3: mtry= 3 
## + Fold3: mtry= 4 
## - Fold3: mtry= 4 
## + Fold3: mtry= 5 
## - Fold3: mtry= 5 
## + Fold3: mtry= 6 
## - Fold3: mtry= 6 
## + Fold3: mtry= 7 
## - Fold3: mtry= 7 
## + Fold3: mtry= 8 
## - Fold3: mtry= 8 
## + Fold3: mtry= 9 
## - Fold3: mtry= 9 
## + Fold3: mtry=10 
## - Fold3: mtry=10 
## + Fold4: mtry= 2 
## - Fold4: mtry= 2 
## + Fold4: mtry= 3 
## - Fold4: mtry= 3 
## + Fold4: mtry= 4 
## - Fold4: mtry= 4 
## + Fold4: mtry= 5 
## - Fold4: mtry= 5 
## + Fold4: mtry= 6 
## - Fold4: mtry= 6 
## + Fold4: mtry= 7 
## - Fold4: mtry= 7 
## + Fold4: mtry= 8 
## - Fold4: mtry= 8 
## + Fold4: mtry= 9 
## - Fold4: mtry= 9 
## + Fold4: mtry=10 
## - Fold4: mtry=10 
## + Fold5: mtry= 2 
## - Fold5: mtry= 2 
## + Fold5: mtry= 3 
## - Fold5: mtry= 3 
## + Fold5: mtry= 4 
## - Fold5: mtry= 4 
## + Fold5: mtry= 5 
## - Fold5: mtry= 5 
## + Fold5: mtry= 6 
## - Fold5: mtry= 6 
## + Fold5: mtry= 7 
## - Fold5: mtry= 7 
## + Fold5: mtry= 8 
## - Fold5: mtry= 8 
## + Fold5: mtry= 9 
## - Fold5: mtry= 9 
## + Fold5: mtry=10 
## - Fold5: mtry=10 
## Aggregating results
## Selecting tuning parameters
## Fitting mtry = 6 on full training set</code></pre>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb167-1" data-line-number="1">my_rf</a></code></pre></div>
<pre><code>## Random Forest 
## 
## 28 samples
## 10 predictors
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 22, 22, 23, 22, 23 
## Resampling results across tuning parameters:
## 
##   mtry  RMSE      Rsquared   MAE     
##    2    2.579696  0.8997291  2.181764
##    3    2.543545  0.9002549  2.181950
##    4    2.538643  0.9059724  2.193849
##    5    2.467108  0.9142192  2.129060
##    6    2.457545  0.9203487  2.130067
##    7    2.510544  0.9103018  2.168747
##    8    2.541710  0.9096472  2.210990
##    9    2.539938  0.9136301  2.198482
##   10    2.592113  0.9070706  2.214741
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was mtry = 6.</code></pre>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb169-1" data-line-number="1"><span class="kw">plot</span>(my_rf)</a></code></pre></div>
<p><img src="biosml_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb170-1" data-line-number="1">Metrics<span class="op">::</span><span class="kw">rmse</span>(Test<span class="op">$</span>mpg,<span class="kw">predict</span>(my_rf,Test))</a></code></pre></div>
<pre><code>## [1] 2.23303</code></pre>
<p>If you have a questions about what hyper parameters can be tuned for a given method then you can refer to the online <a href="https://topepo.github.io/caret/available-models.html">caret documentation</a> Here is a screenshot of the table of supported models and associated tuning parameters.</p>
<p><img src="pics/avmodels.png" /></p>
<p>Another way to do this within the caret package itself is that if you already know the abbreviation for the specific method you wish to use (e.g. “rf”) then you can use some built in functions to help you. Remember that <strong>caret</strong> does not replace or rewrite functions, it merely provides a nice wrapper around them. Since each underlying model is it a standalone R package and associated set of functions you can always call them directly.</p>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb172-1" data-line-number="1"><span class="kw">modelLookup</span>(<span class="st">&quot;rf&quot;</span>)</a></code></pre></div>
<pre><code>##   model parameter                         label forReg forClass probModel
## 1    rf      mtry #Randomly Selected Predictors   TRUE     TRUE      TRUE</code></pre>
<p>Here we get the hyper parameters for the <strong>ranger</strong> function. We see that it has three hyper parameters that could be varied in some way to influence a final model.</p>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb174-1" data-line-number="1"><span class="kw">modelLookup</span>(<span class="st">&quot;ranger&quot;</span>)</a></code></pre></div>
<pre><code>##    model     parameter                         label forReg forClass
## 1 ranger          mtry #Randomly Selected Predictors   TRUE     TRUE
## 2 ranger     splitrule                Splitting Rule   TRUE     TRUE
## 3 ranger min.node.size             Minimal Node Size   TRUE     TRUE
##   probModel
## 1      TRUE
## 2      TRUE
## 3      TRUE</code></pre>
<p>If you just want a list of all the models supported by caret then do something like this:</p>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb176-1" data-line-number="1">models &lt;-<span class="st"> </span><span class="kw">modelLookup</span>()[,<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>]</a>
<a class="sourceLine" id="cb176-2" data-line-number="2"><span class="kw">nrow</span>(models)</a></code></pre></div>
<pre><code>## [1] 502</code></pre>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb178-1" data-line-number="1"><span class="co"># Search for a Model</span></a>
<a class="sourceLine" id="cb178-2" data-line-number="2">models[models<span class="op">$</span>model<span class="op">==</span><span class="st">&quot;rf&quot;</span>,]</a></code></pre></div>
<pre><code>##     model parameter                         label
## 365    rf      mtry #Randomly Selected Predictors</code></pre>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb180-1" data-line-number="1">models[models<span class="op">$</span>model<span class="op">==</span><span class="st">&quot;ranger&quot;</span>,]</a></code></pre></div>
<pre><code>##      model     parameter                         label
## 351 ranger          mtry #Randomly Selected Predictors
## 352 ranger     splitrule                Splitting Rule
## 353 ranger min.node.size             Minimal Node Size</code></pre>
<p>So in the case of the <strong>ranger</strong> function there are actually three hyper parameters that could be tuned.</p>
<div class="sourceCode" id="cb182"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb182-1" data-line-number="1">my_ctrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(</a>
<a class="sourceLine" id="cb182-2" data-line-number="2">    <span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, </a>
<a class="sourceLine" id="cb182-3" data-line-number="3">    <span class="dt">number =</span> <span class="dv">3</span>,</a>
<a class="sourceLine" id="cb182-4" data-line-number="4">    <span class="dt">verboseIter =</span> <span class="ot">FALSE</span> </a>
<a class="sourceLine" id="cb182-5" data-line-number="5">  )</a>
<a class="sourceLine" id="cb182-6" data-line-number="6"></a>
<a class="sourceLine" id="cb182-7" data-line-number="7">my_ranger &lt;-<span class="st"> </span><span class="kw">train</span>(</a>
<a class="sourceLine" id="cb182-8" data-line-number="8">  mpg <span class="op">~</span><span class="st"> </span>., </a>
<a class="sourceLine" id="cb182-9" data-line-number="9">  Train,</a>
<a class="sourceLine" id="cb182-10" data-line-number="10">  <span class="dt">method =</span> <span class="st">&quot;ranger&quot;</span>,</a>
<a class="sourceLine" id="cb182-11" data-line-number="11">  <span class="dt">tuneLength =</span> <span class="dv">6</span>,    </a>
<a class="sourceLine" id="cb182-12" data-line-number="12">  <span class="dt">trControl =</span> my_ctrl</a>
<a class="sourceLine" id="cb182-13" data-line-number="13">)</a>
<a class="sourceLine" id="cb182-14" data-line-number="14"></a>
<a class="sourceLine" id="cb182-15" data-line-number="15">my_ranger</a></code></pre></div>
<pre><code>## Random Forest 
## 
## 28 samples
## 10 predictors
## 
## No pre-processing
## Resampling: Cross-Validated (3 fold) 
## Summary of sample sizes: 18, 19, 19 
## Resampling results across tuning parameters:
## 
##   mtry  splitrule   RMSE      Rsquared   MAE     
##    2    variance    2.761408  0.8672659  2.374636
##    2    extratrees  2.719210  0.8528162  2.323624
##    3    variance    2.745672  0.8706840  2.383784
##    3    extratrees  2.744949  0.8486200  2.367447
##    5    variance    2.834880  0.8593408  2.431688
##    5    extratrees  2.779045  0.8534449  2.397243
##    6    variance    2.827772  0.8627602  2.415079
##    6    extratrees  2.761680  0.8580494  2.404353
##    8    variance    2.858153  0.8550033  2.461909
##    8    extratrees  2.776434  0.8535960  2.438577
##   10    variance    2.853339  0.8602301  2.480190
##   10    extratrees  2.822600  0.8541223  2.444921
## 
## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 5
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were mtry = 2, splitrule =
##  extratrees and min.node.size = 5.</code></pre>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb184-1" data-line-number="1">Metrics<span class="op">::</span><span class="kw">rmse</span>(Test<span class="op">$</span>mpg,<span class="kw">predict</span>(my_ranger,Test))</a></code></pre></div>
<pre><code>## [1] 3.042224</code></pre>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb186-1" data-line-number="1"><span class="kw">plot</span>(my_ranger)</a></code></pre></div>
<p><img src="biosml_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="training-test-data.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="classification-problems.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
