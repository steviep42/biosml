<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 11 Data Pre Processing | Predictive Learning in R</title>
  <meta name="description" content="Chapter 11 Data Pre Processing | Predictive Learning in R" />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 11 Data Pre Processing | Predictive Learning in R" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 11 Data Pre Processing | Predictive Learning in R" />
  
  
  

<meta name="author" content="Steve Pittard" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="picking-the-best-model.html"/>
<link rel="next" href="using-external-ml-frameworks.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#machine-learning"><i class="fa fa-check"></i><b>1.1</b> Machine Learning</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#predictive-modeling"><i class="fa fa-check"></i><b>1.2</b> Predictive Modeling</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#in-sample-vs-out-of-sample-data"><i class="fa fa-check"></i><b>1.3</b> In-Sample vs Out-Of-Sample Data</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#performance-metrics"><i class="fa fa-check"></i><b>1.4</b> Performance Metrics</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#black-box"><i class="fa fa-check"></i><b>1.5</b> Black Box</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html"><i class="fa fa-check"></i><b>2</b> Predictive / Supervised Learning</a><ul>
<li class="chapter" data-level="2.1" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#explanation-vs-prediction"><i class="fa fa-check"></i><b>2.1</b> Explanation vs Prediction</a></li>
<li class="chapter" data-level="2.2" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#two-types-of-predictive-models"><i class="fa fa-check"></i><b>2.2</b> Two Types of Predictive Models:</a></li>
<li class="chapter" data-level="2.3" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#bias-vs-variance"><i class="fa fa-check"></i><b>2.3</b> Bias vs Variance</a><ul>
<li class="chapter" data-level="2.3.1" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#bias"><i class="fa fa-check"></i><b>2.3.1</b> Bias</a></li>
<li class="chapter" data-level="2.3.2" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#variance"><i class="fa fa-check"></i><b>2.3.2</b> Variance</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#overfitting-and-underfitting"><i class="fa fa-check"></i><b>2.4</b> Overfitting and Underfitting</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="a-motivating-example.html"><a href="a-motivating-example.html"><i class="fa fa-check"></i><b>3</b> A Motivating Example</a><ul>
<li class="chapter" data-level="3.1" data-path="a-motivating-example.html"><a href="a-motivating-example.html#suggested-workflow"><i class="fa fa-check"></i><b>3.1</b> Suggested Workflow</a></li>
<li class="chapter" data-level="3.2" data-path="a-motivating-example.html"><a href="a-motivating-example.html#scatterplot"><i class="fa fa-check"></i><b>3.2</b> Scatterplot</a></li>
<li class="chapter" data-level="3.3" data-path="a-motivating-example.html"><a href="a-motivating-example.html#correlations"><i class="fa fa-check"></i><b>3.3</b> Correlations</a></li>
<li class="chapter" data-level="3.4" data-path="a-motivating-example.html"><a href="a-motivating-example.html#building-a-model---in-sample-error"><i class="fa fa-check"></i><b>3.4</b> Building A Model - In Sample Error</a></li>
<li class="chapter" data-level="3.5" data-path="a-motivating-example.html"><a href="a-motivating-example.html#out-of-sample-data"><i class="fa fa-check"></i><b>3.5</b> Out Of Sample Data</a></li>
<li class="chapter" data-level="3.6" data-path="a-motivating-example.html"><a href="a-motivating-example.html#other-methods"><i class="fa fa-check"></i><b>3.6</b> Other Methods ?</a></li>
<li class="chapter" data-level="3.7" data-path="a-motivating-example.html"><a href="a-motivating-example.html#summary"><i class="fa fa-check"></i><b>3.7</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="training-test-data.html"><a href="training-test-data.html"><i class="fa fa-check"></i><b>4</b> Training / Test Data</a><ul>
<li class="chapter" data-level="4.1" data-path="training-test-data.html"><a href="training-test-data.html#cross-fold-validation"><i class="fa fa-check"></i><b>4.1</b> Cross Fold Validation</a></li>
<li class="chapter" data-level="4.2" data-path="training-test-data.html"><a href="training-test-data.html#create-a-function-to-automate-things"><i class="fa fa-check"></i><b>4.2</b> Create A Function To Automate Things</a></li>
<li class="chapter" data-level="4.3" data-path="training-test-data.html"><a href="training-test-data.html#repeated-cross-validation"><i class="fa fa-check"></i><b>4.3</b> Repeated Cross Validation</a></li>
<li class="chapter" data-level="4.4" data-path="training-test-data.html"><a href="training-test-data.html#bootstrap"><i class="fa fa-check"></i><b>4.4</b> Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="caret-package.html"><a href="caret-package.html"><i class="fa fa-check"></i><b>5</b> Caret Package</a><ul>
<li class="chapter" data-level="5.1" data-path="caret-package.html"><a href="caret-package.html#putting-caret-to-work"><i class="fa fa-check"></i><b>5.1</b> Putting caret To Work</a></li>
<li class="chapter" data-level="5.2" data-path="caret-package.html"><a href="caret-package.html#back-to-the-beginning"><i class="fa fa-check"></i><b>5.2</b> Back To The Beginning</a></li>
<li class="chapter" data-level="5.3" data-path="caret-package.html"><a href="caret-package.html#splitting"><i class="fa fa-check"></i><b>5.3</b> Splitting</a></li>
<li class="chapter" data-level="5.4" data-path="caret-package.html"><a href="caret-package.html#calling-the-train-function"><i class="fa fa-check"></i><b>5.4</b> Calling The train() Function</a></li>
<li class="chapter" data-level="5.5" data-path="caret-package.html"><a href="caret-package.html#one-size-fits-all"><i class="fa fa-check"></i><b>5.5</b> One Size Fits All</a></li>
<li class="chapter" data-level="5.6" data-path="caret-package.html"><a href="caret-package.html#hyperparameters"><i class="fa fa-check"></i><b>5.6</b> Hyperparameters</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="classification-problems.html"><a href="classification-problems.html"><i class="fa fa-check"></i><b>6</b> Classification Problems</a><ul>
<li class="chapter" data-level="6.1" data-path="classification-problems.html"><a href="classification-problems.html#performance-measures"><i class="fa fa-check"></i><b>6.1</b> Performance Measures</a></li>
<li class="chapter" data-level="6.2" data-path="classification-problems.html"><a href="classification-problems.html#important-terminology"><i class="fa fa-check"></i><b>6.2</b> Important Terminology</a></li>
<li class="chapter" data-level="6.3" data-path="classification-problems.html"><a href="classification-problems.html#a-basic-model"><i class="fa fa-check"></i><b>6.3</b> A Basic Model</a></li>
<li class="chapter" data-level="6.4" data-path="classification-problems.html"><a href="classification-problems.html#selecting-the-correct-alpha"><i class="fa fa-check"></i><b>6.4</b> Selecting The Correct Alpha</a></li>
<li class="chapter" data-level="6.5" data-path="classification-problems.html"><a href="classification-problems.html#hypothesis-testing"><i class="fa fa-check"></i><b>6.5</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="6.6" data-path="classification-problems.html"><a href="classification-problems.html#confusion-matrix"><i class="fa fa-check"></i><b>6.6</b> Confusion Matrix</a><ul>
<li class="chapter" data-level="6.6.1" data-path="classification-problems.html"><a href="classification-problems.html#computing-performance-metrics"><i class="fa fa-check"></i><b>6.6.1</b> Computing Performance Metrics</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="classification-problems.html"><a href="classification-problems.html#picking-the-right-metric"><i class="fa fa-check"></i><b>6.7</b> Picking the Right Metric</a></li>
<li class="chapter" data-level="6.8" data-path="classification-problems.html"><a href="classification-problems.html#wait.-where-are-we"><i class="fa fa-check"></i><b>6.8</b> Wait. Where Are We ?</a></li>
<li class="chapter" data-level="6.9" data-path="classification-problems.html"><a href="classification-problems.html#better-ways-to-compute-the-roc-curve"><i class="fa fa-check"></i><b>6.9</b> Better Ways To Compute The ROC Curve</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="classification-example.html"><a href="classification-example.html"><i class="fa fa-check"></i><b>7</b> Classification Example</a><ul>
<li class="chapter" data-level="7.1" data-path="classification-example.html"><a href="classification-example.html#exploratory-plots"><i class="fa fa-check"></i><b>7.1</b> Exploratory Plots</a></li>
<li class="chapter" data-level="7.2" data-path="classification-example.html"><a href="classification-example.html#generalized-linear-models"><i class="fa fa-check"></i><b>7.2</b> Generalized Linear Models</a></li>
<li class="chapter" data-level="7.3" data-path="classification-example.html"><a href="classification-example.html#random-forests"><i class="fa fa-check"></i><b>7.3</b> Random Forests</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>8</b> Decision Trees</a><ul>
<li class="chapter" data-level="8.1" data-path="decision-trees.html"><a href="decision-trees.html#advantages"><i class="fa fa-check"></i><b>8.1</b> Advantages</a></li>
<li class="chapter" data-level="8.2" data-path="decision-trees.html"><a href="decision-trees.html#a-classification-example"><i class="fa fa-check"></i><b>8.2</b> A Classification Example</a><ul>
<li class="chapter" data-level="8.2.1" data-path="decision-trees.html"><a href="decision-trees.html#evaluating-performance"><i class="fa fa-check"></i><b>8.2.1</b> Evaluating performance</a></li>
<li class="chapter" data-level="8.2.2" data-path="decision-trees.html"><a href="decision-trees.html#tree-splitting"><i class="fa fa-check"></i><b>8.2.2</b> Tree Splitting</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="decision-trees.html"><a href="decision-trees.html#gini-index"><i class="fa fa-check"></i><b>8.3</b> Gini Index</a></li>
<li class="chapter" data-level="8.4" data-path="decision-trees.html"><a href="decision-trees.html#regression-trees"><i class="fa fa-check"></i><b>8.4</b> Regression Trees</a><ul>
<li class="chapter" data-level="8.4.1" data-path="decision-trees.html"><a href="decision-trees.html#performance-measure"><i class="fa fa-check"></i><b>8.4.1</b> Performance Measure</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="decision-trees.html"><a href="decision-trees.html#parameters-vs-hyperparameters"><i class="fa fa-check"></i><b>8.5</b> Parameters vs Hyperparameters</a></li>
<li class="chapter" data-level="8.6" data-path="decision-trees.html"><a href="decision-trees.html#grid-searching"><i class="fa fa-check"></i><b>8.6</b> Grid Searching</a></li>
<li class="chapter" data-level="8.7" data-path="decision-trees.html"><a href="decision-trees.html#bagged-trees"><i class="fa fa-check"></i><b>8.7</b> Bagged Trees</a></li>
<li class="chapter" data-level="8.8" data-path="decision-trees.html"><a href="decision-trees.html#random-forests-1"><i class="fa fa-check"></i><b>8.8</b> Random Forests</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="using-methods-other-than-lm.html"><a href="using-methods-other-than-lm.html"><i class="fa fa-check"></i><b>9</b> Using Methods Other Than lm</a><ul>
<li class="chapter" data-level="9.1" data-path="using-methods-other-than-lm.html"><a href="using-methods-other-than-lm.html#parameters-vs-hyperparameters-1"><i class="fa fa-check"></i><b>9.1</b> Parameters vs Hyperparameters</a></li>
<li class="chapter" data-level="9.2" data-path="using-methods-other-than-lm.html"><a href="using-methods-other-than-lm.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>9.2</b> Hyperparameter Tuning</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html"><i class="fa fa-check"></i><b>10</b> Picking The Best Model</a><ul>
<li class="chapter" data-level="10.1" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#an-example"><i class="fa fa-check"></i><b>10.1</b> An Example</a></li>
<li class="chapter" data-level="10.2" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#more-comparisons"><i class="fa fa-check"></i><b>10.2</b> More Comparisons</a></li>
<li class="chapter" data-level="10.3" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#using-the-resamples-function"><i class="fa fa-check"></i><b>10.3</b> Using the resamples() function</a></li>
<li class="chapter" data-level="10.4" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#model-performance"><i class="fa fa-check"></i><b>10.4</b> Model Performance</a></li>
<li class="chapter" data-level="10.5" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#feature-selection"><i class="fa fa-check"></i><b>10.5</b> Feature Selection</a><ul>
<li class="chapter" data-level="10.5.1" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#recursive-feature-elimination"><i class="fa fa-check"></i><b>10.5.1</b> Recursive Feature Elimination</a></li>
<li class="chapter" data-level="10.5.2" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#redundant-feature-removal"><i class="fa fa-check"></i><b>10.5.2</b> Redundant Feature Removal</a></li>
<li class="chapter" data-level="10.5.3" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#feature-importance"><i class="fa fa-check"></i><b>10.5.3</b> Feature Importance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="data-pre-processing.html"><a href="data-pre-processing.html"><i class="fa fa-check"></i><b>11</b> Data Pre Processing</a><ul>
<li class="chapter" data-level="11.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#types-of-pre-processing"><i class="fa fa-check"></i><b>11.1</b> Types of Pre Processing</a></li>
<li class="chapter" data-level="11.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#missing-values"><i class="fa fa-check"></i><b>11.2</b> Missing Values</a><ul>
<li class="chapter" data-level="11.2.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#finding-rows-with-missing-data"><i class="fa fa-check"></i><b>11.2.1</b> Finding Rows with Missing Data</a></li>
<li class="chapter" data-level="11.2.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#finding-columns-with-missing-data"><i class="fa fa-check"></i><b>11.2.2</b> Finding Columns With Missing Data</a></li>
<li class="chapter" data-level="11.2.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#use-the-median-approach"><i class="fa fa-check"></i><b>11.2.3</b> Use the Median Approach</a></li>
<li class="chapter" data-level="11.2.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#package-based-approach"><i class="fa fa-check"></i><b>11.2.4</b> Package-based Approach</a></li>
<li class="chapter" data-level="11.2.5" data-path="data-pre-processing.html"><a href="data-pre-processing.html#using-caret"><i class="fa fa-check"></i><b>11.2.5</b> Using caret</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#scaling"><i class="fa fa-check"></i><b>11.3</b> Scaling</a><ul>
<li class="chapter" data-level="11.3.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#methods-that-benefit-from-scaling"><i class="fa fa-check"></i><b>11.3.1</b> Methods That Benefit From Scaling</a></li>
<li class="chapter" data-level="11.3.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#methods-that-do-not-require-scaling"><i class="fa fa-check"></i><b>11.3.2</b> Methods That Do Not Require Scaling</a></li>
<li class="chapter" data-level="11.3.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#how-to-scale"><i class="fa fa-check"></i><b>11.3.3</b> How To Scale</a></li>
<li class="chapter" data-level="11.3.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#order-of-processing"><i class="fa fa-check"></i><b>11.3.4</b> Order of Processing</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#low-variance-variables"><i class="fa fa-check"></i><b>11.4</b> Low Variance Variables</a></li>
<li class="chapter" data-level="11.5" data-path="data-pre-processing.html"><a href="data-pre-processing.html#pca---principal-components-analysis"><i class="fa fa-check"></i><b>11.5</b> PCA - Principal Components Analysis</a><ul>
<li class="chapter" data-level="11.5.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#identify-the-factors"><i class="fa fa-check"></i><b>11.5.1</b> Identify The Factors</a></li>
<li class="chapter" data-level="11.5.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#check-for-high-correlations"><i class="fa fa-check"></i><b>11.5.2</b> Check For High Correlations</a></li>
<li class="chapter" data-level="11.5.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#so-why-use-pca"><i class="fa fa-check"></i><b>11.5.3</b> So Why Use PCA ?</a></li>
<li class="chapter" data-level="11.5.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#check-the-biplot"><i class="fa fa-check"></i><b>11.5.4</b> Check The BiPlot</a></li>
<li class="chapter" data-level="11.5.5" data-path="data-pre-processing.html"><a href="data-pre-processing.html#check-the-screeplot"><i class="fa fa-check"></i><b>11.5.5</b> Check The ScreePlot</a></li>
<li class="chapter" data-level="11.5.6" data-path="data-pre-processing.html"><a href="data-pre-processing.html#use-the-transformed-data"><i class="fa fa-check"></i><b>11.5.6</b> Use The Transformed Data</a></li>
<li class="chapter" data-level="11.5.7" data-path="data-pre-processing.html"><a href="data-pre-processing.html#pls"><i class="fa fa-check"></i><b>11.5.7</b> PLS</a></li>
<li class="chapter" data-level="11.5.8" data-path="data-pre-processing.html"><a href="data-pre-processing.html#summary-1"><i class="fa fa-check"></i><b>11.5.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="data-pre-processing.html"><a href="data-pre-processing.html#order-of-pre-processing"><i class="fa fa-check"></i><b>11.6</b> Order of Pre-Processing</a></li>
<li class="chapter" data-level="11.7" data-path="data-pre-processing.html"><a href="data-pre-processing.html#identifying-redundant-features"><i class="fa fa-check"></i><b>11.7</b> Identifying Redundant Features</a><ul>
<li class="chapter" data-level="11.7.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#highly-correlated-variables"><i class="fa fa-check"></i><b>11.7.1</b> Highly Correlated Variables</a></li>
</ul></li>
<li class="chapter" data-level="11.8" data-path="data-pre-processing.html"><a href="data-pre-processing.html#ranking-features"><i class="fa fa-check"></i><b>11.8</b> Ranking Features</a></li>
<li class="chapter" data-level="11.9" data-path="data-pre-processing.html"><a href="data-pre-processing.html#feature-selection-1"><i class="fa fa-check"></i><b>11.9</b> Feature Selection</a></li>
<li class="chapter" data-level="11.10" data-path="data-pre-processing.html"><a href="data-pre-processing.html#categorical-features"><i class="fa fa-check"></i><b>11.10</b> Categorical Features</a></li>
<li class="chapter" data-level="11.11" data-path="data-pre-processing.html"><a href="data-pre-processing.html#binning"><i class="fa fa-check"></i><b>11.11</b> Binning</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html"><i class="fa fa-check"></i><b>12</b> Using External ML Frameworks</a><ul>
<li class="chapter" data-level="12.1" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#using-h2o"><i class="fa fa-check"></i><b>12.1</b> Using h2o</a></li>
<li class="chapter" data-level="12.2" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#create-some-h20-models"><i class="fa fa-check"></i><b>12.2</b> Create Some h20 Models</a></li>
<li class="chapter" data-level="12.3" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#saving-a-model"><i class="fa fa-check"></i><b>12.3</b> Saving A Model</a></li>
<li class="chapter" data-level="12.4" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#using-the-h2o-auto-ml-feature"><i class="fa fa-check"></i><b>12.4</b> Using The h2o Auto ML Feature</a></li>
<li class="chapter" data-level="12.5" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#launching-a-job"><i class="fa fa-check"></i><b>12.5</b> Launching a Job</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Predictive Learning in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="data-pre-processing" class="section level1">
<h1><span class="header-section-number">Chapter 11</span> Data Pre Processing</h1>
<div id="types-of-pre-processing" class="section level2">
<h2><span class="header-section-number">11.1</span> Types of Pre Processing</h2>
<p>Data rarely arrives in a form that is directly suitable for use with a modeling method. There are a number of considerations to make such as how to handle missing data, highly correlated variables, and class imbalances - some categories are over or under represented. Additionally, some variables, also known as “features”, will require transformation or will need to be used to create new variables. Consider the case where the measured data (the numeric data) might be on different scales (e.g. height vs weight). This might result in the need to scale and center the data. Some methods take this into consideration whereas others do not. Suffice it to say that data prep can be an ongoing process that requires a number of experiments before arriving at the best form of data.</p>
</div>
<div id="missing-values" class="section level2">
<h2><span class="header-section-number">11.2</span> Missing Values</h2>
<p>This is a frequent situation in real life. Think of patients checking in for clinic visits over time. Sometimes they come for their appointments, sometimes they don’t. Sometimes when they do come, their information has changed or some diagnostic test is repeated with a new result which is entered or not. Or, whomever maintains the patient database, decides to add in some new variables to measure for all patients moving forward. This means that all existing patients will have missing values for those new variables. To see how this manifests practically in predictive learning consider the following version of the mtcars data frame which has some missing values:</p>
<div class="sourceCode" id="cb331"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb331-1" data-line-number="1"><span class="kw">library</span>(readr)</a>
<a class="sourceLine" id="cb331-2" data-line-number="2">url &lt;-<span class="st"> &quot;https://raw.githubusercontent.com/steviep42/utilities/master/data/mtcars_na.csv&quot;</span></a>
<a class="sourceLine" id="cb331-3" data-line-number="3">mtcars_na &lt;-<span class="st"> </span><span class="kw">read.csv</span>(url, <span class="dt">stringsAsFactors =</span> <span class="ot">FALSE</span>)</a></code></pre></div>
<div id="finding-rows-with-missing-data" class="section level3">
<h3><span class="header-section-number">11.2.1</span> Finding Rows with Missing Data</h3>
<p>Ar first glance it looks like all features have valid variable values but we can look for missing values which, in R, are indicated by <strong>NA</strong> Base R provides a number of commands to do this. First let’s see how many rows there are in the data frame.</p>
<div class="sourceCode" id="cb332"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb332-1" data-line-number="1"><span class="kw">nrow</span>(mtcars_na)</a></code></pre></div>
<pre><code>## [1] 32</code></pre>
<p>Now let’s see how many rows have at least one column with a missing value. So we have eight rows in the data frame that contain one or more missing values.</p>
<div class="sourceCode" id="cb334"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb334-1" data-line-number="1"><span class="kw">sum</span>(<span class="kw">complete.cases</span>(mtcars_na))</a></code></pre></div>
<pre><code>## [1] 24</code></pre>
</div>
<div id="finding-columns-with-missing-data" class="section level3">
<h3><span class="header-section-number">11.2.2</span> Finding Columns With Missing Data</h3>
<p>What columns have missing values ? Here we leverage the use of the apply family of functions along with the ability to create <strong>anonymous</strong> functions on the fly. Both R and Python provide this capability. We see that the <strong>wt</strong> column has three missing values and the <strong>carb</strong> feature has six missing values.</p>
<div class="sourceCode" id="cb336"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb336-1" data-line-number="1"><span class="kw">sapply</span>(mtcars_na,<span class="cf">function</span>(x) <span class="kw">sum</span>(<span class="kw">is.na</span>(x)))</a></code></pre></div>
<pre><code>##  mpg  cyl disp   hp drat   wt qsec   vs   am gear carb 
##    0    0    0    0    0    3    0    0    0    0    6</code></pre>
<p>If we actually wanted to see all rows with missing values:</p>
<div class="sourceCode" id="cb338"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb338-1" data-line-number="1">mtcars_na[<span class="op">!</span><span class="kw">complete.cases</span>(mtcars_na),]</a></code></pre></div>
<pre><code>##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb
## 2  21.0   6 160.0 110 3.90    NA 17.02  0  1    4    4
## 9  22.8   4 140.8  95 3.92    NA 22.90  1  0    4    2
## 10 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4   NA
## 12 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3   NA
## 19 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4   NA
## 20 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4   NA
## 23 15.2   8 304.0 150 3.15    NA 17.30  0  0    3   NA
## 28 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5   NA</code></pre>
<p>What do we do with these ? It depends on a number of things. How hard is it to get this type of data ? If it’s rare information then we probably want to keep all of it because there isn’t that much of it and not all columns are missing for any row. In fact most of the data in a given row is present so maybe one strategy is to tell whatever modeling method we use to ignore the missing values - they might do this by default without you even asking.</p>
<p>We could just filter out any row from the data frame that contains any missing values but in doing so we would lose eight rows of data. This is low stakes data but if this were rare or hard to obtain information then we wouldn’t want to do this.</p>
<div class="sourceCode" id="cb340"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb340-1" data-line-number="1">mtcars_no_na &lt;-<span class="st"> </span>mtcars_na[<span class="kw">complete.cases</span>(mtcars_na),]</a>
<a class="sourceLine" id="cb340-2" data-line-number="2"><span class="kw">nrow</span>(mtcars_no_na)</a></code></pre></div>
<pre><code>## [1] 24</code></pre>
</div>
<div id="use-the-median-approach" class="section level3">
<h3><span class="header-section-number">11.2.3</span> Use the Median Approach</h3>
<p>What could we do ? Well we could keep all rows even if they contains NAs and then use <strong>imputation</strong> methods to supply values for the missing information. There are R packages that do this but one quick way to do this without going that route is to replace the missing value in the <strong>wt</strong> column with the <strong>median</strong> value for the entire column. Using median is appropriate when the missing values are of the “missing at random” variety. There might some bias in the data that introduces a “not at random” situation. We’ll look at that case momentarily.</p>
<p>Let’s look at a boxplot of the <strong>wt</strong> feature.</p>
<div class="sourceCode" id="cb342"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb342-1" data-line-number="1"><span class="kw">boxplot</span>(mtcars_na<span class="op">$</span>wt,<span class="dt">na.rm=</span><span class="ot">TRUE</span>,<span class="dt">main=</span><span class="st">&quot;Distribution of the wt feature&quot;</span>)</a>
<a class="sourceLine" id="cb342-2" data-line-number="2"><span class="kw">grid</span>()</a></code></pre></div>
<p><img src="biosml_files/figure-html/unnamed-chunk-81-1.png" width="672" /></p>
<p>To make the substitution would involve the following. First we need to find out which row numbers have missing values for the <strong>wt</strong> feature.</p>
<div class="sourceCode" id="cb343"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb343-1" data-line-number="1">(missing_wt_indices &lt;-<span class="st"> </span><span class="kw">which</span>(<span class="kw">is.na</span>(mtcars_na<span class="op">$</span>wt)))</a></code></pre></div>
<pre><code>## [1]  2  9 23</code></pre>
<p>Use this information with the data frame bracket notation see the rows where the NAs occur for the <strong>wt</strong> feature</p>
<div class="sourceCode" id="cb345"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb345-1" data-line-number="1">mtcars_na[missing_wt_indices,]</a></code></pre></div>
<pre><code>##     mpg cyl  disp  hp drat wt  qsec vs am gear carb
## 2  21.0   6 160.0 110 3.90 NA 17.02  0  1    4    4
## 9  22.8   4 140.8  95 3.92 NA 22.90  1  0    4    2
## 23 15.2   8 304.0 150 3.15 NA 17.30  0  0    3   NA</code></pre>
<p>Now do the replacement</p>
<div class="sourceCode" id="cb347"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb347-1" data-line-number="1">mtcars_na[missing_wt_indices,]<span class="op">$</span>wt &lt;-<span class="st"> </span><span class="kw">median</span>(mtcars_na<span class="op">$</span>wt,<span class="dt">na.rm=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<p>Verify that the replacement was done successfully. If so, then we should see the value of 3.44 in place of the previous NA values.</p>
<div class="sourceCode" id="cb348"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb348-1" data-line-number="1">mtcars_na[missing_wt_indices,]</a></code></pre></div>
<pre><code>##     mpg cyl  disp  hp drat   wt  qsec vs am gear carb
## 2  21.0   6 160.0 110 3.90 3.44 17.02  0  1    4    4
## 9  22.8   4 140.8  95 3.92 3.44 22.90  1  0    4    2
## 23 15.2   8 304.0 150 3.15 3.44 17.30  0  0    3   NA</code></pre>
</div>
<div id="package-based-approach" class="section level3">
<h3><span class="header-section-number">11.2.4</span> Package-based Approach</h3>
<p>This seems like a lot of work and maybe it is if you aren’t up to date with your R skills although, conceptually, this is straightforward and simple. The <strong>Hmisc</strong> package provides an easy way to do this:</p>
<div class="sourceCode" id="cb350"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb350-1" data-line-number="1"><span class="kw">suppressMessages</span>(<span class="kw">library</span>(Hmisc))</a>
<a class="sourceLine" id="cb350-2" data-line-number="2"><span class="co"># Reload the versio of mtcars with missing values</span></a>
<a class="sourceLine" id="cb350-3" data-line-number="3">url &lt;-<span class="st"> &quot;https://raw.githubusercontent.com/steviep42/utilities/master/data/mtcars_na.csv&quot;</span></a>
<a class="sourceLine" id="cb350-4" data-line-number="4">mtcars_na &lt;-<span class="st"> </span><span class="kw">read.csv</span>(url, <span class="dt">strip.white =</span> <span class="ot">FALSE</span>)</a></code></pre></div>
<p>The <strong>Hmisc</strong> package provides and <strong>impute</strong> function to do the work for us. Check it out. Notice how it finds the rows for which the <strong>wt</strong> feature is missing.</p>
<div class="sourceCode" id="cb351"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb351-1" data-line-number="1">Hmisc<span class="op">::</span><span class="kw">impute</span>(mtcars_na<span class="op">$</span>wt, median)</a></code></pre></div>
<pre><code>##      1      2      3      4      5      6      7      8      9     10     11     12     13 
##  2.620 3.440*  2.320  3.215  3.440  3.460  3.570  3.190 3.440*  3.440  3.440  4.070  3.730 
##     14     15     16     17     18     19     20     21     22     23     24     25     26 
##  3.780  5.250  5.424  5.345  2.200  1.615  1.835  2.465  3.520 3.440*  3.840  3.845  1.935 
##     27     28     29     30     31     32 
##  2.140  1.513  3.170  2.770  3.570  2.780</code></pre>
<p>To do the replacement is straightforward.</p>
<div class="sourceCode" id="cb353"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb353-1" data-line-number="1">mtcars_na<span class="op">$</span>wt &lt;-<span class="st"> </span>Hmisc<span class="op">::</span><span class="kw">impute</span>(mtcars_na<span class="op">$</span>wt,median)</a></code></pre></div>
</div>
<div id="using-caret" class="section level3">
<h3><span class="header-section-number">11.2.5</span> Using caret</h3>
<p>Another imputation approach is to use the K-Nearest Neighbors method to find observations that are similar to the ones that contain missing data. The missing values can then be filled using information from the most similar observations. We won’t go into that choosing rather to use the convenience offered by the <strong>caret</strong> package to help us.</p>
<div class="sourceCode" id="cb354"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb354-1" data-line-number="1"><span class="co"># Make sure caret is loaded</span></a>
<a class="sourceLine" id="cb354-2" data-line-number="2"><span class="co"># library(caret)</span></a></code></pre></div>
<p>So if we choose to use caret we can use the <strong>preProcess</strong> function to signal our intent to use imputation - in this case the K-Nearest Neighbors technique. KNN imputation is particularly useful for dealing with the “not at random” situation where there could be bias in the way missing values occur. This approach looks at similar observations to those containing missing values which means that it will attempt to fill in missing values as a function of a number of variables as opposed to just one.</p>
<p>One subtlety here is that <strong>caret</strong> requires us to use an alternative to the formula interface (e.g. mpg ~ .) approach when using the <strong>train</strong> function - at least the version of <strong>caret</strong> that I am currently using.</p>
<div class="sourceCode" id="cb355"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb355-1" data-line-number="1"><span class="co"># Get a fresh copy of the mtcars_na data frame</span></a>
<a class="sourceLine" id="cb355-2" data-line-number="2">mtcars_na &lt;-<span class="st"> </span><span class="kw">read.csv</span>(url,<span class="dt">stringsAsFactors =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb355-3" data-line-number="3"></a>
<a class="sourceLine" id="cb355-4" data-line-number="4"><span class="co"># Set the seed for reproducibility</span></a>
<a class="sourceLine" id="cb355-5" data-line-number="5"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb355-6" data-line-number="6"></a>
<a class="sourceLine" id="cb355-7" data-line-number="7"><span class="co"># Get the indices for a 70/30 split</span></a>
<a class="sourceLine" id="cb355-8" data-line-number="8">train_idx &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(mtcars_na<span class="op">$</span>mpg, <span class="dt">p=</span>.<span class="dv">70</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb355-9" data-line-number="9"></a>
<a class="sourceLine" id="cb355-10" data-line-number="10"><span class="co"># Split into a test / train pair</span></a>
<a class="sourceLine" id="cb355-11" data-line-number="11">train &lt;-<span class="st"> </span>mtcars[ train_idx,]</a>
<a class="sourceLine" id="cb355-12" data-line-number="12">test  &lt;-<span class="st"> </span>mtcars[<span class="op">-</span>train_idx,]</a>
<a class="sourceLine" id="cb355-13" data-line-number="13"></a>
<a class="sourceLine" id="cb355-14" data-line-number="14"><span class="co"># Note how we specify knnImpute. Another other option includes</span></a>
<a class="sourceLine" id="cb355-15" data-line-number="15"><span class="co"># medianImpute </span></a>
<a class="sourceLine" id="cb355-16" data-line-number="16"></a>
<a class="sourceLine" id="cb355-17" data-line-number="17">X &lt;-<span class="st"> </span>train[,<span class="op">-</span><span class="dv">1</span>]  <span class="co"># Use Every column EXCEPT mpg</span></a>
<a class="sourceLine" id="cb355-18" data-line-number="18">Y &lt;-<span class="st"> </span>train<span class="op">$</span>mpg   <span class="co"># This is what we want to predict</span></a>
<a class="sourceLine" id="cb355-19" data-line-number="19"></a>
<a class="sourceLine" id="cb355-20" data-line-number="20">lmFit &lt;-<span class="st"> </span><span class="kw">train</span>(X,Y,</a>
<a class="sourceLine" id="cb355-21" data-line-number="21">              <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>,</a>
<a class="sourceLine" id="cb355-22" data-line-number="22">              <span class="dt">metric =</span> <span class="st">&quot;RMSE&quot;</span>,</a>
<a class="sourceLine" id="cb355-23" data-line-number="23">              <span class="dt">preProcess =</span> <span class="st">&quot;knnImpute&quot;</span>)</a></code></pre></div>
<pre><code>## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit may be misleading</code></pre>
<div class="sourceCode" id="cb357"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb357-1" data-line-number="1">lmFit</a></code></pre></div>
<pre><code>## Linear Regression 
## 
## 24 samples
## 10 predictors
## 
## Pre-processing: nearest neighbor imputation (10), centered (10), scaled (10) 
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 24, 24, 24, 24, 24, 24, ... 
## Resampling results:
## 
##   RMSE      Rsquared   MAE    
##   8.723271  0.3681689  6.47164
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<div class="sourceCode" id="cb359"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb359-1" data-line-number="1"><span class="co"># See the RMSE for the test data</span></a>
<a class="sourceLine" id="cb359-2" data-line-number="2">preds &lt;-<span class="st"> </span><span class="kw">predict</span>(lmFit, test)</a>
<a class="sourceLine" id="cb359-3" data-line-number="3">Metrics<span class="op">::</span><span class="kw">rmse</span>(test<span class="op">$</span>mpg,preds)</a></code></pre></div>
<pre><code>## [1] 3.98902</code></pre>
</div>
</div>
<div id="scaling" class="section level2">
<h2><span class="header-section-number">11.3</span> Scaling</h2>
<p>In terms of what methods benefit (or require) you to scale data prior to use, consider that any method that uses the idea of “distance” will require this. This helps address the situation wherein one the size and range of one feature might overshadow another. For example, look at the range of features in the <strong>mtcars</strong> dataframe. Not only are the variables on different scales (e.g. MPG vs Weight vs Horse Power), a feature such as displacement might over influence a distance calculation when compared to qsec.</p>
<div class="sourceCode" id="cb361"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb361-1" data-line-number="1"><span class="kw">sapply</span>(mtcars,range)</a></code></pre></div>
<pre><code>##       mpg cyl  disp  hp drat    wt qsec vs am gear carb
## [1,] 10.4   4  71.1  52 2.76 1.513 14.5  0  0    3    1
## [2,] 33.9   8 472.0 335 4.93 5.424 22.9  1  1    5    8</code></pre>
<div id="methods-that-benefit-from-scaling" class="section level3">
<h3><span class="header-section-number">11.3.1</span> Methods That Benefit From Scaling</h3>
<p>The following approaches benefit from scaling:</p>
<p>Linear/non-linear regression, logistic regression, KNN, SVM, Neural Networks, clustering algorithms like k-means clustering. Methods that employ PCA and dimensionality reduction should use scaled data.</p>
<p>In R and Python, some of the individual functions might have arguments to activate the scaling as part of the process.</p>
</div>
<div id="methods-that-do-not-require-scaling" class="section level3">
<h3><span class="header-section-number">11.3.2</span> Methods That Do Not Require Scaling</h3>
<p>Methods that don’t require scaling (or whose results don’t rely upon it) include rule-based algorithms such as Decision trees and more generally CART - Random Forests, Gradient Boosted Decision Even if you scale the data the relative relationships will be preserved post scaling so the decision to split a tree won’t be impacted.</p>
</div>
<div id="how-to-scale" class="section level3">
<h3><span class="header-section-number">11.3.3</span> How To Scale</h3>
<p>You could do your own scaling and centering which might be helpful to understand what is going on. First, when we say “scaling” we typically mean “centering” and “scaling”.</p>
<div id="centering" class="section level4">
<h4><span class="header-section-number">11.3.3.1</span> Centering</h4>
<p>Take a vector (or column) of numeric data and find the mean. Then subtract the computed mean value from each element of the vector / column. We will use the <strong>wt</strong> column from mtcars as an example.</p>
<div class="sourceCode" id="cb363"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb363-1" data-line-number="1">(centered &lt;-<span class="st"> </span>mtcars<span class="op">$</span>wt <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(mtcars<span class="op">$</span>wt))</a></code></pre></div>
<pre><code>##  [1] -0.59725 -0.34225 -0.89725 -0.00225  0.22275  0.24275  0.35275 -0.02725 -0.06725  0.22275
## [11]  0.22275  0.85275  0.51275  0.56275  2.03275  2.20675  2.12775 -1.01725 -1.60225 -1.38225
## [21] -0.75225  0.30275  0.21775  0.62275  0.62775 -1.28225 -1.07725 -1.70425 -0.04725 -0.44725
## [31]  0.35275 -0.43725</code></pre>
<div class="sourceCode" id="cb365"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb365-1" data-line-number="1"><span class="co">#center &lt;- apply(mtcars,2,function(x) (x - mean(x)))</span></a></code></pre></div>
</div>
<div id="scaling-1" class="section level4">
<h4><span class="header-section-number">11.3.3.2</span> Scaling</h4>
<p>This step involves taking the standard deviation of the vector / column. Then divide each value in the vector / column by this computed standard deviation. We’ll process the <strong>centered</strong> data from the previous computation.</p>
<div class="sourceCode" id="cb366"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb366-1" data-line-number="1">(scaled &lt;-<span class="st"> </span>centered<span class="op">/</span><span class="kw">sd</span>(centered))</a></code></pre></div>
<pre><code>##  [1] -0.610399567 -0.349785269 -0.917004624 -0.002299538  0.227654255  0.248094592  0.360516446
##  [8] -0.027849959 -0.068730634  0.227654255  0.227654255  0.871524874  0.524039143  0.575139986
## [15]  2.077504765  2.255335698  2.174596366 -1.039646647 -1.637526508 -1.412682800 -0.768812180
## [22]  0.309415603  0.222544170  0.636460997  0.641571082 -1.310481114 -1.100967659 -1.741772228
## [29] -0.048290296 -0.457097039  0.360516446 -0.446876870</code></pre>
</div>
<div id="scaling-a-data-frame" class="section level4">
<h4><span class="header-section-number">11.3.3.3</span> Scaling a Data frame</h4>
<p>We could continue to do this by hand</p>
<div class="sourceCode" id="cb368"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb368-1" data-line-number="1">centered &lt;-<span class="st"> </span><span class="kw">apply</span>(mtcars,<span class="dv">2</span>,<span class="cf">function</span>(x) (x <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(x)))</a>
<a class="sourceLine" id="cb368-2" data-line-number="2">scaled &lt;-<span class="st"> </span><span class="kw">apply</span>(centered,<span class="dv">2</span>,<span class="cf">function</span>(x) x<span class="op">/</span><span class="kw">sd</span>(x))</a></code></pre></div>
<p>However, there is a function in R called <strong>scale</strong> which will do this for us. It has the added benefit of providing some attributes in the output that we could later use to <strong>de-scale</strong> the scaled data. This is useful if we build a model with scaled data because the predictions will be in terms of the scaled data which might not make sense to a third party. So you would then need to scale the predictions back into the units of the original data.</p>
<p>Here is how the scale function works:</p>
<div class="sourceCode" id="cb369"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb369-1" data-line-number="1">(smtcars &lt;-<span class="st"> </span><span class="kw">scale</span>(mtcars))</a></code></pre></div>
<pre><code>##                             mpg        cyl        disp          hp        drat           wt
## Mazda RX4            0.15088482 -0.1049878 -0.57061982 -0.53509284  0.56751369 -0.610399567
## Mazda RX4 Wag        0.15088482 -0.1049878 -0.57061982 -0.53509284  0.56751369 -0.349785269
## Datsun 710           0.44954345 -1.2248578 -0.99018209 -0.78304046  0.47399959 -0.917004624
## Hornet 4 Drive       0.21725341 -0.1049878  0.22009369 -0.53509284 -0.96611753 -0.002299538
## Hornet Sportabout   -0.23073453  1.0148821  1.04308123  0.41294217 -0.83519779  0.227654255
## Valiant             -0.33028740 -0.1049878 -0.04616698 -0.60801861 -1.56460776  0.248094592
## Duster 360          -0.96078893  1.0148821  1.04308123  1.43390296 -0.72298087  0.360516446
## Merc 240D            0.71501778 -1.2248578 -0.67793094 -1.23518023  0.17475447 -0.027849959
## Merc 230             0.44954345 -1.2248578 -0.72553512 -0.75387015  0.60491932 -0.068730634
## Merc 280            -0.14777380 -0.1049878 -0.50929918 -0.34548584  0.60491932  0.227654255
## Merc 280C           -0.38006384 -0.1049878 -0.50929918 -0.34548584  0.60491932  0.227654255
## Merc 450SE          -0.61235388  1.0148821  0.36371309  0.48586794 -0.98482035  0.871524874
## Merc 450SL          -0.46302456  1.0148821  0.36371309  0.48586794 -0.98482035  0.524039143
## Merc 450SLC         -0.81145962  1.0148821  0.36371309  0.48586794 -0.98482035  0.575139986
## Cadillac Fleetwood  -1.60788262  1.0148821  1.94675381  0.85049680 -1.24665983  2.077504765
## Lincoln Continental -1.60788262  1.0148821  1.84993175  0.99634834 -1.11574009  2.255335698
## Chrysler Imperial   -0.89442035  1.0148821  1.68856165  1.21512565 -0.68557523  2.174596366
## Fiat 128             2.04238943 -1.2248578 -1.22658929 -1.17683962  0.90416444 -1.039646647
## Honda Civic          1.71054652 -1.2248578 -1.25079481 -1.38103178  2.49390411 -1.637526508
## Toyota Corolla       2.29127162 -1.2248578 -1.28790993 -1.19142477  1.16600392 -1.412682800
## Toyota Corona        0.23384555 -1.2248578 -0.89255318 -0.72469984  0.19345729 -0.768812180
## Dodge Challenger    -0.76168319  1.0148821  0.70420401  0.04831332 -1.56460776  0.309415603
## AMC Javelin         -0.81145962  1.0148821  0.59124494  0.04831332 -0.83519779  0.222544170
## Camaro Z28          -1.12671039  1.0148821  0.96239618  1.43390296  0.24956575  0.636460997
## Pontiac Firebird    -0.14777380  1.0148821  1.36582144  0.41294217 -0.96611753  0.641571082
## Fiat X1-9            1.19619000 -1.2248578 -1.22416874 -1.17683962  0.90416444 -1.310481114
## Porsche 914-2        0.98049211 -1.2248578 -0.89093948 -0.81221077  1.55876313 -1.100967659
## Lotus Europa         1.71054652 -1.2248578 -1.09426581 -0.49133738  0.32437703 -1.741772228
## Ford Pantera L      -0.71190675  1.0148821  0.97046468  1.71102089  1.16600392 -0.048290296
## Ferrari Dino        -0.06481307 -0.1049878 -0.69164740  0.41294217  0.04383473 -0.457097039
## Maserati Bora       -0.84464392  1.0148821  0.56703942  2.74656682 -0.10578782  0.360516446
## Volvo 142E           0.21725341 -1.2248578 -0.88529152 -0.54967799  0.96027290 -0.446876870
##                            qsec         vs         am       gear       carb
## Mazda RX4           -0.77716515 -0.8680278  1.1899014  0.4235542  0.7352031
## Mazda RX4 Wag       -0.46378082 -0.8680278  1.1899014  0.4235542  0.7352031
## Datsun 710           0.42600682  1.1160357  1.1899014  0.4235542 -1.1221521
## Hornet 4 Drive       0.89048716  1.1160357 -0.8141431 -0.9318192 -1.1221521
## Hornet Sportabout   -0.46378082 -0.8680278 -0.8141431 -0.9318192 -0.5030337
## Valiant              1.32698675  1.1160357 -0.8141431 -0.9318192 -1.1221521
## Duster 360          -1.12412636 -0.8680278 -0.8141431 -0.9318192  0.7352031
## Merc 240D            1.20387148  1.1160357 -0.8141431  0.4235542 -0.5030337
## Merc 230             2.82675459  1.1160357 -0.8141431  0.4235542 -0.5030337
## Merc 280             0.25252621  1.1160357 -0.8141431  0.4235542  0.7352031
## Merc 280C            0.58829513  1.1160357 -0.8141431  0.4235542  0.7352031
## Merc 450SE          -0.25112717 -0.8680278 -0.8141431 -0.9318192  0.1160847
## Merc 450SL          -0.13920420 -0.8680278 -0.8141431 -0.9318192  0.1160847
## Merc 450SLC          0.08464175 -0.8680278 -0.8141431 -0.9318192  0.1160847
## Cadillac Fleetwood   0.07344945 -0.8680278 -0.8141431 -0.9318192  0.7352031
## Lincoln Continental -0.01608893 -0.8680278 -0.8141431 -0.9318192  0.7352031
## Chrysler Imperial   -0.23993487 -0.8680278 -0.8141431 -0.9318192  0.7352031
## Fiat 128             0.90727560  1.1160357  1.1899014  0.4235542 -1.1221521
## Honda Civic          0.37564148  1.1160357  1.1899014  0.4235542 -0.5030337
## Toyota Corolla       1.14790999  1.1160357  1.1899014  0.4235542 -1.1221521
## Toyota Corona        1.20946763  1.1160357 -0.8141431 -0.9318192 -1.1221521
## Dodge Challenger    -0.54772305 -0.8680278 -0.8141431 -0.9318192 -0.5030337
## AMC Javelin         -0.30708866 -0.8680278 -0.8141431 -0.9318192 -0.5030337
## Camaro Z28          -1.36476075 -0.8680278 -0.8141431 -0.9318192  0.7352031
## Pontiac Firebird    -0.44699237 -0.8680278 -0.8141431 -0.9318192 -0.5030337
## Fiat X1-9            0.58829513  1.1160357  1.1899014  0.4235542 -1.1221521
## Porsche 914-2       -0.64285758 -0.8680278  1.1899014  1.7789276 -0.5030337
## Lotus Europa        -0.53093460  1.1160357  1.1899014  1.7789276 -0.5030337
## Ford Pantera L      -1.87401028 -0.8680278  1.1899014  1.7789276  0.7352031
## Ferrari Dino        -1.31439542 -0.8680278  1.1899014  1.7789276  1.9734398
## Maserati Bora       -1.81804880 -0.8680278  1.1899014  1.7789276  3.2116766
## Volvo 142E           0.42041067  1.1160357  1.1899014  0.4235542 -0.5030337
## attr(,&quot;scaled:center&quot;)
##        mpg        cyl       disp         hp       drat         wt       qsec         vs 
##  20.090625   6.187500 230.721875 146.687500   3.596563   3.217250  17.848750   0.437500 
##         am       gear       carb 
##   0.406250   3.687500   2.812500 
## attr(,&quot;scaled:scale&quot;)
##         mpg         cyl        disp          hp        drat          wt        qsec          vs 
##   6.0269481   1.7859216 123.9386938  68.5628685   0.5346787   0.9784574   1.7869432   0.5040161 
##          am        gear        carb 
##   0.4989909   0.7378041   1.6152000</code></pre>
<p>The last rows make reference to <strong>attributes</strong> which can be considered as “meta information”. The presence of this information does not prevent you from using the scaled data for other operations. But if you needed to de-scaled the data you would need the standard deviation for each column and the mean of each column to reverse the centering and scaling process. That info is stashed in the attributes:</p>
<div class="sourceCode" id="cb371"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb371-1" data-line-number="1"><span class="kw">attr</span>(smtcars,<span class="st">&quot;scaled:scale&quot;</span>)</a></code></pre></div>
<pre><code>##         mpg         cyl        disp          hp        drat          wt        qsec          vs 
##   6.0269481   1.7859216 123.9386938  68.5628685   0.5346787   0.9784574   1.7869432   0.5040161 
##          am        gear        carb 
##   0.4989909   0.7378041   1.6152000</code></pre>
<div class="sourceCode" id="cb373"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb373-1" data-line-number="1"><span class="kw">attr</span>(smtcars,<span class="st">&quot;scaled:center&quot;</span>)</a></code></pre></div>
<pre><code>##        mpg        cyl       disp         hp       drat         wt       qsec         vs 
##  20.090625   6.187500 230.721875 146.687500   3.596563   3.217250  17.848750   0.437500 
##         am       gear       carb 
##   0.406250   3.687500   2.812500</code></pre>
<p>If we wanted to de-scaled the scaled data frame:</p>
<div class="sourceCode" id="cb375"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb375-1" data-line-number="1">unscaledmt &lt;-<span class="st"> </span><span class="kw">sweep</span>(smtcars,<span class="dv">2</span>,<span class="kw">attr</span>(smtcars,<span class="st">&quot;scaled:scale&quot;</span>),<span class="st">&quot;*&quot;</span>)</a>
<a class="sourceLine" id="cb375-2" data-line-number="2">uncentermt &lt;-<span class="st"> </span><span class="kw">sweep</span>(unscaledmt,<span class="dv">2</span>,<span class="kw">attr</span>(smtcars,<span class="st">&quot;scaled:center&quot;</span>),<span class="st">&quot;+&quot;</span>)</a>
<a class="sourceLine" id="cb375-3" data-line-number="3"></a>
<a class="sourceLine" id="cb375-4" data-line-number="4"><span class="co"># Comapre them</span></a>
<a class="sourceLine" id="cb375-5" data-line-number="5"></a>
<a class="sourceLine" id="cb375-6" data-line-number="6">uncentermt[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>,]</a></code></pre></div>
<pre><code>##                mpg cyl disp  hp drat    wt  qsec vs am gear carb
## Mazda RX4     21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
## Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
## Datsun 710    22.8   4  108  93 3.85 2.320 18.61  1  1    4    1</code></pre>
<div class="sourceCode" id="cb377"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb377-1" data-line-number="1">mtcars[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>,]</a></code></pre></div>
<pre><code>##                mpg cyl disp  hp drat    wt  qsec vs am gear carb
## Mazda RX4     21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
## Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
## Datsun 710    22.8   4  108  93 3.85 2.320 18.61  1  1    4    1</code></pre>
<p>Ugh. What a pain ! But, if we wanted to use the scaled data to build a model, here is what we would do:</p>
<div class="sourceCode" id="cb379"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb379-1" data-line-number="1">smtcars &lt;-<span class="st"> </span><span class="kw">scale</span>(mtcars)</a>
<a class="sourceLine" id="cb379-2" data-line-number="2">mtcars_center_scaled &lt;-<span class="st"> </span><span class="kw">data.frame</span>(smtcars)</a>
<a class="sourceLine" id="cb379-3" data-line-number="3">myLm &lt;-<span class="st"> </span><span class="kw">lm</span>(mpg<span class="op">~</span>.,</a>
<a class="sourceLine" id="cb379-4" data-line-number="4">           <span class="dt">data =</span> mtcars_center_scaled)</a>
<a class="sourceLine" id="cb379-5" data-line-number="5"></a>
<a class="sourceLine" id="cb379-6" data-line-number="6"><span class="co"># Do some predicting. </span></a>
<a class="sourceLine" id="cb379-7" data-line-number="7">preds &lt;-<span class="st"> </span><span class="kw">predict</span>(myLm,mtcars_center_scaled)</a>
<a class="sourceLine" id="cb379-8" data-line-number="8"></a>
<a class="sourceLine" id="cb379-9" data-line-number="9"><span class="co"># The results are scaled. How to get them back to unscaled</span></a>
<a class="sourceLine" id="cb379-10" data-line-number="10">preds[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>]</a></code></pre></div>
<pre><code>##         Mazda RX4     Mazda RX4 Wag        Datsun 710    Hornet 4 Drive Hornet Sportabout 
##         0.4162772         0.3353706         1.0220793         0.1902753        -0.3977454</code></pre>
<p>So how would we re express this information in terms of the unscaled data ? It’s tedious but doable. We leverage the fact that the scaled version of the data has attributes we can use to convert the predictions back to their unscaled form.</p>
<div class="sourceCode" id="cb381"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb381-1" data-line-number="1">(unscaled_preds &lt;-<span class="st"> </span></a>
<a class="sourceLine" id="cb381-2" data-line-number="2"><span class="st">   </span>preds <span class="op">*</span><span class="st"> </span><span class="kw">attr</span>(smtcars,<span class="st">&quot;scaled:scale&quot;</span>)[<span class="st">&#39;mpg&#39;</span>] <span class="op">+</span><span class="st"> </span><span class="kw">attr</span>(smtcars,<span class="st">&quot;scaled:center&quot;</span>)[<span class="st">&#39;mpg&#39;</span>])</a></code></pre></div>
<pre><code>##           Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive 
##            22.59951            22.11189            26.25064            21.23740 
##   Hornet Sportabout             Valiant          Duster 360           Merc 240D 
##            17.69343            20.38304            14.38626            22.49601 
##            Merc 230            Merc 280           Merc 280C          Merc 450SE 
##            24.41909            18.69903            19.19165            14.17216 
##          Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental 
##            15.59957            15.74222            12.03401            10.93644 
##   Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla 
##            10.49363            27.77291            29.89674            29.51237 
##       Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28 
##            23.64310            16.94305            17.73218            13.30602 
##    Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa 
##            16.69168            28.29347            26.15295            27.63627 
##      Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E 
##            18.87004            19.69383            13.94112            24.36827</code></pre>
<p>Does this match what we would get had we not scaled the data in the first place ?</p>
<div class="sourceCode" id="cb383"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb383-1" data-line-number="1">myLm &lt;-<span class="st"> </span><span class="kw">lm</span>(mpg<span class="op">~</span>.,</a>
<a class="sourceLine" id="cb383-2" data-line-number="2">           <span class="dt">data =</span> mtcars)</a>
<a class="sourceLine" id="cb383-3" data-line-number="3"></a>
<a class="sourceLine" id="cb383-4" data-line-number="4"><span class="co"># Do some predicting. </span></a>
<a class="sourceLine" id="cb383-5" data-line-number="5">(preds &lt;-<span class="st"> </span><span class="kw">predict</span>(myLm,mtcars))</a></code></pre></div>
<pre><code>##           Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive 
##            22.59951            22.11189            26.25064            21.23740 
##   Hornet Sportabout             Valiant          Duster 360           Merc 240D 
##            17.69343            20.38304            14.38626            22.49601 
##            Merc 230            Merc 280           Merc 280C          Merc 450SE 
##            24.41909            18.69903            19.19165            14.17216 
##          Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental 
##            15.59957            15.74222            12.03401            10.93644 
##   Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla 
##            10.49363            27.77291            29.89674            29.51237 
##       Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28 
##            23.64310            16.94305            17.73218            13.30602 
##    Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa 
##            16.69168            28.29347            26.15295            27.63627 
##      Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E 
##            18.87004            19.69383            13.94112            24.36827</code></pre>
<div class="sourceCode" id="cb385"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb385-1" data-line-number="1"><span class="kw">all.equal</span>(preds,unscaled_preds)</a></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
</div>
<div id="caret-and-preprocess" class="section level4">
<h4><span class="header-section-number">11.3.3.4</span> caret and preprocess</h4>
<p>But the <strong>caret</strong> package will help you do this using the <strong>preProcess</strong> function. Here we can actually request that the data is “centered” and “scaled” as part of the model assembly process. We could do this before the call to the <strong>Train</strong> function but then we would also have to convert the training and test data ourselves. In the following situation, the data will be centered and scaled though the returned RMSE will be in terms of the unscaled data.</p>
<div class="sourceCode" id="cb387"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb387-1" data-line-number="1">training_idx &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(mtcars<span class="op">$</span>mpg, </a>
<a class="sourceLine" id="cb387-2" data-line-number="2">                                    <span class="dt">p =</span>.<span class="dv">80</span>, <span class="dt">list=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb387-3" data-line-number="3">training &lt;-<span class="st"> </span>mtcars[training_idx,]</a>
<a class="sourceLine" id="cb387-4" data-line-number="4">test &lt;-<span class="st"> </span>mtcars[<span class="op">-</span>training_idx,]</a>
<a class="sourceLine" id="cb387-5" data-line-number="5"></a>
<a class="sourceLine" id="cb387-6" data-line-number="6">myLm &lt;-<span class="st">  </span><span class="kw">train</span>(mpg <span class="op">~</span><span class="st"> </span>.,</a>
<a class="sourceLine" id="cb387-7" data-line-number="7">               <span class="dt">data =</span> training,</a>
<a class="sourceLine" id="cb387-8" data-line-number="8">               <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>,</a>
<a class="sourceLine" id="cb387-9" data-line-number="9">               <span class="dt">preProcess =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>,<span class="st">&quot;scale&quot;</span>)</a>
<a class="sourceLine" id="cb387-10" data-line-number="10">          )</a></code></pre></div>
<pre><code>## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit may be misleading</code></pre>
<p>To verify that the data is being centered and scaled within the call the <strong>train</strong> function, checkout the <strong>myLm</strong> object:</p>
<div class="sourceCode" id="cb389"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb389-1" data-line-number="1"><span class="co"># This </span></a>
<a class="sourceLine" id="cb389-2" data-line-number="2">myLm<span class="op">$</span>finalModel<span class="op">$</span>model[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,]</a></code></pre></div>
<pre><code>##                   .outcome        cyl       disp         hp       drat          wt       qsec
## Mazda.RX4             21.0 -0.1169061 -0.5638891 -0.5495661  0.5813586 -0.60017925 -0.7564127
## Datsun.710            22.8 -1.2080299 -0.9832489 -0.7922316  0.4848103 -0.91982835  0.4135782
## Hornet.4.Drive        21.4 -0.1169061  0.2264428 -0.5495661 -1.0020333  0.03379148  0.8652490
## Hornet.Sportabout     18.7  0.9742177  1.0490331  0.3782728 -0.8668657  0.27352831 -0.4516709
## Duster.360            14.3  0.9742177  1.0490331  1.3774838 -0.7510078  0.41204292 -1.0938054
##                           vs         am       gear       carb
## Mazda.RX4         -0.9141741  1.2207620  0.4446792  0.7113900
## Datsun.710         1.0548163  1.2207620  0.4446792 -1.0994209
## Hornet.4.Drive     1.0548163 -0.7899048 -0.9387672 -1.0994209
## Hornet.Sportabout -0.9141741 -0.7899048 -0.9387672 -0.4958173
## Duster.360        -0.9141741 -0.7899048 -0.9387672  0.7113900</code></pre>
<div class="sourceCode" id="cb391"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb391-1" data-line-number="1"><span class="co"># matches this</span></a>
<a class="sourceLine" id="cb391-2" data-line-number="2"><span class="kw">scale</span>(training)[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,]</a></code></pre></div>
<pre><code>##                          mpg        cyl       disp         hp       drat          wt       qsec
## Mazda RX4          0.1421120 -0.1169061 -0.5638891 -0.5495661  0.5813586 -0.60017925 -0.7564127
## Datsun 710         0.4297605 -1.2080299 -0.9832489 -0.7922316  0.4848103 -0.91982835  0.4135782
## Hornet 4 Drive     0.2060339 -0.1169061  0.2264428 -0.5495661 -1.0020333  0.03379148  0.8652490
## Hornet Sportabout -0.2254388  0.9742177  1.0490331  0.3782728 -0.8668657  0.27352831 -0.4516709
## Duster 360        -0.9285795  0.9742177  1.0490331  1.3774838 -0.7510078  0.41204292 -1.0938054
##                           vs         am       gear       carb
## Mazda RX4         -0.9141741  1.2207620  0.4446792  0.7113900
## Datsun 710         1.0548163  1.2207620  0.4446792 -1.0994209
## Hornet 4 Drive     1.0548163 -0.7899048 -0.9387672 -1.0994209
## Hornet Sportabout -0.9141741 -0.7899048 -0.9387672 -0.4958173
## Duster 360        -0.9141741 -0.7899048 -0.9387672  0.7113900</code></pre>
<p>This is convenient because if we now wish to use the <strong>predict</strong> function, it will scale the <strong>test</strong> data that we provide for use with the <strong>predict</strong> function. What we get back will be in terms of the uncenter and unscaled predicted variable (mpg). We do not have to suffer through the conversion reverse scaling process ourselves.</p>
<div class="sourceCode" id="cb393"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb393-1" data-line-number="1">(preds &lt;-<span class="st"> </span><span class="kw">predict</span>(myLm,test))</a></code></pre></div>
<pre><code>##     Mazda RX4 Wag           Valiant Chrysler Imperial     Porsche 914-2 
##         22.125590         21.707846          8.500784         24.215849</code></pre>
</div>
</div>
<div id="order-of-processing" class="section level3">
<h3><span class="header-section-number">11.3.4</span> Order of Processing</h3>
<p>Note that the order of processing is important. You should center and then scale. Underneath the hood, the centering operation finds the mean of a feature (column) and then subtracts it from each value therein. So the following are equivalent:</p>
<div class="sourceCode" id="cb395"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb395-1" data-line-number="1">(mt_center &lt;-<span class="st"> </span>mtcars<span class="op">$</span>wt <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(mtcars<span class="op">$</span>wt))</a></code></pre></div>
<pre><code>##  [1] -0.59725 -0.34225 -0.89725 -0.00225  0.22275  0.24275  0.35275 -0.02725 -0.06725  0.22275
## [11]  0.22275  0.85275  0.51275  0.56275  2.03275  2.20675  2.12775 -1.01725 -1.60225 -1.38225
## [21] -0.75225  0.30275  0.21775  0.62275  0.62775 -1.28225 -1.07725 -1.70425 -0.04725 -0.44725
## [31]  0.35275 -0.43725</code></pre>
<div class="sourceCode" id="cb397"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb397-1" data-line-number="1"><span class="co">#</span></a>
<a class="sourceLine" id="cb397-2" data-line-number="2"></a>
<a class="sourceLine" id="cb397-3" data-line-number="3"><span class="kw">as.vector</span>(<span class="kw">scale</span>(mtcars<span class="op">$</span>wt,<span class="dt">center=</span>T,<span class="dt">scale=</span>F))</a></code></pre></div>
<pre><code>##  [1] -0.59725 -0.34225 -0.89725 -0.00225  0.22275  0.24275  0.35275 -0.02725 -0.06725  0.22275
## [11]  0.22275  0.85275  0.51275  0.56275  2.03275  2.20675  2.12775 -1.01725 -1.60225 -1.38225
## [21] -0.75225  0.30275  0.21775  0.62275  0.62775 -1.28225 -1.07725 -1.70425 -0.04725 -0.44725
## [31]  0.35275 -0.43725</code></pre>
<p>After centering, the scaling is done by dividing the (centered) columns of the data frame by their respective standard deviations. In terms of dealing with missing data and scaling, one should first do imputation followed by centering and scaling. In a call to the <strong>train</strong> function, this would look like the following. We’ll use our version of mtcars that has missing data.</p>
<div class="sourceCode" id="cb399"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb399-1" data-line-number="1">url &lt;-<span class="st"> &quot;https://raw.githubusercontent.com/steviep42/utilities/master/data/mtcars_na.csv&quot;</span></a>
<a class="sourceLine" id="cb399-2" data-line-number="2"></a>
<a class="sourceLine" id="cb399-3" data-line-number="3">mtcars_na &lt;-<span class="st"> </span><span class="kw">read.csv</span>(url,<span class="dt">stringsAsFactors =</span> <span class="ot">FALSE</span>)</a></code></pre></div>
<div class="sourceCode" id="cb400"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb400-1" data-line-number="1">training_idx &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(mtcars_na<span class="op">$</span>mpg, <span class="dt">p =</span>.<span class="dv">80</span>, <span class="dt">list=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb400-2" data-line-number="2">training &lt;-<span class="st"> </span>mtcars_na[training_idx,]</a>
<a class="sourceLine" id="cb400-3" data-line-number="3">test &lt;-<span class="st"> </span>mtcars_na[<span class="op">-</span>training_idx,]</a>
<a class="sourceLine" id="cb400-4" data-line-number="4"></a>
<a class="sourceLine" id="cb400-5" data-line-number="5">X &lt;-<span class="st"> </span>training[,<span class="op">-</span><span class="dv">1</span>]</a>
<a class="sourceLine" id="cb400-6" data-line-number="6">Y &lt;-<span class="st"> </span>training<span class="op">$</span>mpg </a>
<a class="sourceLine" id="cb400-7" data-line-number="7"></a>
<a class="sourceLine" id="cb400-8" data-line-number="8">myLm &lt;-<span class="st">  </span><span class="kw">train</span>(X, Y,</a>
<a class="sourceLine" id="cb400-9" data-line-number="9">                <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>,</a>
<a class="sourceLine" id="cb400-10" data-line-number="10">                <span class="dt">preProcess =</span> <span class="kw">c</span>(<span class="st">&quot;knnImpute&quot;</span>,<span class="st">&quot;center&quot;</span>,<span class="st">&quot;scale&quot;</span>)</a>
<a class="sourceLine" id="cb400-11" data-line-number="11">          )</a></code></pre></div>
<pre><code>## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit may be misleading

## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit may be misleading</code></pre>
</div>
</div>
<div id="low-variance-variables" class="section level2">
<h2><span class="header-section-number">11.4</span> Low Variance Variables</h2>
<p>Some variables exhibit low variance and might be nearly constant. Such variables can be detected by using some basic functions in R before you begin to build a model. As an example, we’ll use the mtcars data frame and introduce a low variance variable - actually, we’ll make it a constant.</p>
<div class="sourceCode" id="cb402"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb402-1" data-line-number="1"><span class="kw">data</span>(mtcars)</a>
<a class="sourceLine" id="cb402-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb402-3" data-line-number="3"></a>
<a class="sourceLine" id="cb402-4" data-line-number="4">mtcars_nzv &lt;-<span class="st"> </span>mtcars</a>
<a class="sourceLine" id="cb402-5" data-line-number="5"></a>
<a class="sourceLine" id="cb402-6" data-line-number="6"><span class="co"># Make drat low variance</span></a>
<a class="sourceLine" id="cb402-7" data-line-number="7">mtcars_nzv<span class="op">$</span>drat &lt;-<span class="st"> </span><span class="fl">3.0</span></a>
<a class="sourceLine" id="cb402-8" data-line-number="8"></a>
<a class="sourceLine" id="cb402-9" data-line-number="9"><span class="co"># Pretty low isn&#39;t it ?</span></a>
<a class="sourceLine" id="cb402-10" data-line-number="10"><span class="kw">var</span>(mtcars_nzv<span class="op">$</span>drat)</a></code></pre></div>
<pre><code>## [1] 0</code></pre>
<p>What if we use this variable when making a model. We’ll get a lot of problems. While this is a contrived example, it is possible to get this situation when using cross fold validation where the data is segmented into smaller subsets where a variable can be zero variance.</p>
<div class="sourceCode" id="cb404"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb404-1" data-line-number="1">x &lt;-<span class="st"> </span>mtcars_nzv[,<span class="op">-</span><span class="dv">1</span>]</a>
<a class="sourceLine" id="cb404-2" data-line-number="2">y &lt;-<span class="st"> </span>mtcars_nzv[,<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb404-3" data-line-number="3"></a>
<a class="sourceLine" id="cb404-4" data-line-number="4">lowVarLm &lt;-<span class="st"> </span><span class="kw">train</span>(</a>
<a class="sourceLine" id="cb404-5" data-line-number="5">  x,y,</a>
<a class="sourceLine" id="cb404-6" data-line-number="6">  <span class="dt">method=</span><span class="st">&quot;lm&quot;</span>,</a>
<a class="sourceLine" id="cb404-7" data-line-number="7">  <span class="dt">preProcess=</span><span class="kw">c</span>(<span class="st">&quot;center&quot;</span>,<span class="st">&quot;scale&quot;</span>)</a>
<a class="sourceLine" id="cb404-8" data-line-number="8">  )</a></code></pre></div>
<p>The <strong>caret</strong> package has an option to the <strong>preProcess</strong> argument that allows us to remove such variables so it won’t impact the resulting model. Notice that we remove the near zero variance variables before we center which happens before scaling. This is the recommended order.</p>
<div class="sourceCode" id="cb405"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb405-1" data-line-number="1">x &lt;-<span class="st"> </span>mtcars_nzv[,<span class="op">-</span><span class="dv">1</span>]</a>
<a class="sourceLine" id="cb405-2" data-line-number="2">y &lt;-<span class="st"> </span>mtcars_nzv[,<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb405-3" data-line-number="3"></a>
<a class="sourceLine" id="cb405-4" data-line-number="4">lowVarLm &lt;-<span class="st"> </span><span class="kw">train</span>(</a>
<a class="sourceLine" id="cb405-5" data-line-number="5">  x,y,</a>
<a class="sourceLine" id="cb405-6" data-line-number="6">  <span class="dt">method=</span><span class="st">&quot;lm&quot;</span>,</a>
<a class="sourceLine" id="cb405-7" data-line-number="7">  <span class="dt">preProcess=</span><span class="kw">c</span>(<span class="st">&quot;nzv&quot;</span>,<span class="st">&quot;center&quot;</span>,<span class="st">&quot;scale&quot;</span>)</a>
<a class="sourceLine" id="cb405-8" data-line-number="8">  )</a></code></pre></div>
<p>The above now works. There is a subtle consideration at work here. We could pre process the data with <strong>nzv</strong> or <strong>zv</strong> where the former removes “near” zero variance features and the latter removes constant-valued features. With near zero variance features there is a way to specify tolerance for deciding whether a feature has near zero variance. Think of <strong>nzv</strong> as being slightly more permissive and flexible whereas <strong>zv</strong> eliminates zero variance variables. Sometimes you might want to keep near zero variance features around simply because there could be some interesting information therein.</p>
<p>In general we could remove the constant or near zero variance features <strong>before</strong> passing the data to the <strong>train</strong> function. Caret has a standalone function called <strong>nearZeroVariance</strong> to do this. We’ll it doesn’t actually remove the feature but it will tell us which column(s) exhibit very low variance or have a constant value. In this case it is column number 5 which corresponds to <strong>drat</strong>. We already knew that.</p>
<div class="sourceCode" id="cb406"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb406-1" data-line-number="1"><span class="kw">nearZeroVar</span>(mtcars_nzv)</a></code></pre></div>
<pre><code>## [1] 5</code></pre>
<div class="sourceCode" id="cb408"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb408-1" data-line-number="1"><span class="co"># Find the nzv columns</span></a>
<a class="sourceLine" id="cb408-2" data-line-number="2"></a>
<a class="sourceLine" id="cb408-3" data-line-number="3">mtcars_nzv[<span class="kw">nearZeroVar</span>(mtcars_nzv)]</a></code></pre></div>
<pre><code>##                     drat
## Mazda RX4              3
## Mazda RX4 Wag          3
## Datsun 710             3
## Hornet 4 Drive         3
## Hornet Sportabout      3
## Valiant                3
## Duster 360             3
## Merc 240D              3
## Merc 230               3
## Merc 280               3
## Merc 280C              3
## Merc 450SE             3
## Merc 450SL             3
## Merc 450SLC            3
## Cadillac Fleetwood     3
## Lincoln Continental    3
## Chrysler Imperial      3
## Fiat 128               3
## Honda Civic            3
## Toyota Corolla         3
## Toyota Corona          3
## Dodge Challenger       3
## AMC Javelin            3
## Camaro Z28             3
## Pontiac Firebird       3
## Fiat X1-9              3
## Porsche 914-2          3
## Lotus Europa           3
## Ford Pantera L         3
## Ferrari Dino           3
## Maserati Bora          3
## Volvo 142E             3</code></pre>
<p>There is a data frame in the <strong>caret</strong> package which exhibits this behavior in a more organic fashion - that is the data measurements of a number of features are near zero variance.</p>
<div class="sourceCode" id="cb410"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb410-1" data-line-number="1"><span class="kw">data</span>(BloodBrain)</a>
<a class="sourceLine" id="cb410-2" data-line-number="2"><span class="kw">str</span>(bbbDescr,<span class="dv">0</span>)</a></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    208 obs. of  134 variables:</code></pre>
<div class="sourceCode" id="cb412"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb412-1" data-line-number="1"><span class="co"># Which columns are suspected of having near zero variance ? </span></a>
<a class="sourceLine" id="cb412-2" data-line-number="2"></a>
<a class="sourceLine" id="cb412-3" data-line-number="3"><span class="kw">nearZeroVar</span>(bbbDescr)</a></code></pre></div>
<pre><code>## [1]  3 16 17 22 25 50 60</code></pre>
<div class="sourceCode" id="cb414"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb414-1" data-line-number="1"><span class="co"># What are their names ?</span></a>
<a class="sourceLine" id="cb414-2" data-line-number="2"><span class="kw">names</span>(bbbDescr[,<span class="kw">nearZeroVar</span>(bbbDescr)])</a></code></pre></div>
<pre><code>## [1] &quot;negative&quot;     &quot;peoe_vsa.2.1&quot; &quot;peoe_vsa.3.1&quot; &quot;a_acid&quot;       &quot;vsa_acid&quot;     &quot;frac.anion7.&quot;
## [7] &quot;alert&quot;</code></pre>
</div>
<div id="pca---principal-components-analysis" class="section level2">
<h2><span class="header-section-number">11.5</span> PCA - Principal Components Analysis</h2>
<p>So one of the problems with data can be what is called multicollinearity
where high correlations exist between variables in a data set. Consider the mtcars data frame for example. Let’s assume that we want to predict whether a given car has an automatic transmission (0) or maial (1). We’ll remove other columns from the data frame that represent categorical data so we can focus on the continuous numeric variables.</p>
<div id="identify-the-factors" class="section level3">
<h3><span class="header-section-number">11.5.1</span> Identify The Factors</h3>
<div class="sourceCode" id="cb416"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb416-1" data-line-number="1"><span class="kw">sapply</span>(mtcars,<span class="cf">function</span>(x) <span class="kw">length</span>(<span class="kw">unique</span>(x)))</a></code></pre></div>
<pre><code>##  mpg  cyl disp   hp drat   wt qsec   vs   am gear carb 
##   25    3   27   22   22   29   30    2    2    3    6</code></pre>
<p>Let’s eliminate cyl,vs,gear, and carb. We’ll also remove the rownames since, if we don’t, then they will cause problems when we create the biplot of the principal compoments.</p>
<div class="sourceCode" id="cb418"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb418-1" data-line-number="1">mtcars_data &lt;-<span class="st"> </span>mtcars[,<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span><span class="op">:</span><span class="dv">7</span>,<span class="dv">9</span>)]</a>
<a class="sourceLine" id="cb418-2" data-line-number="2"><span class="kw">rownames</span>(mtcars_data) &lt;-<span class="st"> </span><span class="ot">NULL</span></a></code></pre></div>
<p>Now, let’s look at the correlation matrix to see if we have highly correlated variabes. We do have several correlations that exceed .7 which is sufficiently high to consider that they might cause problems when building models. The caret package has a findCorrelation function that can remove predictors that exhibit a correlation above a certain threshold but we’ll see how PCA can help - so we’ll leave them in for now.</p>
</div>
<div id="check-for-high-correlations" class="section level3">
<h3><span class="header-section-number">11.5.2</span> Check For High Correlations</h3>
<div class="sourceCode" id="cb419"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb419-1" data-line-number="1"><span class="co"># Get correlations just for the predictor variables</span></a>
<a class="sourceLine" id="cb419-2" data-line-number="2">DataExplorer<span class="op">::</span><span class="kw">plot_correlation</span>(mtcars_data[,<span class="op">-</span><span class="dv">7</span>])</a></code></pre></div>
<p><img src="biosml_files/figure-html/pcacorr-1.png" width="672" /></p>
<p>This is a graphic equivalent of this command:</p>
<div class="sourceCode" id="cb420"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb420-1" data-line-number="1"><span class="kw">cor</span>(mtcars_data[,<span class="op">-</span><span class="dv">7</span>])</a></code></pre></div>
<pre><code>##             mpg       disp         hp        drat         wt        qsec
## mpg   1.0000000 -0.8475514 -0.7761684  0.68117191 -0.8676594  0.41868403
## disp -0.8475514  1.0000000  0.7909486 -0.71021393  0.8879799 -0.43369788
## hp   -0.7761684  0.7909486  1.0000000 -0.44875912  0.6587479 -0.70822339
## drat  0.6811719 -0.7102139 -0.4487591  1.00000000 -0.7124406  0.09120476
## wt   -0.8676594  0.8879799  0.6587479 -0.71244065  1.0000000 -0.17471588
## qsec  0.4186840 -0.4336979 -0.7082234  0.09120476 -0.1747159  1.00000000</code></pre>
</div>
<div id="so-why-use-pca" class="section level3">
<h3><span class="header-section-number">11.5.3</span> So Why Use PCA ?</h3>
<p>There are several functions for doing Principal Components Analysis on this data. But why are we even thinking about PCA ? Well, it helps us deal with highly correlated data by <strong>reducing</strong> the dimensionality of a data set. In the mtcars data frame we don’t have that many variabesl / columns but wouldn’t it be nice to transform the data in a way that reduced the number of columns that we had to consider while also dealing with the multicollinrarity ?</p>
<p>This is what PCA can do for us. In reality we are using the eigenvectors of the covariance matrix of the original data. We use them to transform the original data into a reduced number of columns to consider. To get the ball rolling, we’ll use the <strong>prcomp</strong> function.</p>
<div class="sourceCode" id="cb422"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb422-1" data-line-number="1">scaled_mtcars_data &lt;-<span class="st"> </span><span class="kw">scale</span>(mtcars_data[,<span class="op">-</span><span class="dv">7</span>])</a>
<a class="sourceLine" id="cb422-2" data-line-number="2">prcs &lt;-<span class="st"> </span><span class="kw">princomp</span>(scaled_mtcars_data)</a>
<a class="sourceLine" id="cb422-3" data-line-number="3">prcs</a></code></pre></div>
<pre><code>## Call:
## princomp(x = scaled_mtcars_data)
## 
## Standard deviations:
##    Comp.1    Comp.2    Comp.3    Comp.4    Comp.5    Comp.6 
## 2.0140855 1.0546249 0.5682775 0.3866999 0.3477012 0.2243967 
## 
##  6  variables and  32 observations.</code></pre>
<div class="sourceCode" id="cb424"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb424-1" data-line-number="1"><span class="kw">summary</span>(prcs)</a></code></pre></div>
<pre><code>## Importance of components:
##                           Comp.1    Comp.2     Comp.3     Comp.4     Comp.5      Comp.6
## Standard deviation     2.0140855 1.0546249 0.56827746 0.38669985 0.34770121 0.224396671
## Proportion of Variance 0.6978994 0.1913520 0.05555944 0.02572676 0.02079933 0.008663031
## Cumulative Proportion  0.6978994 0.8892514 0.94481088 0.97053763 0.99133697 1.000000000</code></pre>
</div>
<div id="check-the-biplot" class="section level3">
<h3><span class="header-section-number">11.5.4</span> Check The BiPlot</h3>
<p>What we get from this summary is that PC1 accounts for roughly 70% of the variation in the data set. By the time we get to PC3, about 95% of the variation is accounted. We can also look at something called a biplot that allows us to see what variables in the first two components are influential. This information is also available just by viewing the default return information from the object itsel but the biplot makes it easier to see.</p>
<div class="sourceCode" id="cb426"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb426-1" data-line-number="1"><span class="kw">biplot</span>(prcs)</a></code></pre></div>
<p><img src="biosml_files/figure-html/pcabiplot-1.png" width="672" /></p>
<p>Back to the summary information, we cal use something called a <strong>screeplot</strong> whih will help us see how many of the components to use. We don’t actually need the plot although it can help. Look at the plot and find the “elbow” which is the point at which the rate of change stabilizes. In the plot below it looks like the elbow is at PC3. In looking at the summary info, if we select three components then we are accounting for about 95% of the variation in the data. IF we selected two components then we would have about 89% of the variation accounted for.</p>
<div class="sourceCode" id="cb427"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb427-1" data-line-number="1"><span class="kw">summary</span>(prcs)</a></code></pre></div>
<pre><code>## Importance of components:
##                           Comp.1    Comp.2     Comp.3     Comp.4     Comp.5      Comp.6
## Standard deviation     2.0140855 1.0546249 0.56827746 0.38669985 0.34770121 0.224396671
## Proportion of Variance 0.6978994 0.1913520 0.05555944 0.02572676 0.02079933 0.008663031
## Cumulative Proportion  0.6978994 0.8892514 0.94481088 0.97053763 0.99133697 1.000000000</code></pre>
</div>
<div id="check-the-screeplot" class="section level3">
<h3><span class="header-section-number">11.5.5</span> Check The ScreePlot</h3>
<div class="sourceCode" id="cb429"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb429-1" data-line-number="1"><span class="kw">screeplot</span>(prcs,<span class="dt">type=</span><span class="st">&quot;line&quot;</span>)</a></code></pre></div>
<p><img src="biosml_files/figure-html/pcascree1-1.png" width="672" /></p>
<p>Let’s pick two components. We’ll multiply our data set (using (matrix multiplication) by the two “loadings” / components that we picked. This will give us our original data in terms of the two principla components.</p>
<div class="sourceCode" id="cb430"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb430-1" data-line-number="1">model2 &lt;-<span class="st"> </span>prcs<span class="op">$</span>loadings[,<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>]</a>
<a class="sourceLine" id="cb430-2" data-line-number="2">model2_scores &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(scaled_mtcars_data) <span class="op">%*%</span><span class="st"> </span>model2</a></code></pre></div>
</div>
<div id="use-the-transformed-data" class="section level3">
<h3><span class="header-section-number">11.5.6</span> Use The Transformed Data</h3>
<p>So now we’ll use the Naive Bayes method to do build a model using the transformed data.</p>
<div class="sourceCode" id="cb431"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb431-1" data-line-number="1">mod2 &lt;-<span class="st"> </span>e1071<span class="op">::</span><span class="kw">naiveBayes</span>(model2_scores,<span class="kw">factor</span>(mtcars_data<span class="op">$</span>am))</a>
<a class="sourceLine" id="cb431-2" data-line-number="2">(mod2t &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="kw">predict</span>(mod2,model2_scores),mtcars_data<span class="op">$</span>am))</a></code></pre></div>
<pre><code>##    
##      0  1
##   0 18  0
##   1  1 13</code></pre>
<div class="sourceCode" id="cb433"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb433-1" data-line-number="1"><span class="kw">cat</span>(<span class="st">&quot;Accuracy for PCA mod: &quot;</span>,<span class="kw">sum</span>(<span class="kw">diag</span>(mod2t))<span class="op">/</span><span class="kw">sum</span>(mod2t),<span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</a></code></pre></div>
<pre><code>## Accuracy for PCA mod:  0.96875</code></pre>
<p>Compare this to a model built using the untransformed data:</p>
<div class="sourceCode" id="cb435"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb435-1" data-line-number="1">mod1 &lt;-<span class="st"> </span>e1071<span class="op">::</span><span class="kw">naiveBayes</span>(mtcars_data[<span class="op">-</span><span class="dv">7</span>],<span class="kw">factor</span>(mtcars_data<span class="op">$</span>am))</a>
<a class="sourceLine" id="cb435-2" data-line-number="2">(mod1t &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="kw">predict</span>(mod1,mtcars_data),mtcars_data<span class="op">$</span>am))</a></code></pre></div>
<pre><code>##    
##      0  1
##   0 16  2
##   1  3 11</code></pre>
<div class="sourceCode" id="cb437"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb437-1" data-line-number="1"><span class="kw">cat</span>(<span class="st">&quot;Accuracy for PCA mod: &quot;</span>,<span class="kw">sum</span>(<span class="kw">diag</span>(mod1t))<span class="op">/</span><span class="kw">sum</span>(mod1t),<span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</a></code></pre></div>
<pre><code>## Accuracy for PCA mod:  0.84375</code></pre>
</div>
<div id="pls" class="section level3">
<h3><span class="header-section-number">11.5.7</span> PLS</h3>
<p>Partial Least Squares regression is related to PCA although, in classification probelms, the latter ignores the variable being predicted. PLS uses information from the variable to be predicted (the class labels) to help maximize the separation of the two classes. Of course, using pls as a method within caret is easy.</p>
<div class="sourceCode" id="cb439"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb439-1" data-line-number="1">control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;cv&quot;</span>,<span class="dt">number =</span> <span class="dv">5</span>)</a>
<a class="sourceLine" id="cb439-2" data-line-number="2"></a>
<a class="sourceLine" id="cb439-3" data-line-number="3"><span class="co">#</span></a>
<a class="sourceLine" id="cb439-4" data-line-number="4">caret_pls &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="kw">factor</span>(am)<span class="op">~</span>.,</a>
<a class="sourceLine" id="cb439-5" data-line-number="5">                  mtcars,</a>
<a class="sourceLine" id="cb439-6" data-line-number="6">                  <span class="dt">method=</span><span class="st">&quot;pls&quot;</span>,</a>
<a class="sourceLine" id="cb439-7" data-line-number="7">                  <span class="dt">preProcess=</span><span class="kw">c</span>(<span class="st">&quot;center&quot;</span>,<span class="st">&quot;scale&quot;</span>),</a>
<a class="sourceLine" id="cb439-8" data-line-number="8">                  <span class="dt">trControl =</span> control)</a></code></pre></div>
<p>We already knew that the mtcars data frame would benefit from PCA. Here we see that PLS uses the first three Principal Components to arrive at an accurcay of 0.97 on the data set.</p>
</div>
<div id="summary-1" class="section level3">
<h3><span class="header-section-number">11.5.8</span> Summary</h3>
<p>So the advantages of PCA should be clear in this case. We have effecively replaced the original data by a smaller data set while also dealing with the correlation issues. We used only two components. Now, a disdvantage here is that the model with the transformed data is in terms of the components which means that the model is less transparent. Perhaps a minor price to pay for better accuracy. In terms of the caret package we can do this using the <strong>preProcess</strong> function:</p>
<div class="sourceCode" id="cb440"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb440-1" data-line-number="1">control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;cv&quot;</span>,<span class="dt">number =</span> <span class="dv">5</span>)</a>
<a class="sourceLine" id="cb440-2" data-line-number="2"></a>
<a class="sourceLine" id="cb440-3" data-line-number="3"><span class="co">#</span></a>
<a class="sourceLine" id="cb440-4" data-line-number="4">caret_nb &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="kw">factor</span>(am)<span class="op">~</span>.,</a>
<a class="sourceLine" id="cb440-5" data-line-number="5">                  mtcars_data,</a>
<a class="sourceLine" id="cb440-6" data-line-number="6">                  <span class="dt">method=</span><span class="st">&quot;nb&quot;</span>,</a>
<a class="sourceLine" id="cb440-7" data-line-number="7">                  <span class="dt">preProcess=</span><span class="kw">c</span>(<span class="st">&quot;center&quot;</span>,<span class="st">&quot;scale&quot;</span>,<span class="st">&quot;pca&quot;</span>),</a>
<a class="sourceLine" id="cb440-8" data-line-number="8">                  <span class="dt">trControl =</span> control)</a></code></pre></div>
<div class="sourceCode" id="cb441"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb441-1" data-line-number="1"><span class="kw">table</span>(<span class="dt">predicted=</span><span class="kw">predict</span>(caret_nb,mtcars),mtcars<span class="op">$</span>am)</a></code></pre></div>
<pre><code>##          
## predicted  0  1
##         0 18  0
##         1  1 13</code></pre>
</div>
</div>
<div id="order-of-pre-processing" class="section level2">
<h2><span class="header-section-number">11.6</span> Order of Pre-Processing</h2>
<p>In reality, if we wanted to line up the order in which to pre process data it would read something like the following:</p>
<ul>
<li>Remove Near Zero Variance Features</li>
<li>Do Imputation (knn or median)</li>
<li>Center</li>
<li>Scale</li>
</ul>
<div class="sourceCode" id="cb443"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb443-1" data-line-number="1">lowVarLm &lt;-<span class="st"> </span><span class="kw">train</span>(</a>
<a class="sourceLine" id="cb443-2" data-line-number="2">  mtcars[,<span class="op">-</span><span class="dv">1</span>],mtcars<span class="op">$</span>mpg,</a>
<a class="sourceLine" id="cb443-3" data-line-number="3">  <span class="dt">method=</span><span class="st">&quot;lm&quot;</span>,</a>
<a class="sourceLine" id="cb443-4" data-line-number="4">  <span class="dt">preProcess=</span><span class="kw">c</span>(<span class="st">&quot;nzv&quot;</span>,<span class="st">&quot;medianImpute&quot;</span>,<span class="st">&quot;center&quot;</span>,<span class="st">&quot;scale&quot;</span>)</a>
<a class="sourceLine" id="cb443-5" data-line-number="5">  )</a></code></pre></div>
</div>
<div id="identifying-redundant-features" class="section level2">
<h2><span class="header-section-number">11.7</span> Identifying Redundant Features</h2>
<p>Lots of Data Scientists and model builders would like to know which features are important and which are redundant - BEFORE building the model. Many times, people just include all features and rely upon post model statistics to diagnose the model. This is fine though there are automated methods to help. Some statisticians do not like this approach (e.g. Step Wise Regression) because it emphasizes a scoring metric that is computed as a function of other measures though that process itself might be incomplete in some way. Ideally, you would have some up front idea about the data.</p>
<div id="highly-correlated-variables" class="section level3">
<h3><span class="header-section-number">11.7.1</span> Highly Correlated Variables</h3>
<p>In an earlier section we looked at the correlations between the variables in the mtcars data frame. When there are a number of strongly correlated variables the issue of multicollinerarity might exist. One variable might be sufficient to represent the information of one or more other variables which is useful when building models because it would allow us to leave out comparatively low information variables.</p>
<p>Techniques like PCA Principal Components Analysis can be used to recombine features in a way that explains most of the variation of data set. A simpler way might be to look for highly correlated sets of variables and remove those over a certain threshold of correlation from the data frame.</p>
<p>Let’s look at the mtcars data frame again to see what variables are correlated.</p>
<div class="sourceCode" id="cb444"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb444-1" data-line-number="1"><span class="kw">data</span>(mtcars)</a>
<a class="sourceLine" id="cb444-2" data-line-number="2">correlations &lt;-<span class="st"> </span><span class="kw">cor</span>(mtcars)</a>
<a class="sourceLine" id="cb444-3" data-line-number="3">correlations[<span class="dv">1</span><span class="op">:</span><span class="dv">6</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">6</span>]</a></code></pre></div>
<pre><code>##             mpg        cyl       disp         hp       drat         wt
## mpg   1.0000000 -0.8521620 -0.8475514 -0.7761684  0.6811719 -0.8676594
## cyl  -0.8521620  1.0000000  0.9020329  0.8324475 -0.6999381  0.7824958
## disp -0.8475514  0.9020329  1.0000000  0.7909486 -0.7102139  0.8879799
## hp   -0.7761684  0.8324475  0.7909486  1.0000000 -0.4487591  0.6587479
## drat  0.6811719 -0.6999381 -0.7102139 -0.4487591  1.0000000 -0.7124406
## wt   -0.8676594  0.7824958  0.8879799  0.6587479 -0.7124406  1.0000000</code></pre>
<p>It turns out that there are lots of strong correlations going on here. The darker the circle the stronger the correlation.</p>
<div class="sourceCode" id="cb446"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb446-1" data-line-number="1"><span class="kw">suppressMessages</span>(<span class="kw">library</span>(corrplot))</a>
<a class="sourceLine" id="cb446-2" data-line-number="2"><span class="kw">corrplot</span>(correlations, <span class="dt">order=</span><span class="st">&quot;hclust&quot;</span>)</a></code></pre></div>
<p><img src="biosml_files/figure-html/unnamed-chunk-106-1.png" width="672" /></p>
<p>The <strong>caret</strong> package has some functions that can help us identify highly correlated variables that might be a candidates for removal prior to use in building a model. One of the variables that is highly correlated with others is <strong>mpg</strong> Since that is the one we are trying to predict, we’ll keep it around. The following columns from mtcars have a correlation of .75 (or higher) with some other variable(s)</p>
<div class="sourceCode" id="cb447"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb447-1" data-line-number="1">(highcorr &lt;-<span class="st"> </span><span class="kw">findCorrelation</span>(correlations, <span class="dt">cutoff=</span>.<span class="dv">75</span>))</a></code></pre></div>
<pre><code>## [1]  2  3  1 10</code></pre>
<div class="sourceCode" id="cb449"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb449-1" data-line-number="1"><span class="kw">names</span>(mtcars[,highcorr])</a></code></pre></div>
<pre><code>## [1] &quot;cyl&quot;  &quot;disp&quot; &quot;mpg&quot;  &quot;gear&quot;</code></pre>
<p>So let’s remove those variables from consideration although we’ll keep the <strong>mpg</strong> variable since that is what we will be predicting.</p>
<div class="sourceCode" id="cb451"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb451-1" data-line-number="1"><span class="kw">data</span>(mtcars)</a>
<a class="sourceLine" id="cb451-2" data-line-number="2">decorrel_mtcars &lt;-<span class="st"> </span>mtcars[,<span class="op">-</span>highcorr[highcorr <span class="op">!=</span><span class="st"> </span><span class="dv">1</span>]]</a></code></pre></div>
<p>This might yield a better model though it’s tough to tell.</p>
<div class="sourceCode" id="cb452"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb452-1" data-line-number="1">idx &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(decorrel_mtcars<span class="op">$</span>mpg, <span class="dt">p =</span> <span class="fl">.8</span>, </a>
<a class="sourceLine" id="cb452-2" data-line-number="2">                                  <span class="dt">list =</span> <span class="ot">FALSE</span>, </a>
<a class="sourceLine" id="cb452-3" data-line-number="3">                                  <span class="dt">times =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb452-4" data-line-number="4">train &lt;-<span class="st"> </span>decorrel_mtcars[ idx,]</a>
<a class="sourceLine" id="cb452-5" data-line-number="5">test  &lt;-<span class="st"> </span>decorrel_mtcars[<span class="op">-</span>idx,]</a>
<a class="sourceLine" id="cb452-6" data-line-number="6"></a>
<a class="sourceLine" id="cb452-7" data-line-number="7">decorrel_lm &lt;-<span class="st"> </span><span class="kw">train</span>(mpg<span class="op">~</span>.,</a>
<a class="sourceLine" id="cb452-8" data-line-number="8">                     <span class="dt">data =</span> train,</a>
<a class="sourceLine" id="cb452-9" data-line-number="9">                     <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>,</a>
<a class="sourceLine" id="cb452-10" data-line-number="10">                     <span class="dt">metric =</span> <span class="st">&quot;RMSE&quot;</span></a>
<a class="sourceLine" id="cb452-11" data-line-number="11">                     )</a></code></pre></div>
</div>
</div>
<div id="ranking-features" class="section level2">
<h2><span class="header-section-number">11.8</span> Ranking Features</h2>
<p>Another approach would be to identify the important variables as determined by a rigorous, reliable and statistically aware process. Some models will report this information as part of the computation. The <strong>caret</strong> package has a function called <strong>varImp</strong> which helps to calculate relative variable importance within an object produced by the <strong>train</strong> function. What this means for us is that we can take most models built with <strong>train</strong> and pass it to the <strong>varImp</strong> function.</p>
<div class="sourceCode" id="cb453"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb453-1" data-line-number="1">idx &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(mtcars<span class="op">$</span>mpg, <span class="dt">p =</span> <span class="fl">.8</span>, </a>
<a class="sourceLine" id="cb453-2" data-line-number="2">                                  <span class="dt">list =</span> <span class="ot">FALSE</span>, </a>
<a class="sourceLine" id="cb453-3" data-line-number="3">                                  <span class="dt">times =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb453-4" data-line-number="4">train &lt;-<span class="st"> </span>mtcars[ idx,]</a>
<a class="sourceLine" id="cb453-5" data-line-number="5">test  &lt;-<span class="st"> </span>mtcars[<span class="op">-</span>idx,]</a>
<a class="sourceLine" id="cb453-6" data-line-number="6"></a>
<a class="sourceLine" id="cb453-7" data-line-number="7">myLm &lt;-<span class="st"> </span><span class="kw">train</span>(mpg<span class="op">~</span>.,</a>
<a class="sourceLine" id="cb453-8" data-line-number="8">                     <span class="dt">data =</span> train,</a>
<a class="sourceLine" id="cb453-9" data-line-number="9">                     <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>,</a>
<a class="sourceLine" id="cb453-10" data-line-number="10">                     <span class="dt">metric =</span> <span class="st">&quot;RMSE&quot;</span></a>
<a class="sourceLine" id="cb453-11" data-line-number="11">                     )</a></code></pre></div>
<pre><code>## Warning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit may be misleading</code></pre>
<div class="sourceCode" id="cb455"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb455-1" data-line-number="1"><span class="kw">summary</span>(myLm)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = .outcome ~ ., data = dat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.6545 -1.6435 -0.0332  0.9472  4.3548 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept) -17.67691   29.04361  -0.609   0.5508  
## cyl          -0.10805    1.31734  -0.082   0.9356  
## disp          0.01709    0.01941   0.880   0.3909  
## hp           -0.01151    0.02592  -0.444   0.6625  
## drat          1.77684    1.86252   0.954   0.3535  
## wt           -4.51440    2.22558  -2.028   0.0585 .
## qsec          2.10070    1.26766   1.657   0.1158  
## vs           -2.15042    3.83235  -0.561   0.5820  
## am            0.72004    2.75256   0.262   0.7968  
## gear          2.22324    2.04781   1.086   0.2928  
## carb         -0.11134    0.97412  -0.114   0.9103  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.768 on 17 degrees of freedom
## Multiple R-squared:  0.8676, Adjusted R-squared:  0.7898 
## F-statistic: 11.14 on 10 and 17 DF,  p-value: 1.239e-05</code></pre>
<p>If you look at the summary of the model it looks pretty bleak because mot of the coefficients aren’t significant. You wouldn’t want to take your career on this model.</p>
<p>Now let’s see what variables are considered to be important by using <strong>varImp</strong>. Remember that this might lead us to use a subset of the more prominent variables in the formation of a new model in case, for example, the coefficients in the existing model weren’t significant.</p>
<div class="sourceCode" id="cb457"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb457-1" data-line-number="1"><span class="kw">plot</span>(<span class="kw">varImp</span>(myLm))</a></code></pre></div>
<p><img src="biosml_files/figure-html/unnamed-chunk-111-1.png" width="672" /></p>
<p>How do things look in the model if we limit the formula to some of the “important” (allegedly) features ? Well, it explains more of the variance with fewer predictors (the R^2 value). Both <strong>wt</strong> and <strong>hp</strong> are significant. That’s a start.</p>
<div class="sourceCode" id="cb458"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb458-1" data-line-number="1">myLm &lt;-<span class="st"> </span><span class="kw">train</span>(mpg<span class="op">~</span>wt<span class="op">+</span>disp<span class="op">+</span>hp,</a>
<a class="sourceLine" id="cb458-2" data-line-number="2">                     <span class="dt">data =</span> train,</a>
<a class="sourceLine" id="cb458-3" data-line-number="3">                     <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>,</a>
<a class="sourceLine" id="cb458-4" data-line-number="4">                     <span class="dt">metric =</span> <span class="st">&quot;RMSE&quot;</span></a>
<a class="sourceLine" id="cb458-5" data-line-number="5">                     )</a>
<a class="sourceLine" id="cb458-6" data-line-number="6"></a>
<a class="sourceLine" id="cb458-7" data-line-number="7"><span class="kw">summary</span>(myLm)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = .outcome ~ ., data = dat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.7930 -1.7171 -0.5208  1.1468  5.7533 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 37.102658   2.440382  15.204 8.09e-14 ***
## wt          -3.705589   1.265889  -2.927  0.00737 ** 
## disp        -0.002895   0.011699  -0.247  0.80664    
## hp          -0.030005   0.012239  -2.452  0.02188 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.798 on 24 degrees of freedom
## Multiple R-squared:  0.809,  Adjusted R-squared:  0.7852 
## F-statistic: 33.89 on 3 and 24 DF,  p-value: 8.601e-09</code></pre>
</div>
<div id="feature-selection-1" class="section level2">
<h2><span class="header-section-number">11.9</span> Feature Selection</h2>
<p>Being able to automatically include only the most important features is desirable (again an example might be Step Wise Regression). Caret provides its own function to do a simple backwards selection to recursively eliminate features based on how they contribute (or not) to the model. You could do all of this by hand yourself of course.</p>
<p>Automatic feature selection methods can be used to build many models with different subsets of a dataset and identify those attributes that are and are not required to build an accurate model. The <strong>caret</strong> package has a function called *rfe** to implement this. However, it involves some additional setup:</p>
<div class="sourceCode" id="cb460"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb460-1" data-line-number="1">rfeControl &lt;-<span class="st"> </span><span class="kw">rfeControl</span>(<span class="dt">functions=</span>lmFuncs, <span class="dt">method=</span><span class="st">&quot;cv&quot;</span>, <span class="dt">number=</span><span class="dv">10</span>)</a>
<a class="sourceLine" id="cb460-2" data-line-number="2"></a>
<a class="sourceLine" id="cb460-3" data-line-number="3">results &lt;-<span class="st"> </span><span class="kw">rfe</span>(mtcars[,<span class="dv">2</span><span class="op">:</span><span class="dv">11</span>], </a>
<a class="sourceLine" id="cb460-4" data-line-number="4">               mtcars[,<span class="dv">1</span>], </a>
<a class="sourceLine" id="cb460-5" data-line-number="5">               <span class="dt">sizes=</span><span class="kw">c</span>(<span class="dv">2</span><span class="op">:</span><span class="dv">11</span>), </a>
<a class="sourceLine" id="cb460-6" data-line-number="6">               <span class="dt">rfeControl=</span>rfeControl)</a>
<a class="sourceLine" id="cb460-7" data-line-number="7"></a>
<a class="sourceLine" id="cb460-8" data-line-number="8"><span class="kw">print</span>(results)</a></code></pre></div>
<pre><code>## 
## Recursive feature selection
## 
## Outer resampling method: Cross-Validated (10 fold) 
## 
## Resampling performance over subset size:
## 
##  Variables  RMSE Rsquared   MAE RMSESD RsquaredSD  MAESD Selected
##          2 3.059   0.9048 2.624 1.1185     0.1043 1.0419         
##          3 3.096   0.8846 2.679 1.1977     0.1247 1.0573         
##          4 2.962   0.8884 2.555 0.8976     0.1382 0.7654        *
##          5 3.227   0.8591 2.671 1.0330     0.1798 0.8204         
##          6 3.175   0.8763 2.643 1.0883     0.1619 0.8602         
##          7 3.220   0.8743 2.666 1.1369     0.1795 0.8943         
##          8 3.162   0.8833 2.587 1.2447     0.1806 0.9916         
##          9 3.316   0.8588 2.739 1.4329     0.1891 1.2213         
##         10 3.203   0.8671 2.702 1.4511     0.1696 1.2667         
## 
## The top 4 variables (out of 4):
##    wt, am, drat, gear</code></pre>
<div class="sourceCode" id="cb462"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb462-1" data-line-number="1"><span class="kw">plot</span>(results,<span class="dt">type=</span><span class="kw">c</span>(<span class="st">&quot;g&quot;</span>,<span class="st">&quot;o&quot;</span>))</a></code></pre></div>
<p><img src="biosml_files/figure-html/unnamed-chunk-113-1.png" width="672" /></p>
<div class="sourceCode" id="cb463"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb463-1" data-line-number="1">myLm &lt;-<span class="st"> </span><span class="kw">train</span>(mpg<span class="op">~</span>wt<span class="op">+</span>am<span class="op">+</span>qsec,</a>
<a class="sourceLine" id="cb463-2" data-line-number="2">                     <span class="dt">data =</span> train,</a>
<a class="sourceLine" id="cb463-3" data-line-number="3">                     <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>,</a>
<a class="sourceLine" id="cb463-4" data-line-number="4">                     <span class="dt">metric =</span> <span class="st">&quot;RMSE&quot;</span></a>
<a class="sourceLine" id="cb463-5" data-line-number="5">                     )</a>
<a class="sourceLine" id="cb463-6" data-line-number="6"></a>
<a class="sourceLine" id="cb463-7" data-line-number="7"><span class="kw">summary</span>(myLm)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = .outcome ~ ., data = dat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.6595 -1.2609 -0.4483  1.5021  4.2625 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   4.5955     8.4322   0.545 0.590789    
## wt           -3.7490     0.8468  -4.427 0.000178 ***
## am            2.9134     1.5280   1.907 0.068605 .  
## qsec          1.4857     0.3552   4.182 0.000332 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.532 on 24 degrees of freedom
## Multiple R-squared:  0.8437, Adjusted R-squared:  0.8242 
## F-statistic: 43.19 on 3 and 24 DF,  p-value: 7.907e-10</code></pre>
</div>
<div id="categorical-features" class="section level2">
<h2><span class="header-section-number">11.10</span> Categorical Features</h2>
<p>Categorical features are those that are labelled or character data which suggest categories (e.g. “male”,“female”,“smoker”,“non-snoker”). These variables, while useful, need to be recoded in a way to make them useful for machine learning methods. For example, read in the following data:</p>
<div class="sourceCode" id="cb465"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb465-1" data-line-number="1">url &lt;-<span class="st"> &quot;https://raw.githubusercontent.com/steviep42/bios534_spring_2020/master/data/etitanic.csv&quot;</span></a>
<a class="sourceLine" id="cb465-2" data-line-number="2">etitanic &lt;-<span class="st"> </span><span class="kw">read.csv</span>(url)</a></code></pre></div>
<p>If you insepct this data frame you will notice that there are already some “factors” in the data. This is good because we can begin to use this information in the development of models although this will be a function of what it is we are trying to predict.</p>
<div class="sourceCode" id="cb466"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb466-1" data-line-number="1"><span class="kw">str</span>(etitanic)</a></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    1046 obs. of  6 variables:
##  $ pclass  : Factor w/ 3 levels &quot;1st&quot;,&quot;2nd&quot;,&quot;3rd&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ survived: int  1 1 0 0 0 1 1 0 1 0 ...
##  $ sex     : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 1 2 1 2 1 2 1 2 1 2 ...
##  $ age     : num  29 0.917 2 30 25 ...
##  $ sibsp   : int  0 1 1 1 1 0 1 0 2 0 ...
##  $ parch   : int  0 2 2 2 2 0 0 0 0 0 ...</code></pre>
<p>If we want to predict, for example, whether someone survived the catastrophe (the survived column), we’ll need to turn that into a factor also since it represents a category. This happens a lot in classification.</p>
<div class="sourceCode" id="cb468"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb468-1" data-line-number="1">etitanic &lt;-<span class="st"> </span>etitanic <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">survived =</span> <span class="kw">factor</span>(survived))</a>
<a class="sourceLine" id="cb468-2" data-line-number="2"><span class="kw">str</span>(etitanic)</a></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    1046 obs. of  6 variables:
##  $ pclass  : Factor w/ 3 levels &quot;1st&quot;,&quot;2nd&quot;,&quot;3rd&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ survived: Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 2 1 1 1 2 2 1 2 1 ...
##  $ sex     : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 1 2 1 2 1 2 1 2 1 2 ...
##  $ age     : num  29 0.917 2 30 25 ...
##  $ sibsp   : int  0 1 1 1 1 0 1 0 2 0 ...
##  $ parch   : int  0 2 2 2 2 0 0 0 0 0 ...</code></pre>
<p>Now we can build a model to predict survival:</p>
<div class="sourceCode" id="cb470"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb470-1" data-line-number="1">control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;cv&quot;</span>,<span class="dt">number=</span><span class="dv">5</span>)</a>
<a class="sourceLine" id="cb470-2" data-line-number="2">etitanic_model &lt;-<span class="st"> </span><span class="kw">train</span>(survived <span class="op">~</span><span class="st"> </span>.,etitanic,<span class="dt">method=</span><span class="st">&quot;glm&quot;</span>,<span class="dt">trControl=</span>control)</a></code></pre></div>
<p>The Accuracy of the model, or the fact that we didn’t make a train / test split, is beside the point here. We need to inform R that we have categories else it won’t know what to do with them or, worse, it will do something unexpected. Let’s look at another situation that frequently occurs where we might have categories that are numbers. Well, that was true in the above example but it was quite apparent that the 0 or 1 motif is a binary situation. So it was fairly obvious. Consider the mtcars data frame:</p>
<div class="sourceCode" id="cb471"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb471-1" data-line-number="1"><span class="kw">str</span>(mtcars)</a></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    32 obs. of  11 variables:
##  $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...
##  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...
##  $ disp: num  160 160 108 258 360 ...
##  $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...
##  $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...
##  $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...
##  $ qsec: num  16.5 17 18.6 19.4 17 ...
##  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...
##  $ am  : num  1 1 1 0 0 0 0 0 0 0 ...
##  $ gear: num  4 4 4 3 3 3 3 4 4 4 ...
##  $ carb: num  4 4 1 1 2 1 4 2 2 4 ...</code></pre>
<p>Notice that the cyl, vs, am, gear, and carb features take on a few unique values. This suggests that they are actually categories.</p>
<div class="sourceCode" id="cb473"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb473-1" data-line-number="1"> <span class="kw">sapply</span>(mtcars,<span class="cf">function</span>(x) <span class="kw">length</span>(<span class="kw">unique</span>(x)))</a></code></pre></div>
<pre><code>##  mpg  cyl disp   hp drat   wt qsec   vs   am gear carb 
##   25    3   27   22   22   29   30    2    2    3    6</code></pre>
<p>For example, the cylinder variable is a classification of whether a car has 4,6, or 8 cylinders. There is no such thing as a 4.5 cylinder automobile. Nor is there a car that had a transmission type of 1.5. In these cases we should probably make factors out of these prior to use with a method.</p>
<div class="sourceCode" id="cb475"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb475-1" data-line-number="1">lm1 &lt;-<span class="st"> </span><span class="kw">train</span>(mpg<span class="op">~</span>cyl<span class="op">+</span>am,</a>
<a class="sourceLine" id="cb475-2" data-line-number="2">             <span class="dt">data=</span>mtcars,</a>
<a class="sourceLine" id="cb475-3" data-line-number="3">             <span class="dt">method=</span><span class="st">&quot;lm&quot;</span>,</a>
<a class="sourceLine" id="cb475-4" data-line-number="4">             <span class="dt">trControl=</span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;cv&quot;</span>,<span class="dt">number=</span><span class="dv">3</span>))</a>
<a class="sourceLine" id="cb475-5" data-line-number="5"></a>
<a class="sourceLine" id="cb475-6" data-line-number="6">lm1</a></code></pre></div>
<pre><code>## Linear Regression 
## 
## 32 samples
##  2 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (3 fold) 
## Summary of sample sizes: 20, 21, 23 
## Resampling results:
## 
##   RMSE     Rsquared   MAE     
##   2.99686  0.7656034  2.467292
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<div class="sourceCode" id="cb477"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb477-1" data-line-number="1"><span class="kw">summary</span>(lm1)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = .outcome ~ ., data = dat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.6856 -1.7172 -0.2657  1.8838  6.8144 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  34.5224     2.6032  13.262 7.69e-14 ***
## cyl          -2.5010     0.3608  -6.931 1.28e-07 ***
## am            2.5670     1.2914   1.988   0.0564 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.059 on 29 degrees of freedom
## Multiple R-squared:  0.759,  Adjusted R-squared:  0.7424 
## F-statistic: 45.67 on 2 and 29 DF,  p-value: 1.094e-09</code></pre>
<p>Now we will create some factors out of am and cyl to show that the resulting model will be different in terms of the coefficents. But first notice how creating a factor will assign an underling numeric value to the feature which is the key to making it usabale by downstream ML methods.</p>
<div class="sourceCode" id="cb479"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb479-1" data-line-number="1"><span class="kw">data</span>(mtcars)</a>
<a class="sourceLine" id="cb479-2" data-line-number="2">mod_mtcars &lt;-<span class="st"> </span>mtcars <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">cyl=</span><span class="kw">factor</span>(cyl),<span class="dt">am=</span><span class="kw">factor</span>(am))</a>
<a class="sourceLine" id="cb479-3" data-line-number="3"></a>
<a class="sourceLine" id="cb479-4" data-line-number="4"><span class="kw">levels</span>(mod_mtcars<span class="op">$</span>am)</a></code></pre></div>
<pre><code>## [1] &quot;0&quot; &quot;1&quot;</code></pre>
<div class="sourceCode" id="cb481"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb481-1" data-line-number="1"><span class="kw">levels</span>(mod_mtcars<span class="op">$</span>cyl)</a></code></pre></div>
<pre><code>## [1] &quot;4&quot; &quot;6&quot; &quot;8&quot;</code></pre>
<p>Now let’s make a model</p>
<div class="sourceCode" id="cb483"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb483-1" data-line-number="1"><span class="kw">data</span>(mtcars)</a>
<a class="sourceLine" id="cb483-2" data-line-number="2">mod_mtcars &lt;-<span class="st"> </span>mtcars <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">cyl=</span><span class="kw">factor</span>(cyl),<span class="dt">am=</span><span class="kw">factor</span>(am))</a>
<a class="sourceLine" id="cb483-3" data-line-number="3">lm2 &lt;-<span class="st"> </span><span class="kw">train</span>(mpg<span class="op">~</span>cyl<span class="op">+</span>am,</a>
<a class="sourceLine" id="cb483-4" data-line-number="4">             <span class="dt">data=</span>mod_mtcars,</a>
<a class="sourceLine" id="cb483-5" data-line-number="5">             <span class="dt">method=</span><span class="st">&quot;lm&quot;</span>,</a>
<a class="sourceLine" id="cb483-6" data-line-number="6">             <span class="dt">trControl=</span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;cv&quot;</span>,<span class="dt">number=</span><span class="dv">3</span>))</a>
<a class="sourceLine" id="cb483-7" data-line-number="7">lm2</a></code></pre></div>
<pre><code>## Linear Regression 
## 
## 32 samples
##  2 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (3 fold) 
## Summary of sample sizes: 21, 21, 22 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   3.058989  0.7537272  2.325309
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<div class="sourceCode" id="cb485"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb485-1" data-line-number="1"><span class="kw">summary</span>(lm2)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = .outcome ~ ., data = dat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.9618 -1.4971 -0.2057  1.8907  6.5382 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   24.802      1.323  18.752  &lt; 2e-16 ***
## cyl6          -6.156      1.536  -4.009 0.000411 ***
## cyl8         -10.068      1.452  -6.933 1.55e-07 ***
## am1            2.560      1.298   1.973 0.058457 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.073 on 28 degrees of freedom
## Multiple R-squared:  0.7651, Adjusted R-squared:  0.7399 
## F-statistic:  30.4 on 3 and 28 DF,  p-value: 5.959e-09</code></pre>
</div>
<div id="binning" class="section level2">
<h2><span class="header-section-number">11.11</span> Binning</h2>
<p>Sometimes we have data that spans a range of values such that it might make more sense to “bin” or collect the values into “buckets” represented by categories. This is akin to what a histogram does. Look at the PimaIndiansDiabetes data set for an example. Specifically, look at the <strong>pregnant</strong> feature in the form of a histogram. Most of those surveyed had less than 5 pregnancies although there are those who had at least that number of pregnancies and more.</p>
<div class="sourceCode" id="cb487"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb487-1" data-line-number="1">pm_new &lt;-<span class="st"> </span>PimaIndiansDiabetes</a>
<a class="sourceLine" id="cb487-2" data-line-number="2"><span class="kw">hist</span>(pm_new<span class="op">$</span>pregnant)</a></code></pre></div>
<p><img src="biosml_files/figure-html/pmbin1-1.png" width="672" /></p>
<p>We could build our models using the number of pregnancies without attemptin to transform the feature in any way. But let’s bin this feature. In R, we can use the <strong>cut</strong> function to chop up a numerical range of data into a specified number of bins and according to some function such as the <strong>quantile</strong> function which would give us for bins labelled from 1 to 4. This will produce sample quantiles according to the 0, 25th, 50th, 75%, and 100% percentiles.</p>
<div class="sourceCode" id="cb488"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb488-1" data-line-number="1"><span class="kw">cut</span>(pm_new<span class="op">$</span>pregnant, <span class="dt">breaks=</span><span class="kw">quantile</span>(pm<span class="op">$</span>pregnant),</a>
<a class="sourceLine" id="cb488-2" data-line-number="2">                     <span class="dt">include.lowest =</span> <span class="ot">TRUE</span>,<span class="dt">labels=</span><span class="dv">1</span><span class="op">:</span><span class="dv">4</span>)</a></code></pre></div>
<pre><code>##   [1] 3 1 4 1 1 3 2 4 2 4 3 4 4 1 3 4 1 4 1 1 2 4 4 4 4 4 4 1 4 3 3 2 2 3 4 3 4 4 2 3 2 4 4 4 4
##  [46] 1 1 2 4 4 1 1 3 4 4 1 4 1 1 1 2 4 3 2 4 3 1 2 1 3 2 3 4 3 1 1 4 3 1 2 2 2 4 1 3 2 4 2 4 1
##  [91] 1 3 4 3 2 3 2 1 3 1 1 1 1 1 2 1 1 3 2 1 2 4 1 3 4 3 3 3 3 3 1 3 2 3 1 1 2 1 1 1 3 4 2 4 2
## [136] 2 1 1 1 3 2 3 2 4 3 1 4 2 3 2 1 3 4 1 4 4 2 1 2 4 3 4 1 2 1 3 2 3 3 2 3 3 2 1 2 4 3 1 3 3
## [181] 3 1 1 3 3 4 4 1 4 3 2 4 4 4 4 3 1 2 3 3 1 1 1 2 3 3 4 3 1 4 2 1 4 1 4 4 3 3 3 3 1 2 4 4 1
## [226] 1 1 2 3 1 3 3 1 3 2 3 4 1 4 1 1 3 2 3 2 4 4 1 4 1 4 2 2 1 4 1 2 2 1 4 2 2 3 2 3 3 1 2 1 2
## [271] 4 2 2 1 4 2 4 1 3 2 1 4 4 4 2 4 3 1 3 3 1 1 2 1 1 3 2 1 4 4 1 2 3 3 2 2 4 1 1 2 3 1 2 2 4
## [316] 2 2 2 2 3 3 2 1 4 2 1 1 4 2 3 4 2 1 4 1 1 1 3 4 4 1 1 1 3 4 4 1 2 2 3 3 3 2 1 2 4 1 4 4 1
## [361] 3 3 3 3 3 3 3 1 2 1 2 1 1 2 2 4 1 1 3 1 1 1 1 1 1 1 3 4 3 2 1 3 1 3 3 2 2 1 2 2 3 3 3 4 3
## [406] 2 3 1 4 1 3 1 1 1 1 2 1 3 1 2 1 2 1 2 4 3 1 1 1 1 2 2 1 2 1 1 4 3 1 3 1 2 3 4 3 1 1 1 1 1
## [451] 1 2 1 2 2 4 1 3 4 4 4 1 4 3 4 1 1 1 4 3 1 1 1 4 3 1 2 4 4 3 2 1 3 1 1 1 1 1 3 4 2 2 3 3 2
## [496] 3 3 2 4 3 2 2 3 4 2 4 1 1 2 4 4 1 4 2 2 2 4 4 4 3 2 2 3 4 2 2 1 2 1 1 2 1 1 3 1 3 1 1 1 2
## [541] 4 2 4 3 1 4 3 3 1 3 1 2 3 1 1 4 1 4 4 4 3 1 1 3 1 2 1 3 3 1 2 2 2 2 1 1 3 2 4 2 1 3 4 4 4
## [586] 1 4 3 2 1 4 2 2 2 3 1 1 1 1 1 1 3 1 4 3 1 1 1 1 1 2 2 4 3 4 2 3 2 4 1 2 2 3 1 2 3 1 1 3 3
## [631] 4 1 2 1 4 4 3 2 4 1 1 3 3 3 2 2 1 1 4 1 1 1 3 2 1 2 2 1 4 2 4 1 4 4 3 1 3 4 3 4 3 1 4 2 4
## [676] 3 4 1 2 2 2 1 1 3 3 2 2 1 1 1 4 4 2 4 2 4 2 1 3 3 2 3 1 2 3 3 4 2 4 2 2 3 4 1 2 4 2 4 1 3
## [721] 3 1 1 3 1 3 1 1 2 2 2 4 2 2 2 3 1 4 2 1 4 2 1 4 4 4 1 1 2 3 3 1 2 1 4 1 4 1 1 3 2 4 4 4 2
## [766] 3 1 1
## Levels: 1 2 3 4</code></pre>
<div class="sourceCode" id="cb490"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb490-1" data-line-number="1">pm_new &lt;-<span class="st"> </span>pm_new <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb490-2" data-line-number="2"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pregnant=</span><span class="kw">cut</span>(pregnant,<span class="dt">breaks=</span><span class="kw">quantile</span>(pregnant),</a>
<a class="sourceLine" id="cb490-3" data-line-number="3">                      <span class="dt">include.lowest=</span><span class="ot">TRUE</span>,<span class="dt">labels=</span><span class="dv">1</span><span class="op">:</span><span class="dv">4</span>))</a>
<a class="sourceLine" id="cb490-4" data-line-number="4"></a>
<a class="sourceLine" id="cb490-5" data-line-number="5"><span class="kw">str</span>(pm_new<span class="op">$</span>pregnant)</a></code></pre></div>
<pre><code>##  Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 3 1 4 1 1 3 2 4 2 4 ...</code></pre>
<p>So we could now build a model using this newly binned information. It might not make a big difference although it might give some insight into to what extent the number of pregnancies influence the model. More specifically, which bin of the pregnancy variable would be influential - if it all ? First, let’s create a model using the untransformed data as found in the PimaIndiansDiabetes data frame.</p>
<div class="sourceCode" id="cb492"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb492-1" data-line-number="1">control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;cv&quot;</span>,<span class="dt">number=</span><span class="dv">5</span>)</a>
<a class="sourceLine" id="cb492-2" data-line-number="2"></a>
<a class="sourceLine" id="cb492-3" data-line-number="3">cutmodel1 &lt;-<span class="st"> </span><span class="kw">train</span>(diabetes<span class="op">~</span>.,</a>
<a class="sourceLine" id="cb492-4" data-line-number="4">                   <span class="dt">data=</span>PimaIndiansDiabetes,</a>
<a class="sourceLine" id="cb492-5" data-line-number="5">                   <span class="dt">method=</span><span class="st">&quot;glm&quot;</span>,</a>
<a class="sourceLine" id="cb492-6" data-line-number="6">                   <span class="dt">trControl=</span>control)</a>
<a class="sourceLine" id="cb492-7" data-line-number="7"><span class="kw">summary</span>(cutmodel1<span class="op">$</span>finalModel)</a></code></pre></div>
<pre><code>## 
## Call:
## NULL
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.5566  -0.7274  -0.4159   0.7267   2.9297  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -8.4046964  0.7166359 -11.728  &lt; 2e-16 ***
## pregnant     0.1231823  0.0320776   3.840 0.000123 ***
## glucose      0.0351637  0.0037087   9.481  &lt; 2e-16 ***
## pressure    -0.0132955  0.0052336  -2.540 0.011072 *  
## triceps      0.0006190  0.0068994   0.090 0.928515    
## insulin     -0.0011917  0.0009012  -1.322 0.186065    
## mass         0.0897010  0.0150876   5.945 2.76e-09 ***
## pedigree     0.9451797  0.2991475   3.160 0.001580 ** 
## age          0.0148690  0.0093348   1.593 0.111192    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 993.48  on 767  degrees of freedom
## Residual deviance: 723.45  on 759  degrees of freedom
## AIC: 741.45
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<div class="sourceCode" id="cb494"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb494-1" data-line-number="1"><span class="kw">varImp</span>(cutmodel1)</a></code></pre></div>
<pre><code>## glm variable importance
## 
##          Overall
## glucose   100.00
## mass       62.35
## pregnant   39.93
## pedigree   32.69
## pressure   26.09
## age        16.01
## insulin    13.12
## triceps     0.00</code></pre>
<p>We definitely see that pregnancy is an important feature here although, again, we don’t know what sub population might have influence, if any, on the overall model. Let’s rerun this analysis on the transformed version of the data frame.</p>
<div class="sourceCode" id="cb496"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb496-1" data-line-number="1">cutmodel2 &lt;-<span class="st"> </span><span class="kw">train</span>(diabetes<span class="op">~</span>.,</a>
<a class="sourceLine" id="cb496-2" data-line-number="2">                   <span class="dt">data=</span>pm_new,</a>
<a class="sourceLine" id="cb496-3" data-line-number="3">                   <span class="dt">method=</span><span class="st">&quot;glm&quot;</span>,</a>
<a class="sourceLine" id="cb496-4" data-line-number="4">                   <span class="dt">trControl=</span>control)</a>
<a class="sourceLine" id="cb496-5" data-line-number="5"></a>
<a class="sourceLine" id="cb496-6" data-line-number="6"><span class="kw">summary</span>(cutmodel2<span class="op">$</span>finalModel)</a></code></pre></div>
<pre><code>## 
## Call:
## NULL
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.5051  -0.7199  -0.4201   0.7282   2.9632  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -8.2900923  0.7432705 -11.154  &lt; 2e-16 ***
## pregnant2    0.2020865  0.2679622   0.754  0.45075    
## pregnant3    0.4018240  0.2722524   1.476  0.13996    
## pregnant4    1.1252206  0.2909948   3.867  0.00011 ***
## glucose      0.0349990  0.0037156   9.420  &lt; 2e-16 ***
## pressure    -0.0130319  0.0052424  -2.486  0.01292 *  
## triceps      0.0009007  0.0069532   0.130  0.89693    
## insulin     -0.0012053  0.0009026  -1.335  0.18173    
## mass         0.0890646  0.0151221   5.890 3.87e-09 ***
## pedigree     0.9126652  0.2990451   3.052  0.00227 ** 
## age          0.0150488  0.0093797   1.604  0.10863    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 993.48  on 767  degrees of freedom
## Residual deviance: 722.27  on 757  degrees of freedom
## AIC: 744.27
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<div class="sourceCode" id="cb498"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb498-1" data-line-number="1"><span class="kw">varImp</span>(cutmodel2)</a></code></pre></div>
<pre><code>## glm variable importance
## 
##           Overall
## glucose   100.000
## mass       62.004
## pregnant4  40.229
## pedigree   31.457
## pressure   25.364
## age        15.876
## pregnant3  14.493
## insulin    12.981
## pregnant2   6.724
## triceps     0.000</code></pre>
<p>This is interesting in that pehaps it’s the 4th bin of the pregnancy feature that has the major influence on the model. So if we look at the distribution of positive cases we see that in the positive cases there are more people from the 4th bin:</p>
<div class="sourceCode" id="cb500"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb500-1" data-line-number="1">pm_new <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">group_by</span>(diabetes,pregnant) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb500-2" data-line-number="2"><span class="st">  </span><span class="kw">count</span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb500-3" data-line-number="3"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>diabetes,<span class="dt">y=</span>n,<span class="dt">fill=</span>pregnant)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_bar</span>(<span class="dt">stat=</span><span class="st">&quot;identity&quot;</span>)</a></code></pre></div>
<p><img src="biosml_files/figure-html/plot4bin-1.png" width="672" /></p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="picking-the-best-model.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="using-external-ml-frameworks.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
