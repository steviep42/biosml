<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Data Pre Processing | Predictive Learning in R</title>
  <meta name="description" content="Chapter 9 Data Pre Processing | Predictive Learning in R" />
  <meta name="generator" content="bookdown 0.14 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Data Pre Processing | Predictive Learning in R" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Data Pre Processing | Predictive Learning in R" />
  
  
  

<meta name="author" content="Steve Pittard" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="picking-the-best-model.html"/>
<link rel="next" href="classification-problems.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a></li>
<li class="chapter" data-level="2" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html"><i class="fa fa-check"></i><b>2</b> Predictive / Supervised Learning</a><ul>
<li class="chapter" data-level="2.1" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#explanation-vs-prediction"><i class="fa fa-check"></i><b>2.1</b> Explanation vs Prediction</a><ul>
<li class="chapter" data-level="2.1.1" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#differences"><i class="fa fa-check"></i><b>2.1.1</b> Differences</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#two-types-of-predictive-models"><i class="fa fa-check"></i><b>2.2</b> Two Types of Predictive Models:</a></li>
<li class="chapter" data-level="2.3" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#bias-vs-variance"><i class="fa fa-check"></i><b>2.3</b> Bias vs Variance</a><ul>
<li class="chapter" data-level="2.3.1" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#bias"><i class="fa fa-check"></i><b>2.3.1</b> Bias</a></li>
<li class="chapter" data-level="2.3.2" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#variance"><i class="fa fa-check"></i><b>2.3.2</b> Variance</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#overfitting-and-underfitting"><i class="fa fa-check"></i><b>2.4</b> Overfitting and Underfitting</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="a-motivating-example-linear-regression.html"><a href="a-motivating-example-linear-regression.html"><i class="fa fa-check"></i><b>3</b> A Motivating Example - Linear Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="a-motivating-example-linear-regression.html"><a href="a-motivating-example-linear-regression.html#scatterplot"><i class="fa fa-check"></i><b>3.1</b> Scatterplot</a></li>
<li class="chapter" data-level="3.2" data-path="a-motivating-example-linear-regression.html"><a href="a-motivating-example-linear-regression.html#correlations"><i class="fa fa-check"></i><b>3.2</b> Correlations</a></li>
<li class="chapter" data-level="3.3" data-path="a-motivating-example-linear-regression.html"><a href="a-motivating-example-linear-regression.html#building-a-model---in-sample-error"><i class="fa fa-check"></i><b>3.3</b> Building A Model - In Sample Error</a></li>
<li class="chapter" data-level="3.4" data-path="a-motivating-example-linear-regression.html"><a href="a-motivating-example-linear-regression.html#out-of-sample-data"><i class="fa fa-check"></i><b>3.4</b> Out Of Sample Data</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>4</b> Decision Trees</a><ul>
<li class="chapter" data-level="4.1" data-path="decision-trees.html"><a href="decision-trees.html#advantages"><i class="fa fa-check"></i><b>4.1</b> Advantages</a></li>
<li class="chapter" data-level="4.2" data-path="decision-trees.html"><a href="decision-trees.html#a-classification-example"><i class="fa fa-check"></i><b>4.2</b> A Classification Example</a><ul>
<li class="chapter" data-level="4.2.1" data-path="decision-trees.html"><a href="decision-trees.html#evaluating-performance"><i class="fa fa-check"></i><b>4.2.1</b> Evaluating performance</a></li>
<li class="chapter" data-level="4.2.2" data-path="decision-trees.html"><a href="decision-trees.html#tree-splitting"><i class="fa fa-check"></i><b>4.2.2</b> Tree Splitting</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="decision-trees.html"><a href="decision-trees.html#gini-index"><i class="fa fa-check"></i><b>4.3</b> Gini Index</a></li>
<li class="chapter" data-level="4.4" data-path="decision-trees.html"><a href="decision-trees.html#regression-trees"><i class="fa fa-check"></i><b>4.4</b> Regression Trees</a><ul>
<li class="chapter" data-level="4.4.1" data-path="decision-trees.html"><a href="decision-trees.html#performance-measure"><i class="fa fa-check"></i><b>4.4.1</b> Performance Measure</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="decision-trees.html"><a href="decision-trees.html#parameters-vs-hyperparameters"><i class="fa fa-check"></i><b>4.5</b> Parameters vs Hyperparameters</a></li>
<li class="chapter" data-level="4.6" data-path="decision-trees.html"><a href="decision-trees.html#grid-searching"><i class="fa fa-check"></i><b>4.6</b> Grid Searching</a></li>
<li class="chapter" data-level="4.7" data-path="decision-trees.html"><a href="decision-trees.html#bagged-trees"><i class="fa fa-check"></i><b>4.7</b> Bagged Trees</a></li>
<li class="chapter" data-level="4.8" data-path="decision-trees.html"><a href="decision-trees.html#random-forests"><i class="fa fa-check"></i><b>4.8</b> Random Forests</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="training-test-data.html"><a href="training-test-data.html"><i class="fa fa-check"></i><b>5</b> Training / Test Data</a><ul>
<li class="chapter" data-level="5.1" data-path="training-test-data.html"><a href="training-test-data.html#cross-fold-validation-continued"><i class="fa fa-check"></i><b>5.1</b> Cross Fold Validation Continued</a></li>
<li class="chapter" data-level="5.2" data-path="training-test-data.html"><a href="training-test-data.html#create-a-function-to-automate-things"><i class="fa fa-check"></i><b>5.2</b> Create A Function To Automate Things</a></li>
<li class="chapter" data-level="5.3" data-path="training-test-data.html"><a href="training-test-data.html#repeated-cross-validation"><i class="fa fa-check"></i><b>5.3</b> Repeated Cross Validation</a></li>
<li class="chapter" data-level="5.4" data-path="training-test-data.html"><a href="training-test-data.html#bootstrap"><i class="fa fa-check"></i><b>5.4</b> Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="using-methods-other-than-lm.html"><a href="using-methods-other-than-lm.html"><i class="fa fa-check"></i><b>6</b> Using Methods Other Than lm</a><ul>
<li class="chapter" data-level="6.1" data-path="using-methods-other-than-lm.html"><a href="using-methods-other-than-lm.html#parameters-vs-hyperparameters-1"><i class="fa fa-check"></i><b>6.1</b> Parameters vs Hyperparameters</a></li>
<li class="chapter" data-level="6.2" data-path="using-methods-other-than-lm.html"><a href="using-methods-other-than-lm.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>6.2</b> Hyperparameter Tuning</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="caret-package.html"><a href="caret-package.html"><i class="fa fa-check"></i><b>7</b> Caret Package</a><ul>
<li class="chapter" data-level="7.1" data-path="caret-package.html"><a href="caret-package.html#putting-caret-to-work"><i class="fa fa-check"></i><b>7.1</b> Putting caret To Work</a></li>
<li class="chapter" data-level="7.2" data-path="caret-package.html"><a href="caret-package.html#back-to-the-beginning"><i class="fa fa-check"></i><b>7.2</b> Back To The Beginning</a></li>
<li class="chapter" data-level="7.3" data-path="caret-package.html"><a href="caret-package.html#splitting"><i class="fa fa-check"></i><b>7.3</b> Splitting</a></li>
<li class="chapter" data-level="7.4" data-path="caret-package.html"><a href="caret-package.html#one-size-fits-all"><i class="fa fa-check"></i><b>7.4</b> One Size Fits All</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html"><i class="fa fa-check"></i><b>8</b> Picking The Best Model</a><ul>
<li class="chapter" data-level="8.1" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#using-the-resamples-function"><i class="fa fa-check"></i><b>8.1</b> Using the resamples() function</a></li>
<li class="chapter" data-level="8.2" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#model-performance"><i class="fa fa-check"></i><b>8.2</b> Model Performance</a></li>
<li class="chapter" data-level="8.3" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#feature-selection"><i class="fa fa-check"></i><b>8.3</b> Feature Selection</a><ul>
<li class="chapter" data-level="8.3.1" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#recursive-feature-elimination"><i class="fa fa-check"></i><b>8.3.1</b> Recursive Feature Elimination</a></li>
<li class="chapter" data-level="8.3.2" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#redundant-feature-removal"><i class="fa fa-check"></i><b>8.3.2</b> Redundant Feature Removal</a></li>
<li class="chapter" data-level="8.3.3" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#feature-importance"><i class="fa fa-check"></i><b>8.3.3</b> Feature Importance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="data-pre-processing.html"><a href="data-pre-processing.html"><i class="fa fa-check"></i><b>9</b> Data Pre Processing</a><ul>
<li class="chapter" data-level="9.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#types-of-pre-processing"><i class="fa fa-check"></i><b>9.1</b> Types of Pre Processing</a></li>
<li class="chapter" data-level="9.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#scaling"><i class="fa fa-check"></i><b>9.2</b> Scaling</a></li>
<li class="chapter" data-level="9.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#missing-data"><i class="fa fa-check"></i><b>9.3</b> Missing Data</a><ul>
<li class="chapter" data-level="9.3.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#finding-rows-with-missing-data"><i class="fa fa-check"></i><b>9.3.1</b> Finding Rows with Missing Data</a></li>
<li class="chapter" data-level="9.3.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#finding-columns-with-missing-data"><i class="fa fa-check"></i><b>9.3.2</b> Finding Columns With Missing Data</a></li>
<li class="chapter" data-level="9.3.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#use-the-median-approach"><i class="fa fa-check"></i><b>9.3.3</b> Use the Median Approach</a></li>
<li class="chapter" data-level="9.3.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#package-based-approach"><i class="fa fa-check"></i><b>9.3.4</b> Package-based Approach</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#look-for-highly-correlated-variables"><i class="fa fa-check"></i><b>9.4</b> Look for Highly Correlated Variables</a></li>
<li class="chapter" data-level="9.5" data-path="data-pre-processing.html"><a href="data-pre-processing.html#scaling-considerations"><i class="fa fa-check"></i><b>9.5</b> Scaling Considerations</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="classification-problems.html"><a href="classification-problems.html"><i class="fa fa-check"></i><b>10</b> Classification Problems</a><ul>
<li class="chapter" data-level="10.1" data-path="classification-problems.html"><a href="classification-problems.html#hypothesis-testing"><i class="fa fa-check"></i><b>10.1</b> Hypothesis Testing</a><ul>
<li class="chapter" data-level="10.1.1" data-path="classification-problems.html"><a href="classification-problems.html#type-i-and-ii-errors"><i class="fa fa-check"></i><b>10.1.1</b> Type I and II Errors</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="classification-problems.html"><a href="classification-problems.html#performance-measures"><i class="fa fa-check"></i><b>10.2</b> Performance Measures</a></li>
<li class="chapter" data-level="10.3" data-path="classification-problems.html"><a href="classification-problems.html#table-of-outcomes"><i class="fa fa-check"></i><b>10.3</b> Table of Outcomes</a><ul>
<li class="chapter" data-level="10.3.1" data-path="classification-problems.html"><a href="classification-problems.html#computing-performance-metrics"><i class="fa fa-check"></i><b>10.3.1</b> Computing Performance Metrics</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="classification-problems.html"><a href="classification-problems.html#picking-the-right-metric"><i class="fa fa-check"></i><b>10.4</b> Picking the Right Metric</a><ul>
<li class="chapter" data-level="10.4.1" data-path="classification-problems.html"><a href="classification-problems.html#working-with-prediction-probabilities"><i class="fa fa-check"></i><b>10.4.1</b> Working With Prediction Probabilities</a></li>
<li class="chapter" data-level="10.4.2" data-path="classification-problems.html"><a href="classification-problems.html#creating-a-roc-curve"><i class="fa fa-check"></i><b>10.4.2</b> Creating a ROC Curve</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="classification-example.html"><a href="classification-example.html"><i class="fa fa-check"></i><b>11</b> Classification Example</a><ul>
<li class="chapter" data-level="11.1" data-path="classification-example.html"><a href="classification-example.html#boxplots-and-densities"><i class="fa fa-check"></i><b>11.1</b> Boxplots And Densities</a></li>
<li class="chapter" data-level="11.2" data-path="classification-example.html"><a href="classification-example.html#generalized-linear-models"><i class="fa fa-check"></i><b>11.2</b> Generalized Linear Models</a></li>
<li class="chapter" data-level="11.3" data-path="classification-example.html"><a href="classification-example.html#random-forests-1"><i class="fa fa-check"></i><b>11.3</b> Random Forests</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html"><i class="fa fa-check"></i><b>12</b> Using External ML Frameworks</a><ul>
<li class="chapter" data-level="12.1" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#using-h2o"><i class="fa fa-check"></i><b>12.1</b> Using h2o</a></li>
<li class="chapter" data-level="12.2" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#create-some-h20-models"><i class="fa fa-check"></i><b>12.2</b> Create Some h20 Models</a></li>
<li class="chapter" data-level="12.3" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#saving-a-model"><i class="fa fa-check"></i><b>12.3</b> Saving A Model</a></li>
<li class="chapter" data-level="12.4" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#using-the-h2o-auto-ml-feature"><i class="fa fa-check"></i><b>12.4</b> Using The h2o Auto ML Feature</a></li>
<li class="chapter" data-level="12.5" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#launching-a-job"><i class="fa fa-check"></i><b>12.5</b> Launching a Job</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Predictive Learning in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="data-pre-processing" class="section level1">
<h1><span class="header-section-number">Chapter 9</span> Data Pre Processing</h1>
<div id="types-of-pre-processing" class="section level2">
<h2><span class="header-section-number">9.1</span> Types of Pre Processing</h2>
<p>Data rarely arrives in a form that is directly suitable for use with a modeling method. There are a number of considerations to make such as how to handle missing data, highly correlated variables, and class imbalances - some categories are over or under represented. Additionally, some variables, also known as “features”, will require transformation or will need to be used to create new variables. Consider the case where the measured data (the numeric data) might be on different scales (e.g. height vs weight). This might result in the need to scale and center the data. Some methods take this into consideration whereas others do not. Suffice it to say that data prep can be an ongoing process that requires a number of experiments before arriving at the best form of data.</p>
</div>
<div id="scaling" class="section level2">
<h2><span class="header-section-number">9.2</span> Scaling</h2>
<p>In terms of what methods benefit (or require) you to scale data prior to use, consider that any method that uses the idea of “distance”. This helps avoid large scale features dominating others. The following approaches benefit from scaling:</p>
<p>Linear/non-linear regression, logistic regression, KNN, SVM, Neural Networks, clustering algorithms like k-means clustering. Methods that employ PCA and dimensionality reduction should use scaled data. In R and Python, many of the functions to employ these methods might have arguements to activate the scaling as part of the process. Some will do it for you.</p>
<p>Methods that don’t require scaling (or whose results don’t rely upon it) include rule-based algorithms such as Decision trees and more generally CART - Random Forests, Gradient Boosted Decision Even if you scale the data the relative relationships will be preserved post scaling so the decision to split a tree won’t be impacted.</p>
</div>
<div id="missing-data" class="section level2">
<h2><span class="header-section-number">9.3</span> Missing Data</h2>
<p>This is a frequent situation in real life. Think of patients checking in for clinic vists over time. Sometimes they come, sometimes they don’t. Sometimes when they do come, their information has changed or some diagnostic test is repeated with a new result which is entered or not. Or, whomever maintains the patient database, decides to add in some new variables to measure for all patients moving forward. This means that all existing patients will have missing values for those new variables. To see how this manifests practically in predictive learning consider the following version of the mtcars data frame which has some missing values:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(readr)
url &lt;-<span class="st"> &quot;https://raw.githubusercontent.com/steviep42/utilities/master/data/mtcars_na.csv&quot;</span>
mtcars_na &lt;-<span class="st"> </span><span class="kw">read_csv</span>(url)</code></pre></div>
<pre><code>## Parsed with column specification:
## cols(
##   mpg = col_double(),
##   cyl = col_double(),
##   disp = col_double(),
##   hp = col_double(),
##   drat = col_double(),
##   wt = col_double(),
##   qsec = col_double(),
##   vs = col_double(),
##   am = col_double(),
##   gear = col_double(),
##   carb = col_double()
## )</code></pre>
<div id="finding-rows-with-missing-data" class="section level3">
<h3><span class="header-section-number">9.3.1</span> Finding Rows with Missing Data</h3>
<p>Ar first glance it looks like all fearures have valid variable values but we can look for missing values which, in R, are indicated by <strong>NA</strong> Base R provides a number of commands to do this. First let’s see how many rows there are in the data frame.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">nrow</span>(mtcars_na)</code></pre></div>
<pre><code>## [1] 32</code></pre>
<p>Now let’s see how many rows have at least one column with a missing value. So we have eight rows in the data frame that contain one or more missing values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sum</span>(<span class="kw">complete.cases</span>(mtcars_na))</code></pre></div>
<pre><code>## [1] 24</code></pre>
</div>
<div id="finding-columns-with-missing-data" class="section level3">
<h3><span class="header-section-number">9.3.2</span> Finding Columns With Missing Data</h3>
<p>What columns have missing values ? Here we leverage the use of the apply family of functions along with the ability to create <strong>anonymous</strong> functions on the fly. Both R and Python provide this capability. We see that the <strong>wt</strong> column has three missing values and the <strong>carb</strong> feature has six missing values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sapply</span>(mtcars_na,<span class="cf">function</span>(x) <span class="kw">sum</span>(<span class="kw">is.na</span>(x)))</code></pre></div>
<pre><code>##  mpg  cyl disp   hp drat   wt qsec   vs   am gear carb 
##    0    0    0    0    0    3    0    0    0    0    6</code></pre>
<p>If we actually wanted to see all rows with missing values:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mtcars_na[<span class="op">!</span><span class="kw">complete.cases</span>(mtcars_na),]</code></pre></div>
<pre><code>## # A tibble: 8 x 11
##     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1  21       6 160     110  3.9  NA     17.0     0     1     4     4
## 2  22.8     4 141.     95  3.92 NA     22.9     1     0     4     2
## 3  19.2     6 168.    123  3.92  3.44  18.3     1     0     4    NA
## 4  16.4     8 276.    180  3.07  4.07  17.4     0     0     3    NA
## 5  30.4     4  75.7    52  4.93  1.62  18.5     1     1     4    NA
## 6  33.9     4  71.1    65  4.22  1.84  19.9     1     1     4    NA
## 7  15.2     8 304     150  3.15 NA     17.3     0     0     3    NA
## 8  30.4     4  95.1   113  3.77  1.51  16.9     1     1     5    NA</code></pre>
<p>What do we do with these ? It depends on a number of things. How hard is it to get this type of data ? If it’s rare information then we probably want to keep all of it because there isn’t that much of it and not all columns are missing for any row. In fact most of the data in a given row is present so maybe one strategy is to tell whatever modeling method we use to ignore the missing values - they might do this by default without you even asking.</p>
<p>We could just filter out any row from the data frame that contains any missing values but in doing so we would lose eight rows of data. This is low stakes data but if this were rare or hard to obtain information then we wouldn’t want to do this.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mtcars_no_na &lt;-<span class="st"> </span>mtcars_na[<span class="kw">complete.cases</span>(mtcars_na),]
<span class="kw">nrow</span>(mtcars_no_na)</code></pre></div>
<pre><code>## [1] 24</code></pre>
</div>
<div id="use-the-median-approach" class="section level3">
<h3><span class="header-section-number">9.3.3</span> Use the Median Approach</h3>
<p>What could we do ? Well we could keep all rows even if they contains NAs and then use <strong>imputation</strong> methods to supply values for the missing information. There are R packages that do this but one quick way to do this without going that route is to replace the missing value in the <strong>wt</strong> column with the <strong>median</strong> value for the entire column. We could first look at a boxplot of this feature to see how those values are distributed:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">boxplot</span>(mtcars_na<span class="op">$</span>wt,<span class="dt">na.rm=</span><span class="ot">TRUE</span>,<span class="dt">main=</span><span class="st">&quot;Distribution of the wt feature&quot;</span>)
<span class="kw">grid</span>()</code></pre></div>
<p><img src="biosml_files/figure-html/unnamed-chunk-73-1.png" width="672" /></p>
<p>To make the substitution would involve the following. First we need to find out which row numbers have missing values for the <strong>wt</strong> feature.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(missing_wt_indices &lt;-<span class="st"> </span><span class="kw">which</span>(<span class="kw">is.na</span>(mtcars_na<span class="op">$</span>wt)))</code></pre></div>
<pre><code>## [1]  2  9 23</code></pre>
<p>Use this information with the data frame bracket notation see the rows where the NAs occur for the <strong>wt</strong> feature</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mtcars_na[missing_wt_indices,]</code></pre></div>
<pre><code>## # A tibble: 3 x 11
##     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1  21       6  160    110  3.9     NA  17.0     0     1     4     4
## 2  22.8     4  141.    95  3.92    NA  22.9     1     0     4     2
## 3  15.2     8  304    150  3.15    NA  17.3     0     0     3    NA</code></pre>
<p>Now do the replacement</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mtcars_na[missing_wt_indices,]<span class="op">$</span>wt &lt;-<span class="st"> </span><span class="kw">median</span>(mtcars_na<span class="op">$</span>wt,<span class="dt">na.rm=</span><span class="ot">TRUE</span>)</code></pre></div>
<p>Verify that the replacement was done successfully. If so, then we should see the value of 3.44 in palce of the previous NA values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mtcars_na[missing_wt_indices,]</code></pre></div>
<pre><code>## # A tibble: 3 x 11
##     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1  21       6  160    110  3.9   3.44  17.0     0     1     4     4
## 2  22.8     4  141.    95  3.92  3.44  22.9     1     0     4     2
## 3  15.2     8  304    150  3.15  3.44  17.3     0     0     3    NA</code></pre>
</div>
<div id="package-based-approach" class="section level3">
<h3><span class="header-section-number">9.3.4</span> Package-based Approach</h3>
<p>This seems like a lot of work and maybe it is if you aren’t up to date with your R skills although, conceptually, this is straightforward and simple. The <strong>Hmisc</strong> package provides an easy way to do this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(Hmisc)</code></pre></div>
<pre><code>## Loading required package: survival</code></pre>
<pre><code>## 
## Attaching package: &#39;survival&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:caret&#39;:
## 
##     cluster</code></pre>
<pre><code>## Loading required package: Formula</code></pre>
<pre><code>## 
## Attaching package: &#39;Hmisc&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:dplyr&#39;:
## 
##     src, summarize</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     format.pval, units</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Reload the versio of mtcars with missing values</span>
url &lt;-<span class="st"> &quot;https://raw.githubusercontent.com/steviep42/utilities/master/data/mtcars_na.csv&quot;</span>
mtcars_na &lt;-<span class="st"> </span><span class="kw">read_csv</span>(url)</code></pre></div>
<pre><code>## Parsed with column specification:
## cols(
##   mpg = col_double(),
##   cyl = col_double(),
##   disp = col_double(),
##   hp = col_double(),
##   drat = col_double(),
##   wt = col_double(),
##   qsec = col_double(),
##   vs = col_double(),
##   am = col_double(),
##   gear = col_double(),
##   carb = col_double()
## )</code></pre>
<p>The <strong>Hmisc</strong> package provides and <strong>impute</strong> function to do the work for us. Check it out. Notice how if finds the rows for which the <strong>wt</strong> feature is missing.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">impute</span>(mtcars_na<span class="op">$</span>wt, median)</code></pre></div>
<pre><code>##      1      2      3      4      5      6      7      8      9     10 
##  2.620 3.440*  2.320  3.215  3.440  3.460  3.570  3.190 3.440*  3.440 
##     11     12     13     14     15     16     17     18     19     20 
##  3.440  4.070  3.730  3.780  5.250  5.424  5.345  2.200  1.615  1.835 
##     21     22     23     24     25     26     27     28     29     30 
##  2.465  3.520 3.440*  3.840  3.845  1.935  2.140  1.513  3.170  2.770 
##     31     32 
##  3.570  2.780</code></pre>
<p>To do the replacement is straightforward.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mtcars_na<span class="op">$</span>wt &lt;-<span class="st"> </span><span class="kw">impute</span>(mtcars_na<span class="op">$</span>wt,median)</code></pre></div>
<p>Another imputation approach is to use the K-Nearest Neighbors method to find observations that are similar to the ones that contain missing data. The missing values can then be filled using information from the most similar observations. We won’t go into that choosing rather to use the convenience offered by the <strong>caret</strong> package to help us.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Make sure caret is loaded</span>
<span class="co"># library(caret)</span></code></pre></div>
<p>So if we choose to use caret we’ll need to use the <strong>preProcess</strong> function to signal our intent to use imputation - in this case the K-Nearest Neighbors technique.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">processed_mtcars &lt;-<span class="st"> </span><span class="kw">preProcess</span>(mtcars_na,
                               <span class="dt">method =</span> <span class="st">&quot;knnImpute&quot;</span>)</code></pre></div>
<p>This returns an object that has lots of information in it:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">names</span>(processed_mtcars)</code></pre></div>
<pre><code>##  [1] &quot;dim&quot;               &quot;bc&quot;                &quot;yj&quot;               
##  [4] &quot;et&quot;                &quot;invHyperbolicSine&quot; &quot;mean&quot;             
##  [7] &quot;std&quot;               &quot;ranges&quot;            &quot;rotation&quot;         
## [10] &quot;method&quot;            &quot;thresh&quot;            &quot;pcaComp&quot;          
## [13] &quot;numComp&quot;           &quot;ica&quot;               &quot;wildcards&quot;        
## [16] &quot;k&quot;                 &quot;knnSummary&quot;        &quot;bagImp&quot;           
## [19] &quot;median&quot;            &quot;data&quot;              &quot;rangeBounds&quot;</code></pre>
<p>The processed data can be accessed as follows. Note that it is also scaled and centered.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">processed_mtcars<span class="op">$</span>data</code></pre></div>
<pre><code>## # A tibble: 26 x 10
##       mpg    cyl    disp     hp   drat   qsec     vs     am   gear   carb
##     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
##  1  0.151 -0.105 -0.571  -0.535  0.568 -0.777 -0.868  1.19   0.424  0.627
##  2  0.151 -0.105 -0.571  -0.535  0.568 -0.464 -0.868  1.19   0.424  0.627
##  3  0.450 -1.22  -0.990  -0.783  0.474  0.426  1.12   1.19   0.424 -1.12 
##  4  0.217 -0.105  0.220  -0.535 -0.966  0.890  1.12  -0.814 -0.932 -1.12 
##  5 -0.231  1.01   1.04    0.413 -0.835 -0.464 -0.868 -0.814 -0.932 -0.537
##  6 -0.330 -0.105 -0.0462 -0.608 -1.56   1.33   1.12  -0.814 -0.932 -1.12 
##  7 -0.961  1.01   1.04    1.43  -0.723 -1.12  -0.868 -0.814 -0.932  0.627
##  8  0.715 -1.22  -0.678  -1.24   0.175  1.20   1.12  -0.814  0.424 -0.537
##  9  0.450 -1.22  -0.726  -0.754  0.605  2.83   1.12  -0.814  0.424 -0.537
## 10 -0.380 -0.105 -0.509  -0.345  0.605  0.588  1.12  -0.814  0.424  0.627
## # … with 16 more rows</code></pre>
<p>Now we could then use this processed data to do some training. We’ll use the <strong>predict</strong> function to give us the mtcars_na data with the imputed values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train_data &lt;-<span class="st"> </span><span class="kw">predict</span>(processed_mtcars,mtcars_na)</code></pre></div>
<p>Now pass this to the <strong>train</strong> function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lmFit &lt;-<span class="st"> </span><span class="kw">train</span>(mpg<span class="op">~</span>wt,
              <span class="dt">data =</span> train_data,
              <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>,
              <span class="dt">metric =</span> <span class="st">&quot;RMSE&quot;</span>
        )</code></pre></div>
</div>
</div>
<div id="look-for-highly-correlated-variables" class="section level2">
<h2><span class="header-section-number">9.4</span> Look for Highly Correlated Variables</h2>
<p>In an earlier section we looked at the correlations between the variables in the mtcars data frame.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(mtcars)
correlations &lt;-<span class="st"> </span><span class="kw">cor</span>(mtcars)
correlations[<span class="dv">1</span><span class="op">:</span><span class="dv">6</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">6</span>]</code></pre></div>
<pre><code>##             mpg        cyl       disp         hp       drat         wt
## mpg   1.0000000 -0.8521620 -0.8475514 -0.7761684  0.6811719 -0.8676594
## cyl  -0.8521620  1.0000000  0.9020329  0.8324475 -0.6999381  0.7824958
## disp -0.8475514  0.9020329  1.0000000  0.7909486 -0.7102139  0.8879799
## hp   -0.7761684  0.8324475  0.7909486  1.0000000 -0.4487591  0.6587479
## drat  0.6811719 -0.6999381 -0.7102139 -0.4487591  1.0000000 -0.7124406
## wt   -0.8676594  0.7824958  0.8879799  0.6587479 -0.7124406  1.0000000</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">suppressMessages</span>(<span class="kw">library</span>(corrplot))
<span class="kw">corrplot</span>(correlations, <span class="dt">order=</span><span class="st">&quot;hclust&quot;</span>)</code></pre></div>
<p><img src="biosml_files/figure-html/unnamed-chunk-88-1.png" width="672" /></p>
<p>The <strong>caret</strong> package has some functions that can help us identify highly correlated variables that might be a candidates for removal prior to use in building a model. Let’s go back to the mtcars data set as it exists by default. One of the variables that is highly correlated with others is <strong>mpg</strong> Since that is the one we are trying to predict, we’ll keep it around.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(highcorr &lt;-<span class="st"> </span><span class="kw">findCorrelation</span>(correlations, <span class="dt">cutoff=</span>.<span class="dv">75</span>))</code></pre></div>
<pre><code>## [1]  2  3  1 10</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">new_mtcars &lt;-<span class="st"> </span>mtcars[,<span class="op">-</span>highcorr[highcorr <span class="op">!=</span><span class="st"> </span><span class="dv">1</span>]]</code></pre></div>
</div>
<div id="scaling-considerations" class="section level2">
<h2><span class="header-section-number">9.5</span> Scaling Considerations</h2>
<p>One thing we skipped over entirely is the issue of data scale. Well, actually, I did mention it at the beginning but then moved on. This is actually a big deal since the data we have been working on has variables measures on different scales. In fact, some of them are actually not continuous quantities. The columns of mtcars that might be categories or factors include cyl, am, vs, gear, and carb. Why do I say this ? We’ll they only take on a specific set of values over all observations</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sapply</span>(new_mtcars,<span class="cf">function</span>(x) <span class="kw">length</span>(<span class="kw">unique</span>(x)))</code></pre></div>
<pre><code>##  mpg   hp drat   wt qsec   vs   am carb 
##   25   22   22   29   30    2    2    6</code></pre>
<p>So, technically, we could turn these into factors before doing anything with the data. We’ll handle these types of variables momentarily. But for now let’s look at a pairs plot of the new_mtcars data frame.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pairs</span>(new_mtcars)</code></pre></div>
<p><img src="biosml_files/figure-html/unnamed-chunk-92-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sapply</span>(new_mtcars,range)</code></pre></div>
<pre><code>##       mpg  hp drat    wt qsec vs am carb
## [1,] 10.4  52 2.76 1.513 14.5  0  0    1
## [2,] 33.9 335 4.93 5.424 22.9  1  1    8</code></pre>
<p>Anyway, the caret package provides a way to easily scale the data prior to the processing of it. We can do this as we call the <strong>train</strong> function. Note that the scaling happens underneath the covers. First, let’s create a Train / Test pair.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">idx &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(new_mtcars<span class="op">$</span>mpg, <span class="dt">p =</span> .<span class="dv">8</span>, 
                                           <span class="dt">list =</span> <span class="ot">FALSE</span>, 
                                           <span class="dt">times =</span> <span class="dv">1</span>)
Train &lt;-<span class="st"> </span>mtcars[ idx,]
Test  &lt;-<span class="st"> </span>mtcars[<span class="op">-</span>idx,]</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">my_ctrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(
    <span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, 
    <span class="dt">number =</span> <span class="dv">3</span>
  )

<span class="co"># Check the preProcess argument below</span>

mod_rf &lt;-<span class="st"> </span><span class="kw">train</span>(mpg <span class="op">~</span><span class="st"> </span>., 
                <span class="dt">data=</span>Train, 
                <span class="dt">method =</span> <span class="st">&quot;rf&quot;</span>, 
                <span class="dt">preProcess =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>,<span class="st">&quot;scale&quot;</span>),
                <span class="dt">trControl =</span> my_ctrl
                )

<span class="kw">compute_rmse</span>(<span class="kw">predict</span>(mod_rf,Train),Train<span class="op">$</span>mpg)</code></pre></div>
<pre><code>## [1] 1.073862</code></pre>
<p>Another possibility exists in that we could use the <strong>preProcess</strong> function in advance of calling the <strong>train</strong> function. We’ll work with the Train and Test data from above.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">proc_train &lt;-<span class="st"> </span><span class="kw">preProcess</span>(Train,<span class="dt">method =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>,<span class="st">&quot;scale&quot;</span>))
scaled_train &lt;-<span class="st"> </span><span class="kw">predict</span>(proc_train, Train)

<span class="co"># Then we would use the data along with the train function</span>

mod_lm_sc &lt;-<span class="st"> </span><span class="kw">train</span>(mpg<span class="op">~</span>.,<span class="dt">data=</span>scaled_train,<span class="dt">method=</span><span class="st">&quot;lm&quot;</span>)</code></pre></div>
<p>If we wanted to then predict against a new data set we would then need to scale the new data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Test_sc &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">scale</span>(Test))
<span class="kw">predict</span>(mod_lm_sc,Test_sc)</code></pre></div>
<pre><code>## Datsun 710    Valiant Duster 360   Fiat 128 
##  0.8126610 -0.3719585 -1.4427770  1.0020745</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">compute_rmse</span>(<span class="kw">predict</span>(mod_lm_sc,Test_sc),Test_sc<span class="op">$</span>mpg)</code></pre></div>
<pre><code>## [1] 0.4575367</code></pre>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="picking-the-best-model.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="classification-problems.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
