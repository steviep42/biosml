<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Decision Trees | Predictive Learning in R</title>
  <meta name="description" content="Chapter 8 Decision Trees | Predictive Learning in R" />
  <meta name="generator" content="bookdown 0.14 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Decision Trees | Predictive Learning in R" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Decision Trees | Predictive Learning in R" />
  
  
  

<meta name="author" content="Steve Pittard" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="classification-example.html"/>
<link rel="next" href="using-methods-other-than-lm.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#machine-learning"><i class="fa fa-check"></i><b>1.1</b> Machine Learning</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#predictive-modeling"><i class="fa fa-check"></i><b>1.2</b> Predictive Modeling</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#in-sample-vs-out-of-sample-data"><i class="fa fa-check"></i><b>1.3</b> In-Sample vs Out-Of-Sample Data</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#performance-metrics"><i class="fa fa-check"></i><b>1.4</b> Performance Metrics</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#black-box"><i class="fa fa-check"></i><b>1.5</b> Black Box</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html"><i class="fa fa-check"></i><b>2</b> Predictive / Supervised Learning</a><ul>
<li class="chapter" data-level="2.1" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#explanation-vs-prediction"><i class="fa fa-check"></i><b>2.1</b> Explanation vs Prediction</a></li>
<li class="chapter" data-level="2.2" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#two-types-of-predictive-models"><i class="fa fa-check"></i><b>2.2</b> Two Types of Predictive Models:</a></li>
<li class="chapter" data-level="2.3" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#bias-vs-variance"><i class="fa fa-check"></i><b>2.3</b> Bias vs Variance</a><ul>
<li class="chapter" data-level="2.3.1" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#bias"><i class="fa fa-check"></i><b>2.3.1</b> Bias</a></li>
<li class="chapter" data-level="2.3.2" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#variance"><i class="fa fa-check"></i><b>2.3.2</b> Variance</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#overfitting-and-underfitting"><i class="fa fa-check"></i><b>2.4</b> Overfitting and Underfitting</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="a-motivating-example.html"><a href="a-motivating-example.html"><i class="fa fa-check"></i><b>3</b> A Motivating Example</a><ul>
<li class="chapter" data-level="3.1" data-path="a-motivating-example.html"><a href="a-motivating-example.html#suggested-workflow"><i class="fa fa-check"></i><b>3.1</b> Suggested Workflow</a></li>
<li class="chapter" data-level="3.2" data-path="a-motivating-example.html"><a href="a-motivating-example.html#scatterplot"><i class="fa fa-check"></i><b>3.2</b> Scatterplot</a></li>
<li class="chapter" data-level="3.3" data-path="a-motivating-example.html"><a href="a-motivating-example.html#correlations"><i class="fa fa-check"></i><b>3.3</b> Correlations</a></li>
<li class="chapter" data-level="3.4" data-path="a-motivating-example.html"><a href="a-motivating-example.html#building-a-model---in-sample-error"><i class="fa fa-check"></i><b>3.4</b> Building A Model - In Sample Error</a></li>
<li class="chapter" data-level="3.5" data-path="a-motivating-example.html"><a href="a-motivating-example.html#out-of-sample-data"><i class="fa fa-check"></i><b>3.5</b> Out Of Sample Data</a></li>
<li class="chapter" data-level="3.6" data-path="a-motivating-example.html"><a href="a-motivating-example.html#other-methods"><i class="fa fa-check"></i><b>3.6</b> Other Methods ?</a></li>
<li class="chapter" data-level="3.7" data-path="a-motivating-example.html"><a href="a-motivating-example.html#summary"><i class="fa fa-check"></i><b>3.7</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="training-test-data.html"><a href="training-test-data.html"><i class="fa fa-check"></i><b>4</b> Training / Test Data</a><ul>
<li class="chapter" data-level="4.1" data-path="training-test-data.html"><a href="training-test-data.html#cross-fold-validation"><i class="fa fa-check"></i><b>4.1</b> Cross Fold Validation</a></li>
<li class="chapter" data-level="4.2" data-path="training-test-data.html"><a href="training-test-data.html#create-a-function-to-automate-things"><i class="fa fa-check"></i><b>4.2</b> Create A Function To Automate Things</a></li>
<li class="chapter" data-level="4.3" data-path="training-test-data.html"><a href="training-test-data.html#repeated-cross-validation"><i class="fa fa-check"></i><b>4.3</b> Repeated Cross Validation</a></li>
<li class="chapter" data-level="4.4" data-path="training-test-data.html"><a href="training-test-data.html#bootstrap"><i class="fa fa-check"></i><b>4.4</b> Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="caret-package.html"><a href="caret-package.html"><i class="fa fa-check"></i><b>5</b> Caret Package</a><ul>
<li class="chapter" data-level="5.1" data-path="caret-package.html"><a href="caret-package.html#putting-caret-to-work"><i class="fa fa-check"></i><b>5.1</b> Putting caret To Work</a></li>
<li class="chapter" data-level="5.2" data-path="caret-package.html"><a href="caret-package.html#back-to-the-beginning"><i class="fa fa-check"></i><b>5.2</b> Back To The Beginning</a></li>
<li class="chapter" data-level="5.3" data-path="caret-package.html"><a href="caret-package.html#splitting"><i class="fa fa-check"></i><b>5.3</b> Splitting</a></li>
<li class="chapter" data-level="5.4" data-path="caret-package.html"><a href="caret-package.html#calling-the-train-function"><i class="fa fa-check"></i><b>5.4</b> Calling The train() Function</a></li>
<li class="chapter" data-level="5.5" data-path="caret-package.html"><a href="caret-package.html#one-size-fits-all"><i class="fa fa-check"></i><b>5.5</b> One Size Fits All</a></li>
<li class="chapter" data-level="5.6" data-path="caret-package.html"><a href="caret-package.html#hyperparameters"><i class="fa fa-check"></i><b>5.6</b> Hyperparameters</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="classification-problems.html"><a href="classification-problems.html"><i class="fa fa-check"></i><b>6</b> Classification Problems</a><ul>
<li class="chapter" data-level="6.1" data-path="classification-problems.html"><a href="classification-problems.html#performance-measures"><i class="fa fa-check"></i><b>6.1</b> Performance Measures</a></li>
<li class="chapter" data-level="6.2" data-path="classification-problems.html"><a href="classification-problems.html#important-terminology"><i class="fa fa-check"></i><b>6.2</b> Important Terminology</a></li>
<li class="chapter" data-level="6.3" data-path="classification-problems.html"><a href="classification-problems.html#a-basic-model"><i class="fa fa-check"></i><b>6.3</b> A Basic Model</a></li>
<li class="chapter" data-level="6.4" data-path="classification-problems.html"><a href="classification-problems.html#selecting-the-correct-alpha"><i class="fa fa-check"></i><b>6.4</b> Selecting The Correct Alpha</a></li>
<li class="chapter" data-level="6.5" data-path="classification-problems.html"><a href="classification-problems.html#hypothesis-testing"><i class="fa fa-check"></i><b>6.5</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="6.6" data-path="classification-problems.html"><a href="classification-problems.html#confusion-matrix"><i class="fa fa-check"></i><b>6.6</b> Confusion Matrix</a><ul>
<li class="chapter" data-level="6.6.1" data-path="classification-problems.html"><a href="classification-problems.html#computing-performance-metrics"><i class="fa fa-check"></i><b>6.6.1</b> Computing Performance Metrics</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="classification-problems.html"><a href="classification-problems.html#picking-the-right-metric"><i class="fa fa-check"></i><b>6.7</b> Picking the Right Metric</a></li>
<li class="chapter" data-level="6.8" data-path="classification-problems.html"><a href="classification-problems.html#wait.-where-are-we"><i class="fa fa-check"></i><b>6.8</b> Wait. Where Are We ?</a></li>
<li class="chapter" data-level="6.9" data-path="classification-problems.html"><a href="classification-problems.html#better-ways-to-compute-the-roc-curve"><i class="fa fa-check"></i><b>6.9</b> Better Ways To Compute The ROC Curve</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="classification-example.html"><a href="classification-example.html"><i class="fa fa-check"></i><b>7</b> Classification Example</a><ul>
<li class="chapter" data-level="7.1" data-path="classification-example.html"><a href="classification-example.html#exploratory-plots"><i class="fa fa-check"></i><b>7.1</b> Exploratory Plots</a></li>
<li class="chapter" data-level="7.2" data-path="classification-example.html"><a href="classification-example.html#generalized-linear-models"><i class="fa fa-check"></i><b>7.2</b> Generalized Linear Models</a></li>
<li class="chapter" data-level="7.3" data-path="classification-example.html"><a href="classification-example.html#random-forests"><i class="fa fa-check"></i><b>7.3</b> Random Forests</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>8</b> Decision Trees</a><ul>
<li class="chapter" data-level="8.1" data-path="decision-trees.html"><a href="decision-trees.html#advantages"><i class="fa fa-check"></i><b>8.1</b> Advantages</a></li>
<li class="chapter" data-level="8.2" data-path="decision-trees.html"><a href="decision-trees.html#a-classification-example"><i class="fa fa-check"></i><b>8.2</b> A Classification Example</a><ul>
<li class="chapter" data-level="8.2.1" data-path="decision-trees.html"><a href="decision-trees.html#evaluating-performance"><i class="fa fa-check"></i><b>8.2.1</b> Evaluating performance</a></li>
<li class="chapter" data-level="8.2.2" data-path="decision-trees.html"><a href="decision-trees.html#tree-splitting"><i class="fa fa-check"></i><b>8.2.2</b> Tree Splitting</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="decision-trees.html"><a href="decision-trees.html#gini-index"><i class="fa fa-check"></i><b>8.3</b> Gini Index</a></li>
<li class="chapter" data-level="8.4" data-path="decision-trees.html"><a href="decision-trees.html#regression-trees"><i class="fa fa-check"></i><b>8.4</b> Regression Trees</a><ul>
<li class="chapter" data-level="8.4.1" data-path="decision-trees.html"><a href="decision-trees.html#performance-measure"><i class="fa fa-check"></i><b>8.4.1</b> Performance Measure</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="decision-trees.html"><a href="decision-trees.html#parameters-vs-hyperparameters"><i class="fa fa-check"></i><b>8.5</b> Parameters vs Hyperparameters</a></li>
<li class="chapter" data-level="8.6" data-path="decision-trees.html"><a href="decision-trees.html#grid-searching"><i class="fa fa-check"></i><b>8.6</b> Grid Searching</a></li>
<li class="chapter" data-level="8.7" data-path="decision-trees.html"><a href="decision-trees.html#bagged-trees"><i class="fa fa-check"></i><b>8.7</b> Bagged Trees</a></li>
<li class="chapter" data-level="8.8" data-path="decision-trees.html"><a href="decision-trees.html#random-forests-1"><i class="fa fa-check"></i><b>8.8</b> Random Forests</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="using-methods-other-than-lm.html"><a href="using-methods-other-than-lm.html"><i class="fa fa-check"></i><b>9</b> Using Methods Other Than lm</a><ul>
<li class="chapter" data-level="9.1" data-path="using-methods-other-than-lm.html"><a href="using-methods-other-than-lm.html#parameters-vs-hyperparameters-1"><i class="fa fa-check"></i><b>9.1</b> Parameters vs Hyperparameters</a></li>
<li class="chapter" data-level="9.2" data-path="using-methods-other-than-lm.html"><a href="using-methods-other-than-lm.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>9.2</b> Hyperparameter Tuning</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html"><i class="fa fa-check"></i><b>10</b> Picking The Best Model</a><ul>
<li class="chapter" data-level="10.1" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#an-example"><i class="fa fa-check"></i><b>10.1</b> An Example</a></li>
<li class="chapter" data-level="10.2" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#more-comparisons"><i class="fa fa-check"></i><b>10.2</b> More Comparisons</a></li>
<li class="chapter" data-level="10.3" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#using-the-resamples-function"><i class="fa fa-check"></i><b>10.3</b> Using the resamples() function</a></li>
<li class="chapter" data-level="10.4" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#model-performance"><i class="fa fa-check"></i><b>10.4</b> Model Performance</a></li>
<li class="chapter" data-level="10.5" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#feature-selection"><i class="fa fa-check"></i><b>10.5</b> Feature Selection</a><ul>
<li class="chapter" data-level="10.5.1" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#recursive-feature-elimination"><i class="fa fa-check"></i><b>10.5.1</b> Recursive Feature Elimination</a></li>
<li class="chapter" data-level="10.5.2" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#redundant-feature-removal"><i class="fa fa-check"></i><b>10.5.2</b> Redundant Feature Removal</a></li>
<li class="chapter" data-level="10.5.3" data-path="picking-the-best-model.html"><a href="picking-the-best-model.html#feature-importance"><i class="fa fa-check"></i><b>10.5.3</b> Feature Importance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="data-pre-processing.html"><a href="data-pre-processing.html"><i class="fa fa-check"></i><b>11</b> Data Pre Processing</a><ul>
<li class="chapter" data-level="11.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#types-of-pre-processing"><i class="fa fa-check"></i><b>11.1</b> Types of Pre Processing</a></li>
<li class="chapter" data-level="11.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#missing-values"><i class="fa fa-check"></i><b>11.2</b> Missing Values</a><ul>
<li class="chapter" data-level="11.2.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#finding-rows-with-missing-data"><i class="fa fa-check"></i><b>11.2.1</b> Finding Rows with Missing Data</a></li>
<li class="chapter" data-level="11.2.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#finding-columns-with-missing-data"><i class="fa fa-check"></i><b>11.2.2</b> Finding Columns With Missing Data</a></li>
<li class="chapter" data-level="11.2.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#use-the-median-approach"><i class="fa fa-check"></i><b>11.2.3</b> Use the Median Approach</a></li>
<li class="chapter" data-level="11.2.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#package-based-approach"><i class="fa fa-check"></i><b>11.2.4</b> Package-based Approach</a></li>
<li class="chapter" data-level="11.2.5" data-path="data-pre-processing.html"><a href="data-pre-processing.html#using-caret"><i class="fa fa-check"></i><b>11.2.5</b> Using caret</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#scaling"><i class="fa fa-check"></i><b>11.3</b> Scaling</a><ul>
<li class="chapter" data-level="11.3.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#methods-that-benefit-from-scaling"><i class="fa fa-check"></i><b>11.3.1</b> Methods That Benefit From Scaling</a></li>
<li class="chapter" data-level="11.3.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#methods-that-do-not-require-scaling"><i class="fa fa-check"></i><b>11.3.2</b> Methods That Do Not Require Scaling</a></li>
<li class="chapter" data-level="11.3.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#how-to-scale"><i class="fa fa-check"></i><b>11.3.3</b> How To Scale</a></li>
<li class="chapter" data-level="11.3.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#order-of-processing"><i class="fa fa-check"></i><b>11.3.4</b> Order of Processing</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#low-variance-variables"><i class="fa fa-check"></i><b>11.4</b> Low Variance Variables</a></li>
<li class="chapter" data-level="11.5" data-path="data-pre-processing.html"><a href="data-pre-processing.html#order-of-pre-processing"><i class="fa fa-check"></i><b>11.5</b> Order of Pre-Processing</a></li>
<li class="chapter" data-level="11.6" data-path="data-pre-processing.html"><a href="data-pre-processing.html#identifying-redundant-features"><i class="fa fa-check"></i><b>11.6</b> Identifying Redundant Features</a><ul>
<li class="chapter" data-level="11.6.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#highly-correlated-variables"><i class="fa fa-check"></i><b>11.6.1</b> Highly Correlated Variables</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="data-pre-processing.html"><a href="data-pre-processing.html#ranking-features"><i class="fa fa-check"></i><b>11.7</b> Ranking Features</a></li>
<li class="chapter" data-level="11.8" data-path="data-pre-processing.html"><a href="data-pre-processing.html#feature-selection-1"><i class="fa fa-check"></i><b>11.8</b> Feature Selection</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html"><i class="fa fa-check"></i><b>12</b> Using External ML Frameworks</a><ul>
<li class="chapter" data-level="12.1" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#using-h2o"><i class="fa fa-check"></i><b>12.1</b> Using h2o</a></li>
<li class="chapter" data-level="12.2" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#create-some-h20-models"><i class="fa fa-check"></i><b>12.2</b> Create Some h20 Models</a></li>
<li class="chapter" data-level="12.3" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#saving-a-model"><i class="fa fa-check"></i><b>12.3</b> Saving A Model</a></li>
<li class="chapter" data-level="12.4" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#using-the-h2o-auto-ml-feature"><i class="fa fa-check"></i><b>12.4</b> Using The h2o Auto ML Feature</a></li>
<li class="chapter" data-level="12.5" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#launching-a-job"><i class="fa fa-check"></i><b>12.5</b> Launching a Job</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Predictive Learning in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="decision-trees" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Decision Trees</h1>
<p>You might also know about the topic of Classification Trees under the name of <strong>CART</strong> which stands for <strong>C</strong>lassification <strong>A</strong>nd <strong>R</strong>egression Trees. Decision trees can be used to predict numeric outcomes or to classify binary outcomes such as a disease result “yes” or “now”, “positive or negative”.</p>
<p>The theme common to both types of trees is that you are trying to predict some outcome variable as a function of some number and combination of predictor variables from the same data set. The outcome will be a “tree” that can usually be easily understood and interpreted. As an example, a very simple decision tree can be generated that predicts / classifies the transmission type of cars in the mtcars data frame.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(mtcars)
rpart_class_mtcars &lt;-<span class="st"> </span><span class="kw">rpart</span>(am<span class="op">~</span>.,<span class="dt">data=</span>mtcars,<span class="dt">method=</span><span class="st">&quot;class&quot;</span>)
<span class="kw">rpart.plot</span>(rpart_class_mtcars,<span class="dt">yesno =</span> <span class="dv">2</span>, <span class="dt">type =</span> <span class="dv">0</span>, <span class="dt">extra =</span> <span class="dv">0</span>)</code></pre></div>
<p><img src="biosml_files/figure-html/unnamed-chunk-43-1.png" width="672" /></p>
<p>If the weight of the car is greater than 3.2 tons then the tree would classify it as having a manual (1) transmission. How well did we do here ? Not too bad actually. We’ll talk more about measuring performance of a classifier in a bit.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">preds &lt;-<span class="st"> </span><span class="kw">ifelse</span>(mtcars<span class="op">$</span>wt <span class="op">&gt;=</span><span class="st"> </span><span class="dv">3</span>,<span class="dv">0</span>,<span class="dv">1</span>)
<span class="kw">table</span>(<span class="dt">predicted=</span>preds,<span class="dt">actual=</span>mtcars<span class="op">$</span>am)</code></pre></div>
<pre><code>##          actual
## predicted  0  1
##         0 18  2
##         1  1 11</code></pre>
<p>If, however, we wanted to predict the MPG of a car from the mtcars data frame we could still use the <strong>rpart</strong> function but we would adjust some arguments - notably the “method” argument. In any case, we get a prediction for MPG based on some criteria.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(mtcars)
rpart_regress_mtcars &lt;-<span class="st"> </span><span class="kw">rpart</span>(mpg<span class="op">~</span>.,<span class="dt">data=</span>mtcars,<span class="dt">method=</span><span class="st">&quot;anova&quot;</span>)
<span class="kw">rpart.plot</span>(rpart_regress_mtcars,<span class="dt">yesno =</span> <span class="dv">2</span>, <span class="dt">type =</span> <span class="dv">0</span>, <span class="dt">extra =</span> <span class="dv">0</span>)</code></pre></div>
<p><img src="biosml_files/figure-html/unnamed-chunk-45-1.png" width="672" /></p>
<div id="advantages" class="section level2">
<h2><span class="header-section-number">8.1</span> Advantages</h2>
<p>The advantages of tree-based methods include that 1 - The model is generally easy to interpret - The path to a decision is plainly spelled out (assuming that the number of tree splits is easy enough to trace). - The method can handle numeric and categorical - One does not generally need to pre process or normalize data - Missing data is less of a big deal</p>
<p>Disadvantages include:</p>
<ul>
<li>Large trees are hard to follow - variance can be high</li>
<li>Trees can be overly complex</li>
<li>Overfitting can be a problem</li>
</ul>
</div>
<div id="a-classification-example" class="section level2">
<h2><span class="header-section-number">8.2</span> A Classification Example</h2>
<p>Let’s use the Pima Indians data set as it relates to predicting whether someone has diabetes. This data is provided by the <strong>mlbench</strong> package. The relevant variables are:</p>
<pre><code>pregnant - Number of times pregnant
glucose  - Plasma glucose concentration (glucose tolerance test)
pressure - Diastolic blood pressure (mm Hg)
triceps  - Triceps skin fold thickness (mm)
insulin  - 2-Hour serum insulin (mu U/ml)
mass       - Body mass index (weight in kg/(height in m)\^2)
pedigree - Diabetes pedigree function
age      - Age (years)
diabetes - Class variable (test for diabetes)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(mlbench)
<span class="kw">data</span>(PimaIndiansDiabetes)
pm &lt;-<span class="st"> </span>PimaIndiansDiabetes

diabetes_mod &lt;-<span class="st"> </span><span class="kw">rpart</span>(diabetes <span class="op">~</span><span class="st"> </span>.,pm,<span class="dt">method=</span><span class="st">&quot;class&quot;</span>)
<span class="kw">rpart.plot</span>(<span class="dt">x =</span> diabetes_mod, <span class="dt">yesno =</span> <span class="dv">2</span>, <span class="dt">type =</span> <span class="dv">0</span>, <span class="dt">extra =</span> <span class="dv">0</span>)</code></pre></div>
<p><img src="biosml_files/figure-html/unnamed-chunk-46-1.png" width="672" /></p>
<p>That’s pretty understandable but it starts to border on being a bit hard to follow by hand. Nonetheless, you could show this to someone who does not know (or care) about the underlying mathematics of decision trees and they would be able to understand the path to a classification of someone having diabetes or not.</p>
<p>It’s common in these scenarios to create what is known as a test / training split so that we can train our model on a test set that we can then use on a test or hold out data set to see how well the model performs. There is much more to say about this but we’ll keep it simple here. We can create a training set that comprises %80 of the data with a holdout set of 20%. This easy to do.</p>
<p>Let’s create a Train / Test Split</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">123</span>)
percent &lt;-<span class="st"> </span>.<span class="dv">80</span>
train_idx &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(pm),
                      <span class="kw">round</span>(percent<span class="op">*</span><span class="kw">nrow</span>(pm)))

train_idx[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>]</code></pre></div>
<pre><code>##  [1] 221 605 314 676 719  35 403 680 420 347</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Subset the pm data frame to training indices only</span>
pm_train &lt;-<span class="st"> </span>pm[train_idx, ]  
  
<span class="co"># Exclude the training indices to create the test set</span>
pm_test &lt;-<span class="st"> </span>pm[<span class="op">-</span>train_idx, ]  </code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Train the model (to predict &#39;default&#39;)</span>
pm_class_tree &lt;-<span class="st"> </span><span class="kw">rpart</span>(<span class="dt">formula =</span> diabetes <span class="op">~</span><span class="st"> </span>., 
                      <span class="dt">data =</span> pm_train, 
                      <span class="dt">method =</span> <span class="st">&quot;class&quot;</span>)

<span class="co"># Look at the model output                      </span>
pm_class_tree</code></pre></div>
<pre><code>## n= 614 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##   1) root 614 211 neg (0.65635179 0.34364821)  
##     2) glucose&lt; 128.5 401  79 neg (0.80299252 0.19700748)  
##       4) age&lt; 28.5 223  20 neg (0.91031390 0.08968610) *
##       5) age&gt;=28.5 178  59 neg (0.66853933 0.33146067)  
##        10) glucose&lt; 99.5 58   7 neg (0.87931034 0.12068966) *
##        11) glucose&gt;=99.5 120  52 neg (0.56666667 0.43333333)  
##          22) mass&lt; 26.35 24   2 neg (0.91666667 0.08333333) *
##          23) mass&gt;=26.35 96  46 pos (0.47916667 0.52083333)  
##            46) pedigree&lt; 0.197 19   3 neg (0.84210526 0.15789474) *
##            47) pedigree&gt;=0.197 77  30 pos (0.38961039 0.61038961)  
##              94) pedigree&lt; 0.561 47  23 pos (0.48936170 0.51063830)  
##               188) pregnant&gt;=1.5 39  17 neg (0.56410256 0.43589744)  
##                 376) mass&gt;=33.9 20   5 neg (0.75000000 0.25000000) *
##                 377) mass&lt; 33.9 19   7 pos (0.36842105 0.63157895) *
##               189) pregnant&lt; 1.5 8   1 pos (0.12500000 0.87500000) *
##              95) pedigree&gt;=0.561 30   7 pos (0.23333333 0.76666667) *
##     3) glucose&gt;=128.5 213  81 pos (0.38028169 0.61971831)  
##       6) mass&lt; 29.95 59  17 neg (0.71186441 0.28813559) *
##       7) mass&gt;=29.95 154  39 pos (0.25324675 0.74675325)  
##        14) glucose&lt; 155.5 81  30 pos (0.37037037 0.62962963)  
##          28) glucose&gt;=152.5 9   2 neg (0.77777778 0.22222222) *
##          29) glucose&lt; 152.5 72  23 pos (0.31944444 0.68055556)  
##            58) age&lt; 42.5 51  20 pos (0.39215686 0.60784314)  
##             116) mass&lt; 41.65 36  17 neg (0.52777778 0.47222222)  
##               232) mass&gt;=31.8 29  11 neg (0.62068966 0.37931034) *
##               233) mass&lt; 31.8 7   1 pos (0.14285714 0.85714286) *
##             117) mass&gt;=41.65 15   1 pos (0.06666667 0.93333333) *
##            59) age&gt;=42.5 21   3 pos (0.14285714 0.85714286) *
##        15) glucose&gt;=155.5 73   9 pos (0.12328767 0.87671233) *</code></pre>
<div id="evaluating-performance" class="section level3">
<h3><span class="header-section-number">8.2.1</span> Evaluating performance</h3>
<p>So this is where things get interesting. We’ll use a confusion matrix to help us figure some things out about this model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Generate predicted classes using the model object</span>
pm_class_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(<span class="dt">object =</span> pm_class_tree,  
                        <span class="dt">newdata =</span> pm_test,   
                        <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)  
                            
<span class="co"># Calculate the confusion matrix for the test set</span>
caret<span class="op">::</span><span class="kw">confusionMatrix</span>(pm_class_pred,       
                       pm_test<span class="op">$</span>diabetes,
                       <span class="dt">positive=</span><span class="st">&quot;pos&quot;</span>)  </code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction neg pos
##        neg  77  24
##        pos  20  33
##                                          
##                Accuracy : 0.7143         
##                  95% CI : (0.636, 0.7841)
##     No Information Rate : 0.6299         
##     P-Value [Acc &gt; NIR] : 0.01718        
##                                          
##                   Kappa : 0.3782         
##                                          
##  Mcnemar&#39;s Test P-Value : 0.65108        
##                                          
##             Sensitivity : 0.5789         
##             Specificity : 0.7938         
##          Pos Pred Value : 0.6226         
##          Neg Pred Value : 0.7624         
##              Prevalence : 0.3701         
##          Detection Rate : 0.2143         
##    Detection Prevalence : 0.3442         
##       Balanced Accuracy : 0.6864         
##                                          
##        &#39;Positive&#39; Class : pos            
## </code></pre>
<p>We can also look at the Area Under the ROC Curve.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(Metrics)
dt_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(pm_class_tree,
                       <span class="dt">newdata =</span> pm_test, <span class="dt">type=</span><span class="st">&quot;prob&quot;</span>)

converted &lt;-<span class="st"> </span><span class="kw">ifelse</span>(pm_test<span class="op">$</span>diabetes <span class="op">==</span><span class="st"> &quot;pos&quot;</span>,<span class="dv">1</span>,<span class="dv">0</span>)

aucval    &lt;-<span class="st"> </span>Metrics<span class="op">::</span><span class="kw">auc</span>(<span class="dt">actual =</span> converted,
                    <span class="dt">predicted =</span> dt_pred[,<span class="dv">2</span>])

aucval</code></pre></div>
<pre><code>## [1] 0.7804305</code></pre>
</div>
<div id="tree-splitting" class="section level3">
<h3><span class="header-section-number">8.2.2</span> Tree Splitting</h3>
<p>The resulting tree can be thought of as an upside down tree with the root at the top. The “trunk” proceeds downward and splits into subsets based on some decision (hence the word “decision” in the title). When classifying data the idea is to segment or partition data into groups/regions where each group contains or represents a single class (“yes/no”, “positive/negative”).</p>
<p>These groups or regions would represent a “pure” region. This is not always possible so a best effort is made. These regions are separated by decision boundaries which are used to make decisions. We’ll plot some example data to illustrate the case.</p>
<p><img src="biosml_files/figure-html/unnamed-chunk-51-1.png" width="672" /></p>
<p><img src="biosml_files/figure-html/unnamed-chunk-52-1.png" width="672" /></p>
<p><img src="biosml_files/figure-html/unnamed-chunk-53-1.png" width="672" /></p>
</div>
</div>
<div id="gini-index" class="section level2">
<h2><span class="header-section-number">8.3</span> Gini Index</h2>
<p>So we have to find a way to make the decisions such that the resulting regions are as pure as possible. This could be measuring the degree of impurity or purity - so we are either maximinizng purity or minimizing impurity The so called “Gini index”&quot; gives us the degree or measure of impurity.</p>
<p>The lower the Gini index, the lower the degree of impurity (this higher purity). The higher the Gini index the higher the degree of impurity (this lower purity). The decision tree will select the split that minimizes or lowers the Gini index. There are other measures or indices that can be used such as the “information” measure.</p>
<p>Let’s train two models that use a different splitting criterion (“gini” and “information”) and then use the test set to choose a “best” model. To do this you’ll use the <strong>parms</strong> argument of the <strong>rpart</strong> function. This argument takes a named list that contains values of different parameters to influence how the model is trained. Finally, to assess the models we’ll use the <strong>ce</strong> function from the <strong>Metrics</strong> package to show the proportion of elements in actual that are not equal to the corresponding element in predicted.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Train a gini-based model</span>
pm_gini_mod &lt;-<span class="st"> </span><span class="kw">rpart</span>(<span class="dt">formula =</span> diabetes <span class="op">~</span><span class="st"> </span>., 
                       <span class="dt">data =</span> pm_train, 
                       <span class="dt">method =</span> <span class="st">&quot;class&quot;</span>,
                       <span class="dt">parms =</span> <span class="kw">list</span>(<span class="dt">split =</span> <span class="st">&quot;gini&quot;</span>))

<span class="co"># Train an information-based model</span>
pm_info_mod &lt;-<span class="st"> </span><span class="kw">rpart</span>(<span class="dt">formula =</span> diabetes <span class="op">~</span><span class="st"> </span>., 
                       <span class="dt">data =</span> pm_train, 
                       <span class="dt">method =</span> <span class="st">&quot;class&quot;</span>,
                       <span class="dt">parms =</span> <span class="kw">list</span>(<span class="dt">split =</span> <span class="st">&quot;information&quot;</span>))

<span class="co"># Create some predictions</span>
gini_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(<span class="dt">object =</span> pm_gini_mod, 
             <span class="dt">newdata =</span> pm_test,
             <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)    

<span class="co"># Generate predictions on the validation set using the information model</span>
info_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(<span class="dt">object =</span> pm_info_mod, 
             <span class="dt">newdata =</span> pm_test,
             <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)

<span class="co"># Compare classification error</span>

Metrics<span class="op">::</span><span class="kw">ce</span>(<span class="dt">actual =</span> pm_test<span class="op">$</span>diabetes, 
   <span class="dt">predicted =</span> gini_pred)</code></pre></div>
<pre><code>## [1] 0.2857143</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Metrics<span class="op">::</span><span class="kw">ce</span>(<span class="dt">actual =</span> pm_test<span class="op">$</span>diabetes, 
   <span class="dt">predicted =</span> info_pred)  </code></pre></div>
<pre><code>## [1] 0.2922078</code></pre>
</div>
<div id="regression-trees" class="section level2">
<h2><span class="header-section-number">8.4</span> Regression Trees</h2>
<p>Predicting numeric outcomes can also be of interest. Given some patient characteristics relative to a disease, we might want to predict a viral load quantity. A gambler might want to predict a final score for a team. Unlike, classification problems, we are looking at estimating a numeric outcome. R has a built in function for this called “lm” which can be used but we can also use Trees to do this since, after, all, it does some nice things for us like not having to worry about normalizing data (not that that is hard) or the mixture of quantitative and categorical data.</p>
<div id="performance-measure" class="section level3">
<h3><span class="header-section-number">8.4.1</span> Performance Measure</h3>
<p>Since we are predicting a numeric outcome we would like to come up with a metric to help us figure out if the model we have is good or not. With classification situations we can employ confusion matrices and ROC curves. Here we will use something more simplistic but effective - Root Mean Square Error. The formula looks like the following where P represents a vector of predictions and O represents a vector of the observed (true) values.</p>
<p><span class="math display">\[
RMSE = \sqrt\frac{\sum_i^n(P_i-O_i)^2}{n}
\]</span></p>
<p>We could even write our own function for this although the <strong>Metrics</strong> package has a function called <strong>rmse</strong> to do this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Lets define a rmse function for future use</span>
compute_rmse &lt;-<span class="st"> </span><span class="cf">function</span>(preds,known) {
  errors &lt;-<span class="st"> </span>preds<span class="op">-</span>known
  rmse   &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">mean</span>(errors<span class="op">^</span><span class="dv">2</span>))
  <span class="kw">return</span>(rmse)
}</code></pre></div>
<p>Let’s build a classification tree model on the Pima Indians data to predict the body mass of a participant.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Train the model</span>
mass_pima_mod &lt;-<span class="st"> </span><span class="kw">rpart</span>(<span class="dt">formula =</span> mass <span class="op">~</span><span class="st"> </span>., 
                     <span class="dt">data =</span> pm_train, 
                     <span class="dt">method =</span> <span class="st">&quot;anova&quot;</span>)

<span class="co"># Look at the model output                      </span>
<span class="kw">print</span>(mass_pima_mod)</code></pre></div>
<pre><code>## n= 614 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
##  1) root 614 39172.9200 32.21352  
##    2) triceps&lt; 26.5 344 20725.0600 29.10901  
##      4) diabetes=neg 243 12450.8400 27.53868  
##        8) pressure&lt; 12 13  2340.9510 18.63846 *
##        9) pressure&gt;=12 230  9021.8990 28.04174  
##         18) pressure&lt; 75.5 168  4324.2790 26.97202  
##           36) pregnant&lt; 1.5 63  1570.2310 24.76032 *
##           37) pregnant&gt;=1.5 105  2260.9700 28.29905 *
##         19) pressure&gt;=75.5 62  3984.4690 30.94032  
##           38) age&gt;=54.5 9   852.0089 23.51111 *
##           39) age&lt; 54.5 53  2551.3700 32.20189 *
##      5) diabetes=pos 101  6233.3130 32.88713  
##       10) glucose&lt; 125.5 38  2940.4690 30.42368  
##         20) pregnant&gt;=6.5 15  1682.7770 26.28667 *
##         21) pregnant&lt; 6.5 23   833.5391 33.12174 *
##       11) glucose&gt;=125.5 63  2923.1440 34.37302 *
##    3) triceps&gt;=26.5 270 10908.2600 36.16889  
##      6) triceps&lt; 35.5 148  4288.5590 33.74122  
##       12) pregnant&gt;=0.5 131  3040.1440 32.95496 *
##       13) pregnant&lt; 0.5 17   543.3800 39.80000 *
##      7) triceps&gt;=35.5 122  4689.3060 39.11393  
##       14) pregnant&gt;=1.5 73  1654.4990 37.28767 *
##       15) pregnant&lt; 1.5 49  2428.6110 41.83469  
##         30) triceps&lt; 40.5 21   452.6029 38.17143 *
##         31) triceps&gt;=40.5 28  1482.8410 44.58214  
##           62) diabetes=neg 15   292.1533 41.03333 *
##           63) diabetes=pos 13   783.8031 48.67692 *</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Plot the tree model</span>
<span class="kw">rpart.plot</span>(<span class="dt">x =</span> mass_pima_mod, 
           <span class="dt">yesno =</span> <span class="dv">2</span>, <span class="dt">type =</span> <span class="dv">0</span>, <span class="dt">extra =</span> <span class="dv">0</span>)</code></pre></div>
<p><img src="biosml_files/figure-html/unnamed-chunk-57-1.png" width="672" /></p>
<p>Let’s compute the RMSE for this model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Generate predictions on a test set</span>
pred &lt;-<span class="st"> </span><span class="kw">predict</span>(<span class="dt">object =</span> mass_pima_mod,   <span class="co"># model object </span>
                <span class="dt">newdata =</span> pm_test)  <span class="co"># test dataset</span>

<span class="co"># Compute the RMSE</span>
Metrics<span class="op">::</span><span class="kw">rmse</span>(<span class="dt">actual =</span> pm_test<span class="op">$</span>mass, 
     <span class="dt">predicted =</span> pred)</code></pre></div>
<pre><code>## [1] 6.527793</code></pre>
<p>Is this Good ? Bad ? Okay ? We don’t know. We’ll need to do some things like Cross Validation. And then there are additional arguments to the <strong>rpart</strong> function that we could use to influence how the model does its job. These are referred to as “hyperparamters”.</p>
</div>
</div>
<div id="parameters-vs-hyperparameters" class="section level2">
<h2><span class="header-section-number">8.5</span> Parameters vs Hyperparameters</h2>
<p><strong>Model parameters</strong> are things that are generated as part of the modeling process. These might be things like slope and intercept from a linear model or, in the case of an rpart model, the number of splits in the final tree or the total number of leaves.</p>
<p><strong>Hyper parameters</strong> (sometimes called <strong>metaparameters</strong>) represent information that is supplied in the form of an argument prior the call to the method to generate results. These parameters might not be something one can intelligently set without some experimentation.</p>
<p>Of course, most modeling functions one would call in R have default values for various arguments but this does not mean that the defaults are appropriate for all cases. To see the hyper parameters of the <strong>rpart</strong> function, check the help page for <strong>rpart.control</strong>.</p>
<div class="figure">
<img src="/Users/esteban/Dropbox/ML/biosml/pics/rpartctrol2.png" />

</div>
<p>Tuning the hyperparameters for <strong>rpart</strong> would involve adjusting the following hyper parameters or using some post processing function to refine the model relative to these parameters:</p>
<ul>
<li><strong>cp</strong> which is the complexity parameter (default is .01) - smaller values means more complexity</li>
<li><strong>minsplit</strong> the minimum number of observations that must exist in a node before a split is attempted (default is 20)</li>
<li><strong>maxdepth</strong> maximum number of nodes between a final node and root node</li>
</ul>
<p>There are other hyper parameters but we can start with these. The <strong>rpart.control</strong> function will do something called cross validation which involves repeatedly running the <strong>rpart</strong> some number of times (10 by default) while internally specifying different values for the above mentioned hyper parameters.</p>
<p>In effect, it is doing some work for you so you don’t have to. At the end of the run it will produce a table for inspection. The results of this table can then be used to “prune” the tree model to get a “better” tree - one that performs better than an “un pruned” tree. Let’s look at the pima mass model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># You can plot the modelling object</span>
<span class="kw">plot</span>(mass_pima_mod)</code></pre></div>
<p><img src="biosml_files/figure-html/unnamed-chunk-59-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># You can inspect the table manually</span>
mass_pima_mod<span class="op">$</span>cptable</code></pre></div>
<pre><code>##            CP nsplit rel error    xerror       xstd
## 1  0.19246963      0 1.0000000 1.0038655 0.09025726
## 2  0.05210009      1 0.8075304 0.8658245 0.07890529
## 3  0.04927878      2 0.7554303 0.8505358 0.08270413
## 4  0.02777394      3 0.7061515 0.7992705 0.07859143
## 5  0.01820522      4 0.6783776 0.7654057 0.07571656
## 6  0.01799800      5 0.6601723 0.7667300 0.07401132
## 7  0.01547488      6 0.6421743 0.7677251 0.07367783
## 8  0.01483399      7 0.6266994 0.7599846 0.07217766
## 9  0.01258949      8 0.6118655 0.7641223 0.07690208
## 10 0.01258721      9 0.5992760 0.7595554 0.07661479
## 11 0.01038689     10 0.5866888 0.7511055 0.07634744
## 12 0.01013267     11 0.5763019 0.7536425 0.07667069
## 13 0.01000000     13 0.5560365 0.7536425 0.07667069</code></pre>
<p>We want to pick the value of CP from the table that corresponds to the minimum <strong>xerror</strong> value. This can also be deduced from the plot but let’s work with the table:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cpt &lt;-<span class="st"> </span><span class="kw">as.data.frame.matrix</span>(mass_pima_mod<span class="op">$</span>cptable) 
cpt_val &lt;-<span class="st"> </span>cpt[<span class="kw">order</span>(cpt<span class="op">$</span>xerror),][<span class="dv">1</span>,]<span class="op">$</span>CP</code></pre></div>
<p>We’ll use the CP value associated with the minimum error which in this case turns out to be 0.01. This is now passed to the <strong>prune</strong> function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mass_pima_mod_opt &lt;-<span class="st"> </span><span class="kw">prune</span>(mass_pima_mod, 
                           <span class="dt">cp =</span> cpt_val)
                          
<span class="co"># Plot the optimized model</span>
<span class="kw">rpart.plot</span>(mass_pima_mod_opt, <span class="dt">yesno =</span> <span class="dv">2</span>, <span class="dt">type =</span> <span class="dv">0</span>, <span class="dt">extra =</span> <span class="dv">0</span>)</code></pre></div>
<p><img src="biosml_files/figure-html/unnamed-chunk-61-1.png" width="672" /></p>
<p>Does this optimized model perform any better ? Not really, because the optimal CP value turned out to be 0.01 which is actually the same as the default.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Generate predictions on a test set</span>
pred &lt;-<span class="st"> </span><span class="kw">predict</span>(<span class="dt">object =</span> mass_pima_mod_opt,   <span class="co"># model object </span>
                <span class="dt">newdata =</span> pm_test)  <span class="co"># test dataset</span>

<span class="co"># Compute the RMSE</span>
Metrics<span class="op">::</span><span class="kw">rmse</span>(<span class="dt">actual =</span> pm_test<span class="op">$</span>mass, 
     <span class="dt">predicted =</span> pred)</code></pre></div>
<pre><code>## [1] 6.529925</code></pre>
</div>
<div id="grid-searching" class="section level2">
<h2><span class="header-section-number">8.6</span> Grid Searching</h2>
<p>We might want to review several different models that correspond to various hyperparamter sets. Our goal is to find the best performing model based on a systematic approach that allows us to assess each model in a fair way. There are functions that can help us build a “grid” of hyperparameter values that can then “feed” the function arguments. So we train models on a combination of these values and compare them using the RMSE for regression or ROC / Confusion Matrix for classification setups.</p>
<p>Setting up the grid involves a manual process (although as we will eventually see) the <strong>caret</strong> package can help automate this for us. Knowing about the valid values for a hyperparameter is critical so some experimentation is important. The following process sets up a data frame of two columns each of which corresponds to a hyperparamter of the <strong>rpart</strong> function. The intent here is to call <strong>rpart</strong> a number of times using each row of the below data frame to supply the values for the respective arguments in <strong>rpart</strong>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mysplits &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">35</span>, <span class="dv">5</span>)
mydepths &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">5</span>,<span class="dv">40</span>, <span class="dv">10</span>)

my_cool_grid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">minsplit =</span> mysplits,
                            <span class="dt">maxdepth =</span> mydepths)

<span class="kw">head</span>(my_cool_grid)</code></pre></div>
<pre><code>##   minsplit maxdepth
## 1        1        5
## 2        6        5
## 3       11        5
## 4       16        5
## 5       21        5
## 6       26        5</code></pre>
<p>We’ll generate models, predictions, and RMSE values and stash them for later review. Once again, we’ll be predicting the mass variable in the Pima Indians data frame.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">do_grid_search &lt;-<span class="st"> </span><span class="cf">function</span>(minsplit,maxdepth) {
  
  <span class="co"># Setup some book keeping structures</span>
  
  mods  &lt;-<span class="st"> </span><span class="kw">list</span>()
  preds &lt;-<span class="st"> </span><span class="kw">list</span>()
  myrmse  &lt;-<span class="st"> </span><span class="kw">vector</span>()
  
  mygrid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">minsplit=</span>minsplit,
                        <span class="dt">maxdepth=</span>maxdepth)
  
  <span class="cf">for</span> (ii <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(mygrid)) {
    minsplit &lt;-<span class="st"> </span>mygrid[ii,]<span class="op">$</span>minsplit
    maxdepth &lt;-<span class="st"> </span>mygrid[ii,]<span class="op">$</span>maxdepth
    
    <span class="co"># Build the Model</span>
    mods[[ii]] &lt;-<span class="st"> </span><span class="kw">rpart</span>(mass <span class="op">~</span><span class="st"> </span>., 
                        <span class="dt">data =</span> pm_train,
                        <span class="dt">method =</span> <span class="st">&quot;anova&quot;</span>,
                        <span class="dt">minsplit =</span> minsplit,
                        <span class="dt">maxdepth =</span> maxdepth)
    
    <span class="co"># Now predict against the test data</span>
    preds[[ii]] &lt;-<span class="st"> </span><span class="kw">predict</span>(mods[[ii]],
                           <span class="dt">newdata =</span> pm_test)
    
    <span class="co"># Get RMSE</span>
    myrmse[ii] &lt;-<span class="st"> </span>Metrics<span class="op">::</span><span class="kw">rmse</span>(<span class="dt">actual =</span> pm_test<span class="op">$</span>mass, 
                                <span class="dt">predicted =</span> preds[[ii]])
  }
  
  <span class="co"># Find the model that has the lowest rmse</span>
  idx &lt;-<span class="st"> </span><span class="kw">which.min</span>(myrmse)
  
  <span class="co"># Get the control parameters for the best model</span>
  optimal_model &lt;-<span class="st"> </span>mods[[idx]]
  rmseval &lt;-<span class="st"> </span>myrmse[idx]
  
  <span class="kw">return</span>(<span class="kw">list</span>(<span class="dt">optimal_model=</span>optimal_model,<span class="dt">optim_rmse=</span>rmseval))
}</code></pre></div>
<p>Now run the function and get the parameters corresponding to the optimal model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">results &lt;-<span class="st"> </span><span class="kw">do_grid_search</span>(<span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">1</span>),<span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">1</span>))
results<span class="op">$</span>optim_rmse</code></pre></div>
<pre><code>## [1] 6.080382</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">results<span class="op">$</span>optimal_model<span class="op">$</span>control</code></pre></div>
<pre><code>## $minsplit
## [1] 2
## 
## $minbucket
## [1] 1
## 
## $cp
## [1] 0.01
## 
## $maxcompete
## [1] 4
## 
## $maxsurrogate
## [1] 5
## 
## $usesurrogate
## [1] 2
## 
## $surrogatestyle
## [1] 0
## 
## $maxdepth
## [1] 4
## 
## $xval
## [1] 10</code></pre>
</div>
<div id="bagged-trees" class="section level2">
<h2><span class="header-section-number">8.7</span> Bagged Trees</h2>
<p>Now we look at bagged trees which involves looking at many trees in aggregate - this is an ensemble method. It helps to reduce the variance associated with a single decision tree which can be highly sensitive to changes in data. The term bagging refers to “bootstrap aggregation”. To review, the Decision Tree process can be represented like this:</p>
<div class="figure">
<img src="/Users/esteban/Dropbox/ML/biosml/pics/tree2.png" />

</div>
<p>The bootstrap method of sampling will resample the training data some number of times (with replacement) and retrain a number of models on the resampled data to average out the error. This looks something like:</p>
<ul>
<li>The input data set is resampled with replacement some number of times (e.g. 10,50, 100)</li>
<li>The resampled data is usually a subset of the data which leaves some portion of the data available to use a mini test set for prediction (“out of bag data”“)</li>
<li>Get the RMSE from the prediction</li>
<li>Average out the RMSE</li>
</ul>
<div class="figure">
<img src="/Users/esteban/Dropbox/ML/biosml/pics/bagged.png" />

</div>
<p>So you get a lot of different trees whose performance can be averaged over boostrapped data sets which might include observations several times or not at all. This should result in less variance.</p>
<p>A reason NOT to use bagged trees involves the idea that a collection of trees is not nearly as easy to look at as a single decision tree. We can actually write our own version of bagged trees using a for loop. Sort of like what we did above. First, let’s get our test / train pair for the Pima Indians data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">123</span>)
percent &lt;-<span class="st"> </span>.<span class="dv">80</span>
train_idx &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(pm),
                      <span class="kw">round</span>(percent<span class="op">*</span><span class="kw">nrow</span>(pm)))

train_idx[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>]</code></pre></div>
<pre><code>##  [1] 221 605 314 676 719  35 403 680 420 347</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Subset the pm data frame to training indices only</span>
pm_train &lt;-<span class="st"> </span>pm[train_idx, ]  
  
<span class="co"># Exclude the training indices to create the test set</span>
pm_test &lt;-<span class="st"> </span>pm[<span class="op">-</span>train_idx, ]  </code></pre></div>
<p>Next, well create 50 different trees based on 50 boostrapped samples of the training data. We will sample WITH replacement. This means that some of the rows from the data will be repeated and some will be left out all together. We can figure out what rows were not included and use them to create a test set referred to as “out of bag” samples.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">training_boot_idx &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(pm),<span class="dt">replace=</span><span class="ot">TRUE</span>)
test_boot_idx &lt;-<span class="st"> </span><span class="op">!</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">32</span> <span class="op">%in%</span><span class="st"> </span>training_boot_idx)</code></pre></div>
<p>Let’s start looping</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(Metrics)

modl   &lt;-<span class="st"> </span><span class="kw">list</span>()
predl  &lt;-<span class="st"> </span><span class="kw">list</span>()
aucval &lt;-<span class="st"> </span><span class="kw">vector</span>()
acc    &lt;-<span class="st"> </span><span class="kw">vector</span>()

number_of_boostraps &lt;-<span class="st"> </span><span class="dv">50</span>

<span class="cf">for</span> (ii <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>number_of_boostraps) {
  
  training_boot_idx &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(pm),<span class="dt">replace=</span><span class="ot">TRUE</span>)
  test_boot_idx &lt;-<span class="st"> </span><span class="op">!</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">32</span> <span class="op">%in%</span><span class="st"> </span>training_boot_idx)
  
  modl[[ii]] &lt;-<span class="st"> </span><span class="kw">rpart</span>(diabetes <span class="op">~</span><span class="st"> </span>., 
                   <span class="dt">data =</span> pm[training_boot_idx,],
                   <span class="dt">method =</span> <span class="st">&quot;class&quot;</span>
                )
  
  <span class="co"># This list will contain / hold the models build on the bootstrap</span>
  
  predl[[ii]]  &lt;-<span class="st"> </span><span class="kw">predict</span>(modl[[ii]],
                          <span class="dt">newdata=</span>pm[test_boot_idx,],
                          <span class="dt">type=</span><span class="st">&quot;prob&quot;</span>)
  
  converted &lt;-<span class="st"> </span><span class="kw">ifelse</span>(pm[test_boot_idx,]<span class="op">$</span>diabetes <span class="op">==</span><span class="st"> &quot;pos&quot;</span>,<span class="dv">1</span>,<span class="dv">0</span>)
  
  <span class="co"># Let&#39;s create an estimate of the AUC</span>
  
  aucval[ii] &lt;-<span class="st"> </span>Metrics<span class="op">::</span><span class="kw">auc</span>(<span class="dt">actual =</span> converted,
                    <span class="dt">predicted =</span> predl[[ii]][,<span class="dv">2</span>])
}</code></pre></div>
<p>Now check all of the accuracy estimates and then average them. Remember this is supposed to help us to better estimate the out of sample error.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">boxplot</span>(aucval)</code></pre></div>
<p><img src="biosml_files/figure-html/unnamed-chunk-69-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(aucval)</code></pre></div>
<pre><code>## [1] 0.8240443</code></pre>
<p>Now we’ll compare this to the function called “bagging” from the <strong>ipred</strong> package which does bagged trees directly.</p>
<div class="figure">
<img src="/Users/esteban/Dropbox/ML/biosml/pics/bagging2.png" />

</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ipred)
bagged_pm &lt;-<span class="st"> </span><span class="kw">bagging</span>(diabetes <span class="op">~</span><span class="st"> </span>.,
                     <span class="dt">data =</span> pm_train, <span class="dt">coob =</span> <span class="ot">TRUE</span>)

bagged_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(bagged_pm,
                       <span class="dt">newdata =</span> pm_test, <span class="dt">type=</span><span class="st">&quot;prob&quot;</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">converted &lt;-<span class="st"> </span><span class="kw">ifelse</span>(pm_test<span class="op">$</span>diabetes <span class="op">==</span><span class="st"> &quot;pos&quot;</span>,<span class="dv">1</span>,<span class="dv">0</span>)

aucval    &lt;-<span class="st"> </span>Metrics<span class="op">::</span><span class="kw">auc</span>(<span class="dt">actual =</span> converted,
                    <span class="dt">predicted =</span> bagged_pred[,<span class="dv">2</span>])

aucval</code></pre></div>
<pre><code>## [1] 0.807741</code></pre>
<p>This is better than the 0.7804305 we got when using a single Decision tree.</p>
</div>
<div id="random-forests-1" class="section level2">
<h2><span class="header-section-number">8.8</span> Random Forests</h2>
<p>In Random Forest, only a subset of features are selected at random at each split in a decision tree. In bagging, all features are used.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(randomForest)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Train a Random Forest</span>
<span class="kw">set.seed</span>(<span class="dv">1</span>)  <span class="co"># for reproducibility</span>
pm_rf_model &lt;-<span class="st"> </span><span class="kw">randomForest</span>(<span class="dt">formula =</span> diabetes <span class="op">~</span><span class="st"> </span>., 
                             <span class="dt">data =</span>pm_train)
                             
<span class="co"># Print the model output                             </span>
<span class="kw">print</span>(pm_rf_model)</code></pre></div>
<pre><code>## 
## Call:
##  randomForest(formula = diabetes ~ ., data = pm_train) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 2
## 
##         OOB estimate of  error rate: 23.94%
## Confusion matrix:
##     neg pos class.error
## neg 344  59   0.1464020
## pos  88 123   0.4170616</code></pre>
<p>Let’s look at the out of bag error matrix</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(pm_rf_model<span class="op">$</span>err.rate)</code></pre></div>
<pre><code>##            OOB       neg       pos
## [1,] 0.2876106 0.1931034 0.4567901
## [2,] 0.2986301 0.2142857 0.4566929
## [3,] 0.3213483 0.2302405 0.4935065
## [4,] 0.3154762 0.2259036 0.4883721
## [5,] 0.2943327 0.2049861 0.4677419
## [6,] 0.2867133 0.2068966 0.4410256</code></pre>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="classification-example.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="using-methods-other-than-lm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
