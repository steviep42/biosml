<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Decision Trees | Predictive Learning in R</title>
  <meta name="description" content="Chapter 8 Decision Trees | Predictive Learning in R" />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Decision Trees | Predictive Learning in R" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Decision Trees | Predictive Learning in R" />
  
  
  

<meta name="author" content="Steve Pittard - wsp@emory.edu" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="classification-example.html"/>
<link rel="next" href="using-different-methods.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#machine-learning"><i class="fa fa-check"></i><b>1.1</b> Machine Learning</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#predictive-modeling"><i class="fa fa-check"></i><b>1.2</b> Predictive Modeling</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#in-sample-vs-out-of-sample-error"><i class="fa fa-check"></i><b>1.3</b> In-Sample vs Out-Of-Sample Error</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#performance-metrics"><i class="fa fa-check"></i><b>1.4</b> Performance Metrics</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#black-box"><i class="fa fa-check"></i><b>1.5</b> Black Box</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html"><i class="fa fa-check"></i><b>2</b> Predictive / Supervised Learning</a><ul>
<li class="chapter" data-level="2.1" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#explanation-vs-prediction"><i class="fa fa-check"></i><b>2.1</b> Explanation vs Prediction</a><ul>
<li class="chapter" data-level="2.1.1" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#titanic-data"><i class="fa fa-check"></i><b>2.1.1</b> Titanic Data</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#bias-vs-variance"><i class="fa fa-check"></i><b>2.2</b> Bias vs Variance</a><ul>
<li class="chapter" data-level="2.2.1" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#bias"><i class="fa fa-check"></i><b>2.2.1</b> Bias</a></li>
<li class="chapter" data-level="2.2.2" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#variance"><i class="fa fa-check"></i><b>2.2.2</b> Variance</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#overfitting-and-underfitting"><i class="fa fa-check"></i><b>2.3</b> Overfitting and Underfitting</a></li>
<li class="chapter" data-level="2.4" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#some-important-terminology"><i class="fa fa-check"></i><b>2.4</b> Some Important Terminology</a></li>
<li class="chapter" data-level="2.5" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#levels-of-measurement"><i class="fa fa-check"></i><b>2.5</b> Levels of Measurement</a><ul>
<li class="chapter" data-level="2.5.1" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#nominal"><i class="fa fa-check"></i><b>2.5.1</b> Nominal</a></li>
<li class="chapter" data-level="2.5.2" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#ordinal"><i class="fa fa-check"></i><b>2.5.2</b> Ordinal</a></li>
<li class="chapter" data-level="2.5.3" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#interval"><i class="fa fa-check"></i><b>2.5.3</b> Interval</a></li>
<li class="chapter" data-level="2.5.4" data-path="predictive-supervised-learning.html"><a href="predictive-supervised-learning.html#ratio"><i class="fa fa-check"></i><b>2.5.4</b> Ratio</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="a-motivating-example.html"><a href="a-motivating-example.html"><i class="fa fa-check"></i><b>3</b> A Motivating Example</a><ul>
<li class="chapter" data-level="3.1" data-path="a-motivating-example.html"><a href="a-motivating-example.html#a-more-detailed-workflow"><i class="fa fa-check"></i><b>3.1</b> A More Detailed Workflow</a></li>
<li class="chapter" data-level="3.2" data-path="a-motivating-example.html"><a href="a-motivating-example.html#visualizations"><i class="fa fa-check"></i><b>3.2</b> Visualizations</a><ul>
<li class="chapter" data-level="3.2.1" data-path="a-motivating-example.html"><a href="a-motivating-example.html#scatterplots"><i class="fa fa-check"></i><b>3.2.1</b> Scatterplots</a></li>
<li class="chapter" data-level="3.2.2" data-path="a-motivating-example.html"><a href="a-motivating-example.html#boxplots"><i class="fa fa-check"></i><b>3.2.2</b> Boxplots</a></li>
<li class="chapter" data-level="3.2.3" data-path="a-motivating-example.html"><a href="a-motivating-example.html#histograms"><i class="fa fa-check"></i><b>3.2.3</b> Histograms</a></li>
<li class="chapter" data-level="3.2.4" data-path="a-motivating-example.html"><a href="a-motivating-example.html#tables"><i class="fa fa-check"></i><b>3.2.4</b> Tables</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="a-motivating-example.html"><a href="a-motivating-example.html#correlations"><i class="fa fa-check"></i><b>3.3</b> Correlations</a></li>
<li class="chapter" data-level="3.4" data-path="a-motivating-example.html"><a href="a-motivating-example.html#building-a-model---in-sample-error"><i class="fa fa-check"></i><b>3.4</b> Building A Model - In Sample Error</a></li>
<li class="chapter" data-level="3.5" data-path="a-motivating-example.html"><a href="a-motivating-example.html#out-of-sample-data"><i class="fa fa-check"></i><b>3.5</b> Out Of Sample Data</a></li>
<li class="chapter" data-level="3.6" data-path="a-motivating-example.html"><a href="a-motivating-example.html#some-additional-considerations"><i class="fa fa-check"></i><b>3.6</b> Some Additional Considerations</a></li>
<li class="chapter" data-level="3.7" data-path="a-motivating-example.html"><a href="a-motivating-example.html#other-methods"><i class="fa fa-check"></i><b>3.7</b> Other Methods ?</a></li>
<li class="chapter" data-level="3.8" data-path="a-motivating-example.html"><a href="a-motivating-example.html#summary"><i class="fa fa-check"></i><b>3.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="training-test-data.html"><a href="training-test-data.html"><i class="fa fa-check"></i><b>4</b> Training / Test Data</a><ul>
<li class="chapter" data-level="4.1" data-path="training-test-data.html"><a href="training-test-data.html#cross-fold-validation"><i class="fa fa-check"></i><b>4.1</b> Cross Fold Validation</a></li>
<li class="chapter" data-level="4.2" data-path="training-test-data.html"><a href="training-test-data.html#create-a-function-to-automate-things"><i class="fa fa-check"></i><b>4.2</b> Create A Function To Automate Things</a></li>
<li class="chapter" data-level="4.3" data-path="training-test-data.html"><a href="training-test-data.html#repeated-cross-validation"><i class="fa fa-check"></i><b>4.3</b> Repeated Cross Validation</a></li>
<li class="chapter" data-level="4.4" data-path="training-test-data.html"><a href="training-test-data.html#bootstrap"><i class="fa fa-check"></i><b>4.4</b> Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="caret-package.html"><a href="caret-package.html"><i class="fa fa-check"></i><b>5</b> Caret Package</a><ul>
<li class="chapter" data-level="5.1" data-path="caret-package.html"><a href="caret-package.html#putting-caret-to-work"><i class="fa fa-check"></i><b>5.1</b> Putting caret To Work</a></li>
<li class="chapter" data-level="5.2" data-path="caret-package.html"><a href="caret-package.html#back-to-the-beginning"><i class="fa fa-check"></i><b>5.2</b> Back To The Beginning</a></li>
<li class="chapter" data-level="5.3" data-path="caret-package.html"><a href="caret-package.html#splitting"><i class="fa fa-check"></i><b>5.3</b> Splitting</a></li>
<li class="chapter" data-level="5.4" data-path="caret-package.html"><a href="caret-package.html#calling-the-train-function"><i class="fa fa-check"></i><b>5.4</b> Calling The train() Function</a></li>
<li class="chapter" data-level="5.5" data-path="caret-package.html"><a href="caret-package.html#reproducible-results"><i class="fa fa-check"></i><b>5.5</b> Reproducible Results</a></li>
<li class="chapter" data-level="5.6" data-path="caret-package.html"><a href="caret-package.html#one-size-fits-all"><i class="fa fa-check"></i><b>5.6</b> One Size Fits All</a></li>
<li class="chapter" data-level="5.7" data-path="caret-package.html"><a href="caret-package.html#alternative-calling-sequence"><i class="fa fa-check"></i><b>5.7</b> Alternative Calling Sequence</a></li>
<li class="chapter" data-level="5.8" data-path="caret-package.html"><a href="caret-package.html#hyperparameters"><i class="fa fa-check"></i><b>5.8</b> Hyperparameters</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="classification-problems.html"><a href="classification-problems.html"><i class="fa fa-check"></i><b>6</b> Classification Problems</a><ul>
<li class="chapter" data-level="6.1" data-path="classification-problems.html"><a href="classification-problems.html#performance-measures"><i class="fa fa-check"></i><b>6.1</b> Performance Measures</a></li>
<li class="chapter" data-level="6.2" data-path="classification-problems.html"><a href="classification-problems.html#important-terminology"><i class="fa fa-check"></i><b>6.2</b> Important Terminology</a></li>
<li class="chapter" data-level="6.3" data-path="classification-problems.html"><a href="classification-problems.html#a-basic-model"><i class="fa fa-check"></i><b>6.3</b> A Basic Model</a></li>
<li class="chapter" data-level="6.4" data-path="classification-problems.html"><a href="classification-problems.html#selecting-the-correct-threshold-alpha"><i class="fa fa-check"></i><b>6.4</b> Selecting The Correct Threshold / Alpha</a><ul>
<li class="chapter" data-level="6.4.1" data-path="classification-problems.html"><a href="classification-problems.html#moving-the-threshold"><i class="fa fa-check"></i><b>6.4.1</b> Moving The Threshold</a></li>
<li class="chapter" data-level="6.4.2" data-path="classification-problems.html"><a href="classification-problems.html#distribution-of-predicted-probabilities"><i class="fa fa-check"></i><b>6.4.2</b> Distribution of Predicted Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="classification-problems.html"><a href="classification-problems.html#hypothesis-testing"><i class="fa fa-check"></i><b>6.5</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="6.6" data-path="classification-problems.html"><a href="classification-problems.html#confusion-matrix"><i class="fa fa-check"></i><b>6.6</b> Confusion Matrix</a><ul>
<li class="chapter" data-level="6.6.1" data-path="classification-problems.html"><a href="classification-problems.html#computing-performance-metrics"><i class="fa fa-check"></i><b>6.6.1</b> Computing Performance Metrics</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="classification-problems.html"><a href="classification-problems.html#picking-the-right-metric"><i class="fa fa-check"></i><b>6.7</b> Picking the Right Metric</a></li>
<li class="chapter" data-level="6.8" data-path="classification-problems.html"><a href="classification-problems.html#wait.-where-are-we"><i class="fa fa-check"></i><b>6.8</b> Wait. Where Are We ?</a></li>
<li class="chapter" data-level="6.9" data-path="classification-problems.html"><a href="classification-problems.html#other-ways-to-compute-the-roc-curve"><i class="fa fa-check"></i><b>6.9</b> Other Ways To Compute The ROC Curve</a></li>
<li class="chapter" data-level="6.10" data-path="classification-problems.html"><a href="classification-problems.html#roc-curve-summary"><i class="fa fa-check"></i><b>6.10</b> ROC Curve Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="classification-example.html"><a href="classification-example.html"><i class="fa fa-check"></i><b>7</b> Classification Example</a><ul>
<li class="chapter" data-level="7.1" data-path="classification-example.html"><a href="classification-example.html#exploratory-plots"><i class="fa fa-check"></i><b>7.1</b> Exploratory Plots</a></li>
<li class="chapter" data-level="7.2" data-path="classification-example.html"><a href="classification-example.html#generalized-linear-models"><i class="fa fa-check"></i><b>7.2</b> Generalized Linear Models</a></li>
<li class="chapter" data-level="7.3" data-path="classification-example.html"><a href="classification-example.html#random-forests"><i class="fa fa-check"></i><b>7.3</b> Random Forests</a></li>
<li class="chapter" data-level="7.4" data-path="classification-example.html"><a href="classification-example.html#feature-importance"><i class="fa fa-check"></i><b>7.4</b> Feature Importance</a></li>
<li class="chapter" data-level="7.5" data-path="classification-example.html"><a href="classification-example.html#target-variable-format"><i class="fa fa-check"></i><b>7.5</b> Target Variable Format</a></li>
<li class="chapter" data-level="7.6" data-path="classification-example.html"><a href="classification-example.html#addressing-class-imbalance"><i class="fa fa-check"></i><b>7.6</b> Addressing Class Imbalance</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>8</b> Decision Trees</a><ul>
<li class="chapter" data-level="8.1" data-path="decision-trees.html"><a href="decision-trees.html#advantages"><i class="fa fa-check"></i><b>8.1</b> Advantages</a></li>
<li class="chapter" data-level="8.2" data-path="decision-trees.html"><a href="decision-trees.html#a-classification-example"><i class="fa fa-check"></i><b>8.2</b> A Classification Example</a></li>
<li class="chapter" data-level="8.3" data-path="decision-trees.html"><a href="decision-trees.html#digging-deeper"><i class="fa fa-check"></i><b>8.3</b> Digging Deeper</a><ul>
<li class="chapter" data-level="8.3.1" data-path="decision-trees.html"><a href="decision-trees.html#evaluating-performance"><i class="fa fa-check"></i><b>8.3.1</b> Evaluating performance</a></li>
<li class="chapter" data-level="8.3.2" data-path="decision-trees.html"><a href="decision-trees.html#tree-splitting"><i class="fa fa-check"></i><b>8.3.2</b> Tree Splitting</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="decision-trees.html"><a href="decision-trees.html#gini-index"><i class="fa fa-check"></i><b>8.4</b> Gini Index</a></li>
<li class="chapter" data-level="8.5" data-path="decision-trees.html"><a href="decision-trees.html#regression-trees"><i class="fa fa-check"></i><b>8.5</b> Regression Trees</a><ul>
<li class="chapter" data-level="8.5.1" data-path="decision-trees.html"><a href="decision-trees.html#performance-measure"><i class="fa fa-check"></i><b>8.5.1</b> Performance Measure</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="decision-trees.html"><a href="decision-trees.html#parameters-vs-hyperparameters"><i class="fa fa-check"></i><b>8.6</b> Parameters vs Hyperparameters</a></li>
<li class="chapter" data-level="8.7" data-path="decision-trees.html"><a href="decision-trees.html#grid-searching"><i class="fa fa-check"></i><b>8.7</b> Grid Searching</a></li>
<li class="chapter" data-level="8.8" data-path="decision-trees.html"><a href="decision-trees.html#bagged-trees"><i class="fa fa-check"></i><b>8.8</b> Bagged Trees</a></li>
<li class="chapter" data-level="8.9" data-path="decision-trees.html"><a href="decision-trees.html#random-forests-1"><i class="fa fa-check"></i><b>8.9</b> Random Forests</a></li>
<li class="chapter" data-level="8.10" data-path="decision-trees.html"><a href="decision-trees.html#boosted-trees"><i class="fa fa-check"></i><b>8.10</b> Boosted Trees</a></li>
<li class="chapter" data-level="8.11" data-path="decision-trees.html"><a href="decision-trees.html#using-caret"><i class="fa fa-check"></i><b>8.11</b> Using caret</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="using-different-methods.html"><a href="using-different-methods.html"><i class="fa fa-check"></i><b>9</b> Using Different Methods</a><ul>
<li class="chapter" data-level="9.1" data-path="using-different-methods.html"><a href="using-different-methods.html#parameters-vs-hyperparameters-1"><i class="fa fa-check"></i><b>9.1</b> Parameters vs Hyperparameters</a></li>
<li class="chapter" data-level="9.2" data-path="using-different-methods.html"><a href="using-different-methods.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>9.2</b> Hyperparameter Tuning</a><ul>
<li class="chapter" data-level="9.2.1" data-path="using-different-methods.html"><a href="using-different-methods.html#multiple-hyperparameters"><i class="fa fa-check"></i><b>9.2.1</b> Multiple Hyperparameters ?</a></li>
<li class="chapter" data-level="9.2.2" data-path="using-different-methods.html"><a href="using-different-methods.html#custom-tuning-grid"><i class="fa fa-check"></i><b>9.2.2</b> Custom Tuning Grid</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="using-different-methods.html"><a href="using-different-methods.html#using-validation-data-sets"><i class="fa fa-check"></i><b>9.3</b> Using Validation Data Sets</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="selecting-the-best-model.html"><a href="selecting-the-best-model.html"><i class="fa fa-check"></i><b>10</b> Selecting The Best Model</a><ul>
<li class="chapter" data-level="10.1" data-path="selecting-the-best-model.html"><a href="selecting-the-best-model.html#an-example"><i class="fa fa-check"></i><b>10.1</b> An Example</a></li>
<li class="chapter" data-level="10.2" data-path="selecting-the-best-model.html"><a href="selecting-the-best-model.html#more-comparisons"><i class="fa fa-check"></i><b>10.2</b> More Comparisons</a></li>
<li class="chapter" data-level="10.3" data-path="selecting-the-best-model.html"><a href="selecting-the-best-model.html#using-the-resamples-function"><i class="fa fa-check"></i><b>10.3</b> Using the resamples() function</a></li>
<li class="chapter" data-level="10.4" data-path="selecting-the-best-model.html"><a href="selecting-the-best-model.html#model-performance"><i class="fa fa-check"></i><b>10.4</b> Model Performance</a></li>
<li class="chapter" data-level="10.5" data-path="selecting-the-best-model.html"><a href="selecting-the-best-model.html#feature-evaluation"><i class="fa fa-check"></i><b>10.5</b> Feature Evaluation</a><ul>
<li class="chapter" data-level="10.5.1" data-path="selecting-the-best-model.html"><a href="selecting-the-best-model.html#identifying-redundant-features"><i class="fa fa-check"></i><b>10.5.1</b> Identifying Redundant Features</a></li>
<li class="chapter" data-level="10.5.2" data-path="selecting-the-best-model.html"><a href="selecting-the-best-model.html#highly-correlated-variables"><i class="fa fa-check"></i><b>10.5.2</b> Highly Correlated Variables</a></li>
<li class="chapter" data-level="10.5.3" data-path="selecting-the-best-model.html"><a href="selecting-the-best-model.html#ranking-features"><i class="fa fa-check"></i><b>10.5.3</b> Ranking Features</a></li>
<li class="chapter" data-level="10.5.4" data-path="selecting-the-best-model.html"><a href="selecting-the-best-model.html#feature-selection"><i class="fa fa-check"></i><b>10.5.4</b> Feature Selection</a></li>
<li class="chapter" data-level="10.5.5" data-path="selecting-the-best-model.html"><a href="selecting-the-best-model.html#recursive-feature-elimination"><i class="fa fa-check"></i><b>10.5.5</b> Recursive Feature Elimination</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="data-pre-processing.html"><a href="data-pre-processing.html"><i class="fa fa-check"></i><b>11</b> Data Pre Processing</a><ul>
<li class="chapter" data-level="11.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#types-of-pre-processing"><i class="fa fa-check"></i><b>11.1</b> Types of Pre Processing</a></li>
<li class="chapter" data-level="11.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#missing-values"><i class="fa fa-check"></i><b>11.2</b> Missing Values</a><ul>
<li class="chapter" data-level="11.2.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#finding-rows-with-missing-data"><i class="fa fa-check"></i><b>11.2.1</b> Finding Rows with Missing Data</a></li>
<li class="chapter" data-level="11.2.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#finding-columns-with-missing-data"><i class="fa fa-check"></i><b>11.2.2</b> Finding Columns With Missing Data</a></li>
<li class="chapter" data-level="11.2.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#use-the-median-approach"><i class="fa fa-check"></i><b>11.2.3</b> Use the Median Approach</a></li>
<li class="chapter" data-level="11.2.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#package-based-approach"><i class="fa fa-check"></i><b>11.2.4</b> Package-based Approach</a></li>
<li class="chapter" data-level="11.2.5" data-path="data-pre-processing.html"><a href="data-pre-processing.html#using-caret-1"><i class="fa fa-check"></i><b>11.2.5</b> Using caret</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#scaling"><i class="fa fa-check"></i><b>11.3</b> Scaling</a><ul>
<li class="chapter" data-level="11.3.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#methods-that-benefit-from-scaling"><i class="fa fa-check"></i><b>11.3.1</b> Methods That Benefit From Scaling</a></li>
<li class="chapter" data-level="11.3.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#methods-that-do-not-require-scaling"><i class="fa fa-check"></i><b>11.3.2</b> Methods That Do Not Require Scaling</a></li>
<li class="chapter" data-level="11.3.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#how-to-scale"><i class="fa fa-check"></i><b>11.3.3</b> How To Scale</a></li>
<li class="chapter" data-level="11.3.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#order-of-processing"><i class="fa fa-check"></i><b>11.3.4</b> Order of Processing</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#low-variance-variables"><i class="fa fa-check"></i><b>11.4</b> Low Variance Variables</a></li>
<li class="chapter" data-level="11.5" data-path="data-pre-processing.html"><a href="data-pre-processing.html#pca---principal-components-analysis"><i class="fa fa-check"></i><b>11.5</b> PCA - Principal Components Analysis</a><ul>
<li class="chapter" data-level="11.5.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#identify-the-factors"><i class="fa fa-check"></i><b>11.5.1</b> Identify The Factors</a></li>
<li class="chapter" data-level="11.5.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#check-for-high-correlations"><i class="fa fa-check"></i><b>11.5.2</b> Check For High Correlations</a></li>
<li class="chapter" data-level="11.5.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#so-why-use-pca"><i class="fa fa-check"></i><b>11.5.3</b> So Why Use PCA ?</a></li>
<li class="chapter" data-level="11.5.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#check-the-biplot"><i class="fa fa-check"></i><b>11.5.4</b> Check The BiPlot</a></li>
<li class="chapter" data-level="11.5.5" data-path="data-pre-processing.html"><a href="data-pre-processing.html#check-the-screeplot"><i class="fa fa-check"></i><b>11.5.5</b> Check The ScreePlot</a></li>
<li class="chapter" data-level="11.5.6" data-path="data-pre-processing.html"><a href="data-pre-processing.html#use-the-transformed-data"><i class="fa fa-check"></i><b>11.5.6</b> Use The Transformed Data</a></li>
<li class="chapter" data-level="11.5.7" data-path="data-pre-processing.html"><a href="data-pre-processing.html#pls"><i class="fa fa-check"></i><b>11.5.7</b> PLS</a></li>
<li class="chapter" data-level="11.5.8" data-path="data-pre-processing.html"><a href="data-pre-processing.html#summary-1"><i class="fa fa-check"></i><b>11.5.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="data-pre-processing.html"><a href="data-pre-processing.html#order-of-pre-processing"><i class="fa fa-check"></i><b>11.6</b> Order of Pre-Processing</a></li>
<li class="chapter" data-level="11.7" data-path="data-pre-processing.html"><a href="data-pre-processing.html#handling-categories"><i class="fa fa-check"></i><b>11.7</b> Handling Categories</a><ul>
<li class="chapter" data-level="11.7.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#examples"><i class="fa fa-check"></i><b>11.7.1</b> Examples</a></li>
<li class="chapter" data-level="11.7.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#admissions-data"><i class="fa fa-check"></i><b>11.7.2</b> Admissions Data</a></li>
<li class="chapter" data-level="11.7.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#is-rank-a-category"><i class="fa fa-check"></i><b>11.7.3</b> Is Rank A Category ?</a></li>
<li class="chapter" data-level="11.7.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#relationship-to-one-hot-encoding"><i class="fa fa-check"></i><b>11.7.4</b> Relationship To One Hot Encoding</a></li>
</ul></li>
<li class="chapter" data-level="11.8" data-path="data-pre-processing.html"><a href="data-pre-processing.html#binning"><i class="fa fa-check"></i><b>11.8</b> Binning</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html"><i class="fa fa-check"></i><b>12</b> Using External ML Frameworks</a><ul>
<li class="chapter" data-level="12.1" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#using-h2o"><i class="fa fa-check"></i><b>12.1</b> Using h2o</a></li>
<li class="chapter" data-level="12.2" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#create-some-h20-models"><i class="fa fa-check"></i><b>12.2</b> Create Some h20 Models</a></li>
<li class="chapter" data-level="12.3" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#saving-a-model"><i class="fa fa-check"></i><b>12.3</b> Saving A Model</a></li>
<li class="chapter" data-level="12.4" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#using-the-h2o-auto-ml-feature"><i class="fa fa-check"></i><b>12.4</b> Using The h2o Auto ML Feature</a></li>
<li class="chapter" data-level="12.5" data-path="using-external-ml-frameworks.html"><a href="using-external-ml-frameworks.html#launching-a-job"><i class="fa fa-check"></i><b>12.5</b> Launching a Job</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Predictive Learning in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="decision-trees" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Decision Trees</h1>
<p>Tree-based methods employ a segmentation strategy that partitions the feature / predictor space into a series of decisions which has the added benefit of being easy to understand. Think of it as a flow chart for making decisions. The viewer of the chart is presented with a diagram that offers outcomes in response to (yes / no) questions (decisions) about important predictors found in the data set.</p>
<div id="advantages" class="section level2">
<h2><span class="header-section-number">8.1</span> Advantages</h2>
<p>The advantages of tree-based methods include that 1
- The model is generally easy to interpret
- The path to a decision is plainly spelled out (assuming that the number of tree splits is easy enough to trace).
- The method can handle numeric and categorical
- One does not generally need to pre process or normalize data
- Missing data is less of a big deal</p>
<p>Disadvantages include:</p>
<ul>
<li>Large trees are hard to follow - variance can be high</li>
<li>Trees can be overly complex</li>
<li>Overfitting can be a problem</li>
</ul>
</div>
<div id="a-classification-example" class="section level2">
<h2><span class="header-section-number">8.2</span> A Classification Example</h2>
<p>Let’s use the Pima Indians data set as it relates to predicting whether someone has diabetes. This data is provided by the <strong>mlbench</strong> package. The relevant variables are:</p>
<pre><code>pregnant - Number of times pregnant
glucose  - Plasma glucose concentration (glucose tolerance test)
pressure - Diastolic blood pressure (mm Hg)
triceps  - Triceps skin fold thickness (mm)
insulin  - 2-Hour serum insulin (mu U/ml)
mass       - Body mass index (weight in kg/(height in m)\^2)
pedigree - Diabetes pedigree function
age      - Age (years)
diabetes - Class variable (test for diabetes)</code></pre>
<div class="sourceCode" id="cb314"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb314-1" data-line-number="1"><span class="kw">library</span>(mlbench)</a>
<a class="sourceLine" id="cb314-2" data-line-number="2"><span class="kw">data</span>(PimaIndiansDiabetes)</a>
<a class="sourceLine" id="cb314-3" data-line-number="3">pm &lt;-<span class="st"> </span>PimaIndiansDiabetes</a>
<a class="sourceLine" id="cb314-4" data-line-number="4"></a>
<a class="sourceLine" id="cb314-5" data-line-number="5">diabetes_mod_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">rpart</span>(diabetes <span class="op">~</span><span class="st"> </span>.,pm,</a>
<a class="sourceLine" id="cb314-6" data-line-number="6">                        <span class="dt">method=</span><span class="st">&quot;class&quot;</span>,<span class="dt">cp=</span><span class="fl">0.017</span>)</a>
<a class="sourceLine" id="cb314-7" data-line-number="7"><span class="kw">rpart.plot</span>(<span class="dt">x =</span> diabetes_mod_<span class="dv">1</span>)</a></code></pre></div>
<p><img src="biosml_files/figure-html/diabtree-1.png" width="672" /></p>
<p>That’s pretty understandable and you could show this to someone and they would probably get it without too much explanation as long as they had an awareness of the features in the data set. The “node” at the top is called the “root node” and the lines are “branches” that go either to a “terminal node” or to “leaf nodes” which involve some comparison. It’s a flow chart for decisions about whether someone has diabetes or not. Note that there is also information about what percentages of “pos” or “neg” there are in each branch.</p>
<div class="sourceCode" id="cb315"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb315-1" data-line-number="1">diabetes_preds &lt;-<span class="st"> </span><span class="kw">predict</span>(diabetes_mod_<span class="dv">1</span>,<span class="dt">type=</span><span class="st">&quot;class&quot;</span>)</a>
<a class="sourceLine" id="cb315-2" data-line-number="2"><span class="kw">table</span>(diabetes_preds,pm<span class="op">$</span>diabetes)</a></code></pre></div>
<pre><code>##               
## diabetes_preds neg pos
##            neg 470 131
##            pos  30 137</code></pre>
<div class="sourceCode" id="cb317"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb317-1" data-line-number="1"><span class="co"># Calculate the confusion matrix for the test set</span></a>
<a class="sourceLine" id="cb317-2" data-line-number="2">caret<span class="op">::</span><span class="kw">confusionMatrix</span>(diabetes_preds,       </a>
<a class="sourceLine" id="cb317-3" data-line-number="3">                       pm<span class="op">$</span>diabetes,</a>
<a class="sourceLine" id="cb317-4" data-line-number="4">                       <span class="dt">positive=</span><span class="st">&quot;pos&quot;</span>)  </a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction neg pos
##        neg 470 131
##        pos  30 137
##                                        
##                Accuracy : 0.79         
##                  95% CI : (0.76, 0.819)
##     No Information Rate : 0.651        
##     P-Value [Acc &gt; NIR] : &lt; 2e-16      
##                                        
##                   Kappa : 0.494        
##                                        
##  Mcnemar&#39;s Test P-Value : 3.25e-15     
##                                        
##             Sensitivity : 0.511        
##             Specificity : 0.940        
##          Pos Pred Value : 0.820        
##          Neg Pred Value : 0.782        
##              Prevalence : 0.349        
##          Detection Rate : 0.178        
##    Detection Prevalence : 0.217        
##       Balanced Accuracy : 0.726        
##                                        
##        &#39;Positive&#39; Class : pos          
## </code></pre>
<p>Perhaps you noticed that in the example I included an argument called <strong>cp</strong> which
corresponds to a “complexity parameter” which influences how the tree splits at various nodes.
&gt; The main role of this parameter is to save computing time by pruning off splits that are obviously not worthwhile. Essentially,the user informs the program that any split which does not improve the fit by cp will likely be pruned off by cross-validation, and that hence the program need not pursue it.</p>
<p>We’ll explore this in more detail momentarily. This is what is known as a “hyperparameter”.</p>
</div>
<div id="digging-deeper" class="section level2">
<h2><span class="header-section-number">8.3</span> Digging Deeper</h2>
<p>Let’s create a training set that comprises 80% of the data with a holdout set of 20%.</p>
<div class="sourceCode" id="cb319"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb319-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb319-2" data-line-number="2">percent &lt;-<span class="st"> </span><span class="fl">.80</span></a>
<a class="sourceLine" id="cb319-3" data-line-number="3">train_idx &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(pm),</a>
<a class="sourceLine" id="cb319-4" data-line-number="4">                      <span class="kw">round</span>(percent<span class="op">*</span><span class="kw">nrow</span>(pm)))</a>
<a class="sourceLine" id="cb319-5" data-line-number="5"></a>
<a class="sourceLine" id="cb319-6" data-line-number="6">train_idx[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>]</a></code></pre></div>
<pre><code>##  [1] 415 463 179 526 195 118 299 229 244  14</code></pre>
<div class="sourceCode" id="cb321"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb321-1" data-line-number="1"><span class="co"># Subset the pm data frame to training indices only</span></a>
<a class="sourceLine" id="cb321-2" data-line-number="2">pm_train &lt;-<span class="st"> </span>pm[train_idx, ]  </a>
<a class="sourceLine" id="cb321-3" data-line-number="3">  </a>
<a class="sourceLine" id="cb321-4" data-line-number="4"><span class="co"># Exclude the training indices to create the test set</span></a>
<a class="sourceLine" id="cb321-5" data-line-number="5">pm_test &lt;-<span class="st"> </span>pm[<span class="op">-</span>train_idx, ]  </a></code></pre></div>
<div class="sourceCode" id="cb322"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb322-1" data-line-number="1"><span class="co"># Train the model (to predict &#39;default&#39;)</span></a>
<a class="sourceLine" id="cb322-2" data-line-number="2">pm_class_tree &lt;-<span class="st"> </span><span class="kw">rpart</span>(<span class="dt">formula =</span> diabetes <span class="op">~</span><span class="st"> </span>., </a>
<a class="sourceLine" id="cb322-3" data-line-number="3">                       <span class="dt">data =</span> pm_train, </a>
<a class="sourceLine" id="cb322-4" data-line-number="4">                       <span class="dt">method =</span> <span class="st">&quot;class&quot;</span>)</a>
<a class="sourceLine" id="cb322-5" data-line-number="5"></a>
<a class="sourceLine" id="cb322-6" data-line-number="6"><span class="co"># Look at the model output                      </span></a>
<a class="sourceLine" id="cb322-7" data-line-number="7">pm_class_tree</a></code></pre></div>
<pre><code>## n= 614 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##  1) root 614 216 neg (0.6482 0.3518)  
##    2) glucose&lt; 144 475 113 neg (0.7621 0.2379)  
##      4) glucose&lt; 104 187  17 neg (0.9091 0.0909) *
##      5) glucose&gt;=104 288  96 neg (0.6667 0.3333)  
##       10) mass&lt; 26.9 72   7 neg (0.9028 0.0972) *
##       11) mass&gt;=26.9 216  89 neg (0.5880 0.4120)  
##         22) age&lt; 30.5 119  34 neg (0.7143 0.2857)  
##           44) pressure&gt;=22 112  28 neg (0.7500 0.2500) *
##           45) pressure&lt; 22 7   1 pos (0.1429 0.8571) *
##         23) age&gt;=30.5 97  42 pos (0.4330 0.5670)  
##           46) age&gt;=56.5 9   2 neg (0.7778 0.2222) *
##           47) age&lt; 56.5 88  35 pos (0.3977 0.6023)  
##             94) pedigree&lt; 0.201 11   3 neg (0.7273 0.2727) *
##             95) pedigree&gt;=0.201 77  27 pos (0.3506 0.6494) *
##    3) glucose&gt;=144 139  36 pos (0.2590 0.7410)  
##      6) mass&lt; 29.9 27  12 neg (0.5556 0.4444)  
##       12) pressure&gt;=74.5 9   1 neg (0.8889 0.1111) *
##       13) pressure&lt; 74.5 18   7 pos (0.3889 0.6111) *
##      7) mass&gt;=29.9 112  21 pos (0.1875 0.8125) *</code></pre>
<div class="sourceCode" id="cb324"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb324-1" data-line-number="1"><span class="kw">rpart.plot</span>(pm_class_tree)</a></code></pre></div>
<p><img src="biosml_files/figure-html/rpartbuild-1.png" width="672" /></p>
<div id="evaluating-performance" class="section level3">
<h3><span class="header-section-number">8.3.1</span> Evaluating performance</h3>
<p>So this is where things get interesting. We’ll use a confusion matrix to help us figure some things out about this model.</p>
<div class="sourceCode" id="cb325"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb325-1" data-line-number="1"><span class="co"># Generate predicted classes using the model object</span></a>
<a class="sourceLine" id="cb325-2" data-line-number="2">pm_class_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(<span class="dt">object =</span> pm_class_tree,  </a>
<a class="sourceLine" id="cb325-3" data-line-number="3">                        <span class="dt">newdata =</span> pm_test,   </a>
<a class="sourceLine" id="cb325-4" data-line-number="4">                        <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)  </a>
<a class="sourceLine" id="cb325-5" data-line-number="5">                            </a>
<a class="sourceLine" id="cb325-6" data-line-number="6"><span class="co"># Calculate the confusion matrix for the test set</span></a>
<a class="sourceLine" id="cb325-7" data-line-number="7">caret<span class="op">::</span><span class="kw">confusionMatrix</span>(pm_class_pred,       </a>
<a class="sourceLine" id="cb325-8" data-line-number="8">                       pm_test<span class="op">$</span>diabetes,</a>
<a class="sourceLine" id="cb325-9" data-line-number="9">                       <span class="dt">positive=</span><span class="st">&quot;pos&quot;</span>)  </a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction neg pos
##        neg  79  20
##        pos  23  32
##                                        
##                Accuracy : 0.721        
##                  95% CI : (0.643, 0.79)
##     No Information Rate : 0.662        
##     P-Value [Acc &gt; NIR] : 0.0721       
##                                        
##                   Kappa : 0.384        
##                                        
##  Mcnemar&#39;s Test P-Value : 0.7604       
##                                        
##             Sensitivity : 0.615        
##             Specificity : 0.775        
##          Pos Pred Value : 0.582        
##          Neg Pred Value : 0.798        
##              Prevalence : 0.338        
##          Detection Rate : 0.208        
##    Detection Prevalence : 0.357        
##       Balanced Accuracy : 0.695        
##                                        
##        &#39;Positive&#39; Class : pos          
## </code></pre>
<p>We can also look at the Area Under the ROC Curve.</p>
<div class="sourceCode" id="cb327"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb327-1" data-line-number="1"><span class="kw">library</span>(Metrics)</a>
<a class="sourceLine" id="cb327-2" data-line-number="2">dt_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(pm_class_tree,</a>
<a class="sourceLine" id="cb327-3" data-line-number="3">                       <span class="dt">newdata =</span> pm_test, <span class="dt">type=</span><span class="st">&quot;prob&quot;</span>)</a>
<a class="sourceLine" id="cb327-4" data-line-number="4"></a>
<a class="sourceLine" id="cb327-5" data-line-number="5">converted &lt;-<span class="st"> </span><span class="kw">ifelse</span>(pm_test<span class="op">$</span>diabetes <span class="op">==</span><span class="st"> &quot;pos&quot;</span>,<span class="dv">1</span>,<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb327-6" data-line-number="6"></a>
<a class="sourceLine" id="cb327-7" data-line-number="7">aucval    &lt;-<span class="st"> </span>Metrics<span class="op">::</span><span class="kw">auc</span>(<span class="dt">actual =</span> converted,</a>
<a class="sourceLine" id="cb327-8" data-line-number="8">                    <span class="dt">predicted =</span> dt_pred[,<span class="dv">2</span>])</a>
<a class="sourceLine" id="cb327-9" data-line-number="9"></a>
<a class="sourceLine" id="cb327-10" data-line-number="10">aucval</a></code></pre></div>
<pre><code>## [1] 0.739</code></pre>
</div>
<div id="tree-splitting" class="section level3">
<h3><span class="header-section-number">8.3.2</span> Tree Splitting</h3>
<p>The resulting tree can be thought of as an upside down tree with the root at the top. The “trunk” proceeds downward and splits into subsets based on some decision (hence the word “decision” in the title). When classifying data the idea is to segment or partition data into groups/regions where each group contains or represents a single class (“yes/no”, “positive/negative”).</p>
<p>These groups or regions would represent a “pure” region. This is not always possible so a best effort is made. These regions are separated by decision boundaries which are used to make decisions. We’ll plot some example data to illustrate the case.</p>
<p><img src="biosml_files/figure-html/unnamed-chunk-42-1.png" width="672" /></p>
<p><img src="biosml_files/figure-html/unnamed-chunk-43-1.png" width="672" /></p>
<p><img src="biosml_files/figure-html/unnamed-chunk-44-1.png" width="672" /></p>
</div>
</div>
<div id="gini-index" class="section level2">
<h2><span class="header-section-number">8.4</span> Gini Index</h2>
<p>So we have to find a way to make the decisions such that the resulting regions are as pure as possible. This could be measuring the degree of impurity or purity - so we are either maximizing purity or minimizing impurity The so called “Gini index”&quot; gives us the degree or measure of impurity.</p>
<p>The lower the Gini index, the lower the degree of impurity (this higher purity). The higher the Gini index the higher the degree of impurity (this lower purity). The decision tree will select the split that minimizes or lowers the Gini index. There are other measures or indices that can be used such as the “information” measure.</p>
<p>Let’s train two models that use a different splitting criterion (“gini” and “information”) and then use the test set to choose a “best” model. To do this you’ll use the <strong>parms</strong> argument of the <strong>rpart</strong> function. This argument takes a named list that contains values of different parameters to influence how the model is trained. Finally, to assess the models we’ll use the <strong>ce</strong> function from the <strong>Metrics</strong> package to show the proportion of elements in actual that are not equal to the corresponding element in predicted.</p>
<div class="sourceCode" id="cb329"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb329-1" data-line-number="1"><span class="co"># Train a gini-based model</span></a>
<a class="sourceLine" id="cb329-2" data-line-number="2">pm_gini_mod &lt;-<span class="st"> </span><span class="kw">rpart</span>(<span class="dt">formula =</span> diabetes <span class="op">~</span><span class="st"> </span>., </a>
<a class="sourceLine" id="cb329-3" data-line-number="3">                       <span class="dt">data =</span> pm_train, </a>
<a class="sourceLine" id="cb329-4" data-line-number="4">                       <span class="dt">method =</span> <span class="st">&quot;class&quot;</span>,</a>
<a class="sourceLine" id="cb329-5" data-line-number="5">                       <span class="dt">parms =</span> <span class="kw">list</span>(<span class="dt">split =</span> <span class="st">&quot;gini&quot;</span>))</a>
<a class="sourceLine" id="cb329-6" data-line-number="6"></a>
<a class="sourceLine" id="cb329-7" data-line-number="7"><span class="co"># Train an information-based model</span></a>
<a class="sourceLine" id="cb329-8" data-line-number="8">pm_info_mod &lt;-<span class="st"> </span><span class="kw">rpart</span>(<span class="dt">formula =</span> diabetes <span class="op">~</span><span class="st"> </span>., </a>
<a class="sourceLine" id="cb329-9" data-line-number="9">                       <span class="dt">data =</span> pm_train, </a>
<a class="sourceLine" id="cb329-10" data-line-number="10">                       <span class="dt">method =</span> <span class="st">&quot;class&quot;</span>,</a>
<a class="sourceLine" id="cb329-11" data-line-number="11">                       <span class="dt">parms =</span> <span class="kw">list</span>(<span class="dt">split =</span> <span class="st">&quot;information&quot;</span>))</a>
<a class="sourceLine" id="cb329-12" data-line-number="12"></a>
<a class="sourceLine" id="cb329-13" data-line-number="13"><span class="co"># Create some predictions</span></a>
<a class="sourceLine" id="cb329-14" data-line-number="14">gini_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(<span class="dt">object =</span> pm_gini_mod, </a>
<a class="sourceLine" id="cb329-15" data-line-number="15">             <span class="dt">newdata =</span> pm_test,</a>
<a class="sourceLine" id="cb329-16" data-line-number="16">             <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)    </a>
<a class="sourceLine" id="cb329-17" data-line-number="17"></a>
<a class="sourceLine" id="cb329-18" data-line-number="18"><span class="co"># Generate predictions on the validation set using the information model</span></a>
<a class="sourceLine" id="cb329-19" data-line-number="19">info_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(<span class="dt">object =</span> pm_info_mod, </a>
<a class="sourceLine" id="cb329-20" data-line-number="20">             <span class="dt">newdata =</span> pm_test,</a>
<a class="sourceLine" id="cb329-21" data-line-number="21">             <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)</a>
<a class="sourceLine" id="cb329-22" data-line-number="22"></a>
<a class="sourceLine" id="cb329-23" data-line-number="23"><span class="co"># Compare classification error</span></a>
<a class="sourceLine" id="cb329-24" data-line-number="24"></a>
<a class="sourceLine" id="cb329-25" data-line-number="25">Metrics<span class="op">::</span><span class="kw">ce</span>(<span class="dt">actual =</span> pm_test<span class="op">$</span>diabetes, </a>
<a class="sourceLine" id="cb329-26" data-line-number="26">   <span class="dt">predicted =</span> gini_pred)</a></code></pre></div>
<pre><code>## [1] 0.279</code></pre>
<div class="sourceCode" id="cb331"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb331-1" data-line-number="1">Metrics<span class="op">::</span><span class="kw">ce</span>(<span class="dt">actual =</span> pm_test<span class="op">$</span>diabetes, </a>
<a class="sourceLine" id="cb331-2" data-line-number="2">   <span class="dt">predicted =</span> info_pred)  </a></code></pre></div>
<pre><code>## [1] 0.26</code></pre>
</div>
<div id="regression-trees" class="section level2">
<h2><span class="header-section-number">8.5</span> Regression Trees</h2>
<p>Predicting numeric outcomes can also be of interest. Given some patient characteristics relative to a disease, we might want to predict a viral load quantity. A gambler might want to predict a final score for a team. Unlike, classification problems, we are looking at estimating a numeric outcome. R has a built in function for this called “lm” which can be used but we can also use Trees to do this since, after, all, it does some nice things for us like not having to worry about normalizing data (not that that is hard) or the mixture of quantitative and categorical data.</p>
<div id="performance-measure" class="section level3">
<h3><span class="header-section-number">8.5.1</span> Performance Measure</h3>
<p>Since we are predicting a numeric outcome we would like to come up with a metric to help us figure out if the model we have is good or not. With classification situations we can employ confusion matrices and ROC curves. Here we will use something more simplistic but effective - Root Mean Square Error. The formula looks like the following where P represents a vector of predictions and O represents a vector of the observed (true) values.</p>
<p><span class="math display">\[
RMSE = \sqrt\frac{\sum_i^n(P_i-O_i)^2}{n}
\]</span></p>
<p>We could even write our own function for this although the <strong>Metrics</strong> package has a function called <strong>rmse</strong> to do this. Let’s build a classification tree model on the Pima Indians data to predict the body mass of a participant.</p>
<div class="sourceCode" id="cb333"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb333-1" data-line-number="1"><span class="co"># Train the model</span></a>
<a class="sourceLine" id="cb333-2" data-line-number="2">mass_pima_mod &lt;-<span class="st"> </span><span class="kw">rpart</span>(<span class="dt">formula =</span> mass <span class="op">~</span><span class="st"> </span>., </a>
<a class="sourceLine" id="cb333-3" data-line-number="3">                     <span class="dt">data =</span> pm_train, </a>
<a class="sourceLine" id="cb333-4" data-line-number="4">                     <span class="dt">method =</span> <span class="st">&quot;anova&quot;</span>)</a>
<a class="sourceLine" id="cb333-5" data-line-number="5"></a>
<a class="sourceLine" id="cb333-6" data-line-number="6"><span class="co"># Look at the model output                      </span></a>
<a class="sourceLine" id="cb333-7" data-line-number="7"><span class="kw">print</span>(mass_pima_mod)</a></code></pre></div>
<pre><code>## n= 614 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
##  1) root 614 36900 32.0  
##    2) triceps&lt; 30.5 422 22000 29.7  
##      4) diabetes=neg 293 13600 28.3  
##        8) pressure&lt; 45 20  2690 21.2  
##         16) age&lt; 27 12  1620 16.2 *
##         17) age&gt;=27 8   309 28.7 *
##        9) pressure&gt;=45 273  9850 28.8  
##         18) triceps&lt; 22.5 190  6890 27.8 *
##         19) triceps&gt;=22.5 83  2420 30.9 *
##      5) diabetes=pos 129  6240 33.1  
##       10) glucose&lt; 126 47  3040 30.6  
##         20) pregnant&gt;=5.5 19  1700 26.6 *
##         21) pregnant&lt; 5.5 28   840 33.3 *
##       11) glucose&gt;=126 82  2730 34.6 *
##    3) triceps&gt;=30.5 192  8250 36.9  
##      6) pregnant&gt;=1.5 128  4020 35.1  
##       12) triceps&lt; 35.5 64  2190 33.1 *
##       13) triceps&gt;=35.5 64  1310 37.1 *
##      7) pregnant&lt; 1.5 64  3070 40.3  
##       14) glucose&lt; 128 38  1010 38.0 *
##       15) glucose&gt;=128 26  1570 43.7  
##         30) pressure&lt; 75 10   123 38.7 *
##         31) pressure&gt;=75 16  1030 46.8 *</code></pre>
<div class="sourceCode" id="cb335"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb335-1" data-line-number="1"><span class="co"># Plot the tree model</span></a>
<a class="sourceLine" id="cb335-2" data-line-number="2"><span class="kw">rpart.plot</span>(<span class="dt">x =</span> mass_pima_mod, </a>
<a class="sourceLine" id="cb335-3" data-line-number="3">           <span class="dt">yesno =</span> <span class="dv">2</span>, <span class="dt">type =</span> <span class="dv">0</span>, <span class="dt">extra =</span> <span class="dv">0</span>)</a></code></pre></div>
<p><img src="biosml_files/figure-html/unnamed-chunk-46-1.png" width="672" /></p>
<p>Let’s compute the RMSE for this model.</p>
<div class="sourceCode" id="cb336"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb336-1" data-line-number="1"><span class="co"># Generate predictions on a test set</span></a>
<a class="sourceLine" id="cb336-2" data-line-number="2">pred &lt;-<span class="st"> </span><span class="kw">predict</span>(<span class="dt">object =</span> mass_pima_mod,   <span class="co"># model object </span></a>
<a class="sourceLine" id="cb336-3" data-line-number="3">                <span class="dt">newdata =</span> pm_test)  <span class="co"># test dataset</span></a>
<a class="sourceLine" id="cb336-4" data-line-number="4"></a>
<a class="sourceLine" id="cb336-5" data-line-number="5"><span class="co"># Compute the RMSE</span></a>
<a class="sourceLine" id="cb336-6" data-line-number="6">Metrics<span class="op">::</span><span class="kw">rmse</span>(<span class="dt">actual =</span> pm_test<span class="op">$</span>mass, </a>
<a class="sourceLine" id="cb336-7" data-line-number="7">     <span class="dt">predicted =</span> pred)</a></code></pre></div>
<pre><code>## [1] 6.63</code></pre>
<p>Is this Good ? Bad ? Okay ? We don’t know. We’ll need to do some things like Cross Validation. And then there are additional arguments to the <strong>rpart</strong> function that we could use to influence how the model does its job. These are referred to as “hyperparamters”.</p>
</div>
</div>
<div id="parameters-vs-hyperparameters" class="section level2">
<h2><span class="header-section-number">8.6</span> Parameters vs Hyperparameters</h2>
<p><strong>Model parameters</strong> are things that are generated as part of the modeling process. These might be things like slope and intercept from a linear model or, in the case of an rpart model, the number of splits in the final tree or the total number of leaves.</p>
<p><strong>Hyper parameters</strong> (sometimes called <strong>metaparameters</strong>) represent information that is supplied in the form of an argument prior the call to the method to generate results. These parameters might not be something one can intelligently set without some experimentation.</p>
<p>Of course, most modeling functions one would call in R have default values for various arguments but this does not mean that the defaults are appropriate for all cases. To see the hyper parameters of the <strong>rpart</strong> function, check the help page for <strong>rpart.control</strong>.</p>
<p><img src="pics/rpartctrol2.png" /></p>
<p>Tuning the hyperparameters for <strong>rpart</strong> would involve adjusting the following hyper parameters or using some post processing function to refine the model relative to these parameters:</p>
<ul>
<li><strong>cp</strong> which is the complexity parameter (default is .01) - smaller values means more complexity</li>
<li><strong>minsplit</strong> the minimum number of observations that must exist in a node before a split is attempted (default is 20)</li>
<li><strong>maxdepth</strong> maximum number of nodes between a final node and root node</li>
</ul>
<p>There are other hyper parameters but we can start with these. The <strong>rpart.control</strong> function will do something called cross validation which involves repeatedly running the <strong>rpart</strong> some number of times (10 by default) while internally specifying different values for the above mentioned hyper parameters.</p>
<p>In effect, it is doing some work for you so you don’t have to. At the end of the run it will produce a table for inspection. The results of this table can then be used to “prune” the tree model to get a “better” tree - one that performs better than an “un pruned” tree. Let’s look at the pima mass model:</p>
<div class="sourceCode" id="cb338"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb338-1" data-line-number="1"><span class="co"># You can plot the modelling object</span></a>
<a class="sourceLine" id="cb338-2" data-line-number="2"><span class="kw">rpart.plot</span>(mass_pima_mod,<span class="dt">yesno =</span> <span class="dv">2</span>, <span class="dt">type =</span> <span class="dv">0</span>, <span class="dt">extra =</span> <span class="dv">0</span>)</a></code></pre></div>
<p><img src="biosml_files/figure-html/unnamed-chunk-48-1.png" width="672" /></p>
<div class="sourceCode" id="cb339"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb339-1" data-line-number="1"><span class="co"># You can inspect the table manually</span></a>
<a class="sourceLine" id="cb339-2" data-line-number="2">mass_pima_mod<span class="op">$</span>cptable</a></code></pre></div>
<pre><code>##        CP nsplit rel error xerror   xstd
## 1  0.1815      0     1.000  1.002 0.0938
## 2  0.0570      1     0.818  0.845 0.0809
## 3  0.0315      2     0.761  0.807 0.0795
## 4  0.0290      3     0.730  0.790 0.0794
## 5  0.0206      4     0.701  0.772 0.0763
## 6  0.0146      5     0.680  0.780 0.0781
## 7  0.0141      6     0.666  0.801 0.0773
## 8  0.0134      7     0.652  0.793 0.0762
## 9  0.0132      8     0.638  0.793 0.0771
## 10 0.0111     10     0.612  0.802 0.0772
## 11 0.0100     11     0.601  0.804 0.0730</code></pre>
<p>We want to pick the value of CP from the table that corresponds to the minimum <strong>xerror</strong> value. This can also be deduced from the plot but let’s work with the table:</p>
<div class="sourceCode" id="cb341"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb341-1" data-line-number="1">cpt &lt;-<span class="st"> </span><span class="kw">as.data.frame.matrix</span>(mass_pima_mod<span class="op">$</span>cptable) </a>
<a class="sourceLine" id="cb341-2" data-line-number="2">cpt_val &lt;-<span class="st"> </span>cpt[<span class="kw">order</span>(cpt<span class="op">$</span>xerror),][<span class="dv">1</span>,]<span class="op">$</span>CP</a></code></pre></div>
<p>We’ll use the CP value associated with the minimum error which in this case turns out to be 0.01. This is now passed to the <strong>prune</strong> function.</p>
<div class="sourceCode" id="cb342"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb342-1" data-line-number="1">mass_pima_mod_opt &lt;-<span class="st"> </span><span class="kw">prune</span>(mass_pima_mod, </a>
<a class="sourceLine" id="cb342-2" data-line-number="2">                           <span class="dt">cp =</span> cpt_val)</a>
<a class="sourceLine" id="cb342-3" data-line-number="3">                          </a>
<a class="sourceLine" id="cb342-4" data-line-number="4"><span class="co"># Plot the optimized model</span></a>
<a class="sourceLine" id="cb342-5" data-line-number="5"><span class="kw">rpart.plot</span>(mass_pima_mod_opt, <span class="dt">yesno =</span> <span class="dv">2</span>, <span class="dt">type =</span> <span class="dv">0</span>, <span class="dt">extra =</span> <span class="dv">0</span>)</a></code></pre></div>
<p><img src="biosml_files/figure-html/unnamed-chunk-50-1.png" width="672" /></p>
<p>Does this optimized model perform any better ? Not really, because the optimal CP value turned out to be 0.01 which is actually the same as the default.</p>
<div class="sourceCode" id="cb343"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb343-1" data-line-number="1"><span class="co"># Generate predictions on a test set</span></a>
<a class="sourceLine" id="cb343-2" data-line-number="2">pred &lt;-<span class="st"> </span><span class="kw">predict</span>(<span class="dt">object =</span> mass_pima_mod_opt,   <span class="co"># model object </span></a>
<a class="sourceLine" id="cb343-3" data-line-number="3">                <span class="dt">newdata =</span> pm_test)  <span class="co"># test dataset</span></a>
<a class="sourceLine" id="cb343-4" data-line-number="4"></a>
<a class="sourceLine" id="cb343-5" data-line-number="5"><span class="co"># Compute the RMSE</span></a>
<a class="sourceLine" id="cb343-6" data-line-number="6">Metrics<span class="op">::</span><span class="kw">rmse</span>(<span class="dt">actual =</span> pm_test<span class="op">$</span>mass, </a>
<a class="sourceLine" id="cb343-7" data-line-number="7">     <span class="dt">predicted =</span> pred)</a></code></pre></div>
<pre><code>## [1] 6.79</code></pre>
</div>
<div id="grid-searching" class="section level2">
<h2><span class="header-section-number">8.7</span> Grid Searching</h2>
<p>We might want to review several different models that correspond to various hyperparamter sets. Our goal is to find the best performing model based on a systematic approach that allows us to assess each model in a fair way. There are functions that can help us build a “grid” of hyperparameter values that can then “feed” the function arguments. So we train models on a combination of these values and compare them using the RMSE for regression or ROC / Confusion Matrix for classification setups.</p>
<p>Setting up the grid involves a manual process (although as we will eventually see) the <strong>caret</strong> package can help automate this for us. Knowing about the valid values for a hyperparameter is critical so some experimentation is important. The following process sets up a data frame of two columns each of which corresponds to a hyperparamter of the <strong>rpart</strong> function. The intent here is to call <strong>rpart</strong> a number of times using each row of the below data frame to supply the values for the respective arguments in <strong>rpart</strong>.</p>
<div class="sourceCode" id="cb345"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb345-1" data-line-number="1">mysplits &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">35</span>, <span class="dv">5</span>)</a>
<a class="sourceLine" id="cb345-2" data-line-number="2">mydepths &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">5</span>,<span class="dv">40</span>, <span class="dv">10</span>)</a>
<a class="sourceLine" id="cb345-3" data-line-number="3"></a>
<a class="sourceLine" id="cb345-4" data-line-number="4">my_cool_grid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">minsplit =</span> mysplits,</a>
<a class="sourceLine" id="cb345-5" data-line-number="5">                            <span class="dt">maxdepth =</span> mydepths)</a>
<a class="sourceLine" id="cb345-6" data-line-number="6"></a>
<a class="sourceLine" id="cb345-7" data-line-number="7"><span class="kw">head</span>(my_cool_grid)</a></code></pre></div>
<pre><code>##   minsplit maxdepth
## 1        1        5
## 2        6        5
## 3       11        5
## 4       16        5
## 5       21        5
## 6       26        5</code></pre>
<p>We’ll generate models, predictions, and RMSE values and stash them for later review. Once again, we’ll be predicting the mass variable in the Pima Indians data frame.</p>
<div class="sourceCode" id="cb347"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb347-1" data-line-number="1">do_grid_search &lt;-<span class="st"> </span><span class="cf">function</span>(minsplit,maxdepth) {</a>
<a class="sourceLine" id="cb347-2" data-line-number="2">  </a>
<a class="sourceLine" id="cb347-3" data-line-number="3">  <span class="co"># Setup some book keeping structures</span></a>
<a class="sourceLine" id="cb347-4" data-line-number="4">  </a>
<a class="sourceLine" id="cb347-5" data-line-number="5">  mods  &lt;-<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb347-6" data-line-number="6">  preds &lt;-<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb347-7" data-line-number="7">  myrmse  &lt;-<span class="st"> </span><span class="kw">vector</span>()</a>
<a class="sourceLine" id="cb347-8" data-line-number="8">  </a>
<a class="sourceLine" id="cb347-9" data-line-number="9">  mygrid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">minsplit=</span>minsplit,</a>
<a class="sourceLine" id="cb347-10" data-line-number="10">                        <span class="dt">maxdepth=</span>maxdepth)</a>
<a class="sourceLine" id="cb347-11" data-line-number="11">  </a>
<a class="sourceLine" id="cb347-12" data-line-number="12">  <span class="cf">for</span> (ii <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(mygrid)) {</a>
<a class="sourceLine" id="cb347-13" data-line-number="13">    minsplit &lt;-<span class="st"> </span>mygrid[ii,]<span class="op">$</span>minsplit</a>
<a class="sourceLine" id="cb347-14" data-line-number="14">    maxdepth &lt;-<span class="st"> </span>mygrid[ii,]<span class="op">$</span>maxdepth</a>
<a class="sourceLine" id="cb347-15" data-line-number="15">    </a>
<a class="sourceLine" id="cb347-16" data-line-number="16">    <span class="co"># Build the Model</span></a>
<a class="sourceLine" id="cb347-17" data-line-number="17">    mods[[ii]] &lt;-<span class="st"> </span><span class="kw">rpart</span>(mass <span class="op">~</span><span class="st"> </span>., </a>
<a class="sourceLine" id="cb347-18" data-line-number="18">                        <span class="dt">data =</span> pm_train,</a>
<a class="sourceLine" id="cb347-19" data-line-number="19">                        <span class="dt">method =</span> <span class="st">&quot;anova&quot;</span>,</a>
<a class="sourceLine" id="cb347-20" data-line-number="20">                        <span class="dt">minsplit =</span> minsplit,</a>
<a class="sourceLine" id="cb347-21" data-line-number="21">                        <span class="dt">maxdepth =</span> maxdepth)</a>
<a class="sourceLine" id="cb347-22" data-line-number="22">    </a>
<a class="sourceLine" id="cb347-23" data-line-number="23">    <span class="co"># Now predict against the test data</span></a>
<a class="sourceLine" id="cb347-24" data-line-number="24">    preds[[ii]] &lt;-<span class="st"> </span><span class="kw">predict</span>(mods[[ii]],</a>
<a class="sourceLine" id="cb347-25" data-line-number="25">                           <span class="dt">newdata =</span> pm_test)</a>
<a class="sourceLine" id="cb347-26" data-line-number="26">    </a>
<a class="sourceLine" id="cb347-27" data-line-number="27">    <span class="co"># Get RMSE</span></a>
<a class="sourceLine" id="cb347-28" data-line-number="28">    myrmse[ii] &lt;-<span class="st"> </span>Metrics<span class="op">::</span><span class="kw">rmse</span>(<span class="dt">actual =</span> pm_test<span class="op">$</span>mass, </a>
<a class="sourceLine" id="cb347-29" data-line-number="29">                                <span class="dt">predicted =</span> preds[[ii]])</a>
<a class="sourceLine" id="cb347-30" data-line-number="30">  }</a>
<a class="sourceLine" id="cb347-31" data-line-number="31">  </a>
<a class="sourceLine" id="cb347-32" data-line-number="32">  <span class="co"># Find the model that has the lowest rmse</span></a>
<a class="sourceLine" id="cb347-33" data-line-number="33">  idx &lt;-<span class="st"> </span><span class="kw">which.min</span>(myrmse)</a>
<a class="sourceLine" id="cb347-34" data-line-number="34">  </a>
<a class="sourceLine" id="cb347-35" data-line-number="35">  <span class="co"># Get the control parameters for the best model</span></a>
<a class="sourceLine" id="cb347-36" data-line-number="36">  optimal_model &lt;-<span class="st"> </span>mods[[idx]]</a>
<a class="sourceLine" id="cb347-37" data-line-number="37">  rmseval &lt;-<span class="st"> </span>myrmse[idx]</a>
<a class="sourceLine" id="cb347-38" data-line-number="38">  </a>
<a class="sourceLine" id="cb347-39" data-line-number="39">  <span class="kw">return</span>(<span class="kw">list</span>(<span class="dt">optimal_model=</span>optimal_model,<span class="dt">optim_rmse=</span>rmseval))</a>
<a class="sourceLine" id="cb347-40" data-line-number="40">}</a></code></pre></div>
<p>Now run the function and get the parameters corresponding to the optimal model.</p>
<div class="sourceCode" id="cb348"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb348-1" data-line-number="1">results &lt;-<span class="st"> </span><span class="kw">do_grid_search</span>(<span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">1</span>),<span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">1</span>))</a>
<a class="sourceLine" id="cb348-2" data-line-number="2">results<span class="op">$</span>optim_rmse</a></code></pre></div>
<pre><code>## [1] 6.85</code></pre>
<div class="sourceCode" id="cb350"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb350-1" data-line-number="1">results<span class="op">$</span>optimal_model<span class="op">$</span>control</a></code></pre></div>
<pre><code>## $minsplit
## [1] 2
## 
## $minbucket
## [1] 1
## 
## $cp
## [1] 0.01
## 
## $maxcompete
## [1] 4
## 
## $maxsurrogate
## [1] 5
## 
## $usesurrogate
## [1] 2
## 
## $surrogatestyle
## [1] 0
## 
## $maxdepth
## [1] 4
## 
## $xval
## [1] 10</code></pre>
</div>
<div id="bagged-trees" class="section level2">
<h2><span class="header-section-number">8.8</span> Bagged Trees</h2>
<p>Now we look at bagged trees which involves looking at many trees in aggregate - this is an ensemble method. It helps to reduce the variance associated with a single decision tree which can be highly sensitive to changes in data. The term bagging refers to “bootstrap aggregation”. To review, the Decision Tree process can be represented like this:</p>
<p><img src="pics/tree2.png" /></p>
<p>The bootstrap method of sampling will resample the training data some number of times (with replacement) and retrain a number of models on the resampled data to average out the error. This looks something like:</p>
<ul>
<li>The input data set is resampled with replacement some number of times (e.g. 10,50, 100)</li>
<li>The resampled data is usually a subset of the data which leaves some portion of the data available to use a mini test set for prediction (“out of bag data”&quot;)</li>
<li>Get the RMSE from the prediction</li>
<li>Average out the RMSE</li>
</ul>
<p><img src="pics/bagged.png" /></p>
<p>So you get a lot of different trees whose performance can be averaged over boostrapped data sets which might include observations several times or not at all. This should result in less variance. A reason NOT to use bagged trees involves the idea that a collection of trees is not nearly as easy to look at as a single decision tree. We can actually write our own version of bagged trees using a for loop. Sort of like what we did above. First, let’s get our test / train pair for the Pima Indians data.</p>
<div class="sourceCode" id="cb352"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb352-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb352-2" data-line-number="2">percent &lt;-<span class="st"> </span><span class="fl">.80</span></a>
<a class="sourceLine" id="cb352-3" data-line-number="3">train_idx &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(pm),</a>
<a class="sourceLine" id="cb352-4" data-line-number="4">                      <span class="kw">round</span>(percent<span class="op">*</span><span class="kw">nrow</span>(pm)))</a>
<a class="sourceLine" id="cb352-5" data-line-number="5"></a>
<a class="sourceLine" id="cb352-6" data-line-number="6">train_idx[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>]</a></code></pre></div>
<pre><code>##  [1] 415 463 179 526 195 118 299 229 244  14</code></pre>
<div class="sourceCode" id="cb354"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb354-1" data-line-number="1"><span class="co"># Subset the pm data frame to training indices only</span></a>
<a class="sourceLine" id="cb354-2" data-line-number="2">pm_train &lt;-<span class="st"> </span>pm[train_idx, ]  </a>
<a class="sourceLine" id="cb354-3" data-line-number="3">  </a>
<a class="sourceLine" id="cb354-4" data-line-number="4"><span class="co"># Exclude the training indices to create the test set</span></a>
<a class="sourceLine" id="cb354-5" data-line-number="5">pm_test &lt;-<span class="st"> </span>pm[<span class="op">-</span>train_idx, ]  </a></code></pre></div>
<p>Next, well create 50 different trees based on 50 bootstrapped samples of the training data. We will sample WITH replacement. This means that some of the rows from the data will be repeated and some will be left out all together. We can figure out what rows were not included and use them to create a test set referred to as “out of bag” samples.</p>
<div class="sourceCode" id="cb355"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb355-1" data-line-number="1">training_boot_idx &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(pm),<span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb355-2" data-line-number="2">test_boot_idx &lt;-<span class="st"> </span><span class="op">!</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">32</span> <span class="op">%in%</span><span class="st"> </span>training_boot_idx)</a></code></pre></div>
<p>Let’s start looping</p>
<div class="sourceCode" id="cb356"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb356-1" data-line-number="1"><span class="kw">library</span>(Metrics)</a>
<a class="sourceLine" id="cb356-2" data-line-number="2"></a>
<a class="sourceLine" id="cb356-3" data-line-number="3">modl   &lt;-<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb356-4" data-line-number="4">predl  &lt;-<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb356-5" data-line-number="5">aucval &lt;-<span class="st"> </span><span class="kw">vector</span>()</a>
<a class="sourceLine" id="cb356-6" data-line-number="6">acc    &lt;-<span class="st"> </span><span class="kw">vector</span>()</a>
<a class="sourceLine" id="cb356-7" data-line-number="7"></a>
<a class="sourceLine" id="cb356-8" data-line-number="8">number_of_boostraps &lt;-<span class="st"> </span><span class="dv">50</span></a>
<a class="sourceLine" id="cb356-9" data-line-number="9"></a>
<a class="sourceLine" id="cb356-10" data-line-number="10"><span class="cf">for</span> (ii <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>number_of_boostraps) {</a>
<a class="sourceLine" id="cb356-11" data-line-number="11">  </a>
<a class="sourceLine" id="cb356-12" data-line-number="12">  training_boot_idx &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(pm),<span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb356-13" data-line-number="13">  test_boot_idx &lt;-<span class="st"> </span><span class="op">!</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">32</span> <span class="op">%in%</span><span class="st"> </span>training_boot_idx)</a>
<a class="sourceLine" id="cb356-14" data-line-number="14">  </a>
<a class="sourceLine" id="cb356-15" data-line-number="15">  modl[[ii]] &lt;-<span class="st"> </span><span class="kw">rpart</span>(diabetes <span class="op">~</span><span class="st"> </span>., </a>
<a class="sourceLine" id="cb356-16" data-line-number="16">                   <span class="dt">data =</span> pm[training_boot_idx,],</a>
<a class="sourceLine" id="cb356-17" data-line-number="17">                   <span class="dt">method =</span> <span class="st">&quot;class&quot;</span></a>
<a class="sourceLine" id="cb356-18" data-line-number="18">                )</a>
<a class="sourceLine" id="cb356-19" data-line-number="19">  </a>
<a class="sourceLine" id="cb356-20" data-line-number="20">  <span class="co"># This list will contain / hold the models build on the bootstrap</span></a>
<a class="sourceLine" id="cb356-21" data-line-number="21">  </a>
<a class="sourceLine" id="cb356-22" data-line-number="22">  predl[[ii]]  &lt;-<span class="st"> </span><span class="kw">predict</span>(modl[[ii]],</a>
<a class="sourceLine" id="cb356-23" data-line-number="23">                          <span class="dt">newdata=</span>pm[test_boot_idx,],</a>
<a class="sourceLine" id="cb356-24" data-line-number="24">                          <span class="dt">type=</span><span class="st">&quot;prob&quot;</span>)</a>
<a class="sourceLine" id="cb356-25" data-line-number="25">  </a>
<a class="sourceLine" id="cb356-26" data-line-number="26">  converted &lt;-<span class="st"> </span><span class="kw">ifelse</span>(pm[test_boot_idx,]<span class="op">$</span>diabetes <span class="op">==</span><span class="st"> &quot;pos&quot;</span>,<span class="dv">1</span>,<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb356-27" data-line-number="27">  </a>
<a class="sourceLine" id="cb356-28" data-line-number="28">  <span class="co"># Let&#39;s create an estimate of the AUC</span></a>
<a class="sourceLine" id="cb356-29" data-line-number="29">  </a>
<a class="sourceLine" id="cb356-30" data-line-number="30">  aucval[ii] &lt;-<span class="st"> </span>Metrics<span class="op">::</span><span class="kw">auc</span>(<span class="dt">actual =</span> converted,</a>
<a class="sourceLine" id="cb356-31" data-line-number="31">                    <span class="dt">predicted =</span> predl[[ii]][,<span class="dv">2</span>])</a>
<a class="sourceLine" id="cb356-32" data-line-number="32">}</a></code></pre></div>
<p>Now check all of the accuracy estimates and then average them. Remember this is supposed to help us to better estimate the out of sample error.</p>
<div class="sourceCode" id="cb357"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb357-1" data-line-number="1"><span class="kw">boxplot</span>(aucval)</a></code></pre></div>
<p><img src="biosml_files/figure-html/unnamed-chunk-55-1.png" width="672" /></p>
<div class="sourceCode" id="cb358"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb358-1" data-line-number="1"><span class="kw">mean</span>(aucval)</a></code></pre></div>
<pre><code>## [1] 0.822</code></pre>
<p>Now we’ll compare this to the function called “bagging” from the <strong>ipred</strong> package which does bagged trees directly.</p>
<p><img src="pics/bagging2.png" /></p>
<div class="sourceCode" id="cb360"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb360-1" data-line-number="1"><span class="kw">library</span>(ipred)</a>
<a class="sourceLine" id="cb360-2" data-line-number="2">bagged_pm &lt;-<span class="st"> </span><span class="kw">bagging</span>(diabetes <span class="op">~</span><span class="st"> </span>.,</a>
<a class="sourceLine" id="cb360-3" data-line-number="3">                     <span class="dt">data =</span> pm_train, <span class="dt">coob =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb360-4" data-line-number="4"></a>
<a class="sourceLine" id="cb360-5" data-line-number="5">bagged_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(bagged_pm,</a>
<a class="sourceLine" id="cb360-6" data-line-number="6">                       <span class="dt">newdata =</span> pm_test, <span class="dt">type=</span><span class="st">&quot;prob&quot;</span>)</a></code></pre></div>
<div class="sourceCode" id="cb361"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb361-1" data-line-number="1">converted &lt;-<span class="st"> </span><span class="kw">ifelse</span>(pm_test<span class="op">$</span>diabetes <span class="op">==</span><span class="st"> &quot;pos&quot;</span>,<span class="dv">1</span>,<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb361-2" data-line-number="2"></a>
<a class="sourceLine" id="cb361-3" data-line-number="3">aucval    &lt;-<span class="st"> </span>Metrics<span class="op">::</span><span class="kw">auc</span>(<span class="dt">actual =</span> converted,</a>
<a class="sourceLine" id="cb361-4" data-line-number="4">                    <span class="dt">predicted =</span> bagged_pred[,<span class="dv">2</span>])</a>
<a class="sourceLine" id="cb361-5" data-line-number="5"></a>
<a class="sourceLine" id="cb361-6" data-line-number="6">aucval</a></code></pre></div>
<pre><code>## [1] 0.831</code></pre>
<p>This is better than the 0.7804305 we got when using a single Decision tree. Single decision trees are sometimes called “weak learners” because</p>
</div>
<div id="random-forests-1" class="section level2">
<h2><span class="header-section-number">8.9</span> Random Forests</h2>
<p>In bagging, all features are used when considering if and when to split. However, with the Random Forest approach, a subset of features are selected at random at each split in a decision tree. You could think of random forests as being an extension of Bagged Trees. Typically they are also an improvement. As with other tree methods, we don’t have to worry a lot about preprocessing the data although we could if we wanted to. Basically, one of the main advantages of tree methods is that it tolerates a combination of data types on different scales which makes it good as a “go to” method for beginners.</p>
<p>Random forests will sample some number of features when considering a split. This can be influenced by a hyperparameter called <strong>mtry</strong> which is limited to the number of features in the data set.</p>
<div class="sourceCode" id="cb363"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb363-1" data-line-number="1"><span class="kw">library</span>(randomForest)</a></code></pre></div>
<div class="sourceCode" id="cb364"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb364-1" data-line-number="1"><span class="co"># Train a Random Forest</span></a>
<a class="sourceLine" id="cb364-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">1</span>)  <span class="co"># for reproducibility</span></a>
<a class="sourceLine" id="cb364-3" data-line-number="3">pm_rf_model &lt;-<span class="st"> </span><span class="kw">randomForest</span>(<span class="dt">formula =</span> diabetes <span class="op">~</span><span class="st"> </span>., </a>
<a class="sourceLine" id="cb364-4" data-line-number="4">                             <span class="dt">data =</span> pm_train)</a>
<a class="sourceLine" id="cb364-5" data-line-number="5">                             </a>
<a class="sourceLine" id="cb364-6" data-line-number="6"><span class="co"># Print the model output                             </span></a>
<a class="sourceLine" id="cb364-7" data-line-number="7"><span class="kw">print</span>(pm_rf_model)</a></code></pre></div>
<pre><code>## 
## Call:
##  randomForest(formula = diabetes ~ ., data = pm_train) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 2
## 
##         OOB estimate of  error rate: 25.6%
## Confusion matrix:
##     neg pos class.error
## neg 336  62       0.156
## pos  95 121       0.440</code></pre>
<p>Let’s look at the out of bag error matrix</p>
<div class="sourceCode" id="cb366"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb366-1" data-line-number="1"><span class="kw">head</span>(pm_rf_model<span class="op">$</span>err.rate)</a></code></pre></div>
<pre><code>##        OOB   neg   pos
## [1,] 0.327 0.282 0.405
## [2,] 0.319 0.253 0.426
## [3,] 0.298 0.213 0.447
## [4,] 0.286 0.201 0.438
## [5,] 0.299 0.214 0.448
## [6,] 0.293 0.193 0.473</code></pre>
<div class="sourceCode" id="cb368"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb368-1" data-line-number="1"><span class="kw">plot</span>(pm_rf_model)</a></code></pre></div>
<p><img src="biosml_files/figure-html/unnamed-chunk-58-1.png" width="672" /></p>
<p>Do some predictions with this model.</p>
<div class="sourceCode" id="cb369"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb369-1" data-line-number="1">rf_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(pm_rf_model,</a>
<a class="sourceLine" id="cb369-2" data-line-number="2">                   <span class="dt">newdata =</span> pm_test, <span class="dt">type=</span><span class="st">&quot;prob&quot;</span>)</a>
<a class="sourceLine" id="cb369-3" data-line-number="3"></a>
<a class="sourceLine" id="cb369-4" data-line-number="4">converted &lt;-<span class="st"> </span><span class="kw">ifelse</span>(pm_test<span class="op">$</span>diabetes <span class="op">==</span><span class="st"> &quot;pos&quot;</span>,<span class="dv">1</span>,<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb369-5" data-line-number="5"></a>
<a class="sourceLine" id="cb369-6" data-line-number="6">aucval    &lt;-<span class="st"> </span>Metrics<span class="op">::</span><span class="kw">auc</span>(<span class="dt">actual =</span> converted,</a>
<a class="sourceLine" id="cb369-7" data-line-number="7">                    <span class="dt">predicted =</span> rf_pred[,<span class="dv">2</span>])</a>
<a class="sourceLine" id="cb369-8" data-line-number="8"></a>
<a class="sourceLine" id="cb369-9" data-line-number="9">aucval</a></code></pre></div>
<pre><code>## [1] 0.845</code></pre>
<p>A slight improvement over the bagged trees.</p>
</div>
<div id="boosted-trees" class="section level2">
<h2><span class="header-section-number">8.10</span> Boosted Trees</h2>
<p>We’ll finish off this section with a discussion on boosted trees which represents an extension to random forests. Here is how we have been progressing thus far:</p>
<ol style="list-style-type: decimal">
<li>Single Decision Tree</li>
<li>Bagged Decision Trees (Aggregated Trees using all features)</li>
<li>Random Forests (Many Trees using a number of of sampled features)</li>
</ol>
<p>Methods 2 and 3 will use bootstrap sampling on the input data which means there will be sampling with replacement to generate a training set. After a tree is built then it will be applied to the OOB (Out Of Band) data left over from the bootstrap sample. This will then be used in the computation of an average.</p>
<p>With boosting, we don’t try to keep up with the idea of reducing the variance emerging from a number of individual trees (aka “learners”). Nor do we consider each tree as being independent and later try to integrate it into an average tree. With boosting we create a sequence of trees such that any subsequent tree represents an improvement on the previous tree(s) thus there is some dependency in the interest of improvement.</p>
<p>The process attempts to learn from previous “mistakes” in the creation of down stream trees. Boosting looks at the residuals from a tree and pays attention to any problematic observations when creating a subsequent tree. Here is an example. We’ll need to do a bit more prep on the data though prior to use:</p>
<div class="sourceCode" id="cb371"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb371-1" data-line-number="1">prep_pm_train &lt;-<span class="st"> </span>pm_train</a>
<a class="sourceLine" id="cb371-2" data-line-number="2">prep_pm_train<span class="op">$</span>diabetes &lt;-<span class="st"> </span><span class="kw">ifelse</span>(pm_train<span class="op">$</span>diabetes<span class="op">==</span><span class="st">&quot;pos&quot;</span>,<span class="dv">1</span>,<span class="dv">0</span>)</a></code></pre></div>
<p>We had to turn the “pos” / “neg” values into, respectively, 1 and 0. The <strong>gbm</strong> function requires this for classification problems.</p>
<div class="sourceCode" id="cb372"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb372-1" data-line-number="1"><span class="kw">library</span>(gbm)</a>
<a class="sourceLine" id="cb372-2" data-line-number="2">pm_boost &lt;-<span class="st"> </span><span class="kw">gbm</span>(diabetes <span class="op">~</span><span class="st"> </span>. ,</a>
<a class="sourceLine" id="cb372-3" data-line-number="3">                <span class="dt">distribution=</span><span class="st">&quot;bernoulli&quot;</span>,</a>
<a class="sourceLine" id="cb372-4" data-line-number="4">                <span class="dt">data =</span> prep_pm_train,<span class="dt">n.trees=</span><span class="dv">1000</span>)</a>
<a class="sourceLine" id="cb372-5" data-line-number="5"><span class="kw">summary</span>(pm_boost)</a></code></pre></div>
<p><img src="biosml_files/figure-html/gbmload-1.png" width="672" /></p>
<pre><code>##               var rel.inf
## glucose   glucose   24.38
## mass         mass   16.15
## pedigree pedigree   14.77
## age           age   12.97
## insulin   insulin    9.67
## pressure pressure    8.55
## pregnant pregnant    7.54
## triceps   triceps    5.96</code></pre>
<div class="sourceCode" id="cb374"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb374-1" data-line-number="1">boost_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(pm_boost,</a>
<a class="sourceLine" id="cb374-2" data-line-number="2">                      <span class="dt">newdata =</span> pm_test, </a>
<a class="sourceLine" id="cb374-3" data-line-number="3">                      <span class="dt">type=</span><span class="st">&quot;response&quot;</span>,</a>
<a class="sourceLine" id="cb374-4" data-line-number="4">                      <span class="dt">n.trees=</span><span class="dv">50</span>)</a>
<a class="sourceLine" id="cb374-5" data-line-number="5"></a>
<a class="sourceLine" id="cb374-6" data-line-number="6">converted &lt;-<span class="st"> </span><span class="kw">ifelse</span>(pm_test<span class="op">$</span>diabetes <span class="op">==</span><span class="st"> &quot;pos&quot;</span>,<span class="dv">1</span>,<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb374-7" data-line-number="7"></a>
<a class="sourceLine" id="cb374-8" data-line-number="8">aucval    &lt;-<span class="st"> </span>Metrics<span class="op">::</span><span class="kw">auc</span>(<span class="dt">actual =</span> converted,</a>
<a class="sourceLine" id="cb374-9" data-line-number="9">                    <span class="dt">predicted =</span> boost_pred)</a>
<a class="sourceLine" id="cb374-10" data-line-number="10"></a>
<a class="sourceLine" id="cb374-11" data-line-number="11">aucval</a></code></pre></div>
<pre><code>## [1] 0.838</code></pre>
<p>Of course, we can use the <strong>caret</strong> package to knit these altogether into one framework that dispenses wit the need to understand the details of each method although it is important for you to see examples of standalone functions since 1) it can help your understanding of the method and 2) in languages such as Python there is no equivalent to <strong>caret</strong>.</p>
</div>
<div id="using-caret" class="section level2">
<h2><span class="header-section-number">8.11</span> Using caret</h2>
<p>We looked at individual methods including <strong>rpart, bagging, random forests</strong>, and <strong>boosted trees</strong>. Each of these had its own approach and hyperparameters. With caret we can do basically plugin methods.</p>
<div class="sourceCode" id="cb376"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb376-1" data-line-number="1">control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;cv&quot;</span>,</a>
<a class="sourceLine" id="cb376-2" data-line-number="2">                        <span class="dt">number=</span><span class="dv">10</span>,</a>
<a class="sourceLine" id="cb376-3" data-line-number="3">                        <span class="dt">summaryFunction =</span> twoClassSummary,</a>
<a class="sourceLine" id="cb376-4" data-line-number="4">                        <span class="dt">classProbs =</span> <span class="ot">TRUE</span>,</a>
<a class="sourceLine" id="cb376-5" data-line-number="5">                        <span class="dt">savePredictions =</span> <span class="ot">TRUE</span>,</a>
<a class="sourceLine" id="cb376-6" data-line-number="6">                        <span class="dt">verboseIter =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb376-7" data-line-number="7"></a>
<a class="sourceLine" id="cb376-8" data-line-number="8">rpart_caret &lt;-<span class="st"> </span><span class="kw">train</span>(diabetes <span class="op">~</span><span class="st"> </span>.,</a>
<a class="sourceLine" id="cb376-9" data-line-number="9">                     <span class="dt">data =</span> pm_test,</a>
<a class="sourceLine" id="cb376-10" data-line-number="10">                     <span class="dt">method =</span> <span class="st">&quot;rpart&quot;</span>,</a>
<a class="sourceLine" id="cb376-11" data-line-number="11">                     <span class="dt">metric =</span> <span class="st">&quot;ROC&quot;</span>,</a>
<a class="sourceLine" id="cb376-12" data-line-number="12">                     <span class="dt">trControl =</span> control)</a>
<a class="sourceLine" id="cb376-13" data-line-number="13">rpart_caret</a></code></pre></div>
<pre><code>## CART 
## 
## 154 samples
##   8 predictor
##   2 classes: &#39;neg&#39;, &#39;pos&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 139, 138, 139, 139, 139, 137, ... 
## Resampling results across tuning parameters:
## 
##   cp      ROC    Sens   Spec 
##   0.0288  0.660  0.803  0.417
##   0.0481  0.664  0.803  0.460
##   0.1731  0.501  0.852  0.160
## 
## ROC was used to select the optimal model using the largest value.
## The final value used for the model was cp = 0.0481.</code></pre>
<p>Next, we try out the bagging approach</p>
<div class="sourceCode" id="cb378"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb378-1" data-line-number="1">bagging_caret &lt;-<span class="st">    </span><span class="kw">train</span>(diabetes <span class="op">~</span><span class="st"> </span>.,</a>
<a class="sourceLine" id="cb378-2" data-line-number="2">                     <span class="dt">data =</span> pm_test,</a>
<a class="sourceLine" id="cb378-3" data-line-number="3">                     <span class="dt">method =</span> <span class="st">&quot;treebag&quot;</span>,</a>
<a class="sourceLine" id="cb378-4" data-line-number="4">                     <span class="dt">metric =</span> <span class="st">&quot;ROC&quot;</span>,</a>
<a class="sourceLine" id="cb378-5" data-line-number="5">                     <span class="dt">trControl =</span> control)</a>
<a class="sourceLine" id="cb378-6" data-line-number="6">bagging_caret</a></code></pre></div>
<pre><code>## Bagged CART 
## 
## 154 samples
##   8 predictor
##   2 classes: &#39;neg&#39;, &#39;pos&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 139, 138, 137, 138, 139, 139, ... 
## Resampling results:
## 
##   ROC    Sens   Spec 
##   0.781  0.834  0.557</code></pre>
<p>Next we try out the random forest function</p>
<div class="sourceCode" id="cb380"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb380-1" data-line-number="1">rf_caret &lt;-<span class="st">    </span><span class="kw">train</span>(diabetes <span class="op">~</span><span class="st"> </span>.,</a>
<a class="sourceLine" id="cb380-2" data-line-number="2">                     <span class="dt">data =</span> pm_test,</a>
<a class="sourceLine" id="cb380-3" data-line-number="3">                     <span class="dt">method =</span> <span class="st">&quot;rf&quot;</span>,</a>
<a class="sourceLine" id="cb380-4" data-line-number="4">                     <span class="dt">metric =</span> <span class="st">&quot;ROC&quot;</span>,</a>
<a class="sourceLine" id="cb380-5" data-line-number="5">                     <span class="dt">trControl =</span> control)</a>
<a class="sourceLine" id="cb380-6" data-line-number="6">rf_caret</a></code></pre></div>
<pre><code>## Random Forest 
## 
## 154 samples
##   8 predictor
##   2 classes: &#39;neg&#39;, &#39;pos&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 138, 139, 138, 139, 138, 139, ... 
## Resampling results across tuning parameters:
## 
##   mtry  ROC    Sens   Spec 
##   2     0.814  0.864  0.550
##   5     0.810  0.853  0.553
##   8     0.799  0.853  0.533
## 
## ROC was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 2.</code></pre>
<p>And finally, the boosted approach:</p>
<div class="sourceCode" id="cb382"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb382-1" data-line-number="1">boosted_caret &lt;-<span class="st">    </span><span class="kw">train</span>(diabetes <span class="op">~</span><span class="st"> </span>.,</a>
<a class="sourceLine" id="cb382-2" data-line-number="2">                         <span class="dt">data =</span> pm_test,</a>
<a class="sourceLine" id="cb382-3" data-line-number="3">                         <span class="dt">method =</span> <span class="st">&quot;gbm&quot;</span>,</a>
<a class="sourceLine" id="cb382-4" data-line-number="4">                         <span class="dt">metric =</span> <span class="st">&quot;ROC&quot;</span>,</a>
<a class="sourceLine" id="cb382-5" data-line-number="5">                         <span class="dt">verbose =</span> <span class="ot">FALSE</span>,</a>
<a class="sourceLine" id="cb382-6" data-line-number="6">                         <span class="dt">tuneLength =</span> <span class="dv">10</span>,</a>
<a class="sourceLine" id="cb382-7" data-line-number="7">                         <span class="dt">trControl =</span> control)</a></code></pre></div>
<p>Let’s look at how they all performed:</p>
<div class="sourceCode" id="cb383"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb383-1" data-line-number="1"><span class="kw">cat</span>(<span class="st">&quot;Results for Rpart </span><span class="ch">\n</span><span class="st">&quot;</span>)</a></code></pre></div>
<pre><code>## Results for Rpart</code></pre>
<div class="sourceCode" id="cb385"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb385-1" data-line-number="1">rpart_caret<span class="op">$</span>results[<span class="dv">1</span>,]</a></code></pre></div>
<pre><code>##       cp  ROC  Sens  Spec ROCSD SensSD SpecSD
## 1 0.0288 0.66 0.803 0.417 0.133 0.0955  0.221</code></pre>
<div class="sourceCode" id="cb387"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb387-1" data-line-number="1"><span class="kw">cat</span>(<span class="st">&quot;Results for Bagging </span><span class="ch">\n</span><span class="st">&quot;</span>)</a></code></pre></div>
<pre><code>## Results for Bagging</code></pre>
<div class="sourceCode" id="cb389"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb389-1" data-line-number="1">bagging_caret<span class="op">$</span>results[<span class="dv">1</span>,]</a></code></pre></div>
<pre><code>##   parameter   ROC  Sens  Spec  ROCSD SensSD SpecSD
## 1      none 0.781 0.834 0.557 0.0976 0.0788   0.21</code></pre>
<div class="sourceCode" id="cb391"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb391-1" data-line-number="1"><span class="kw">cat</span>(<span class="st">&quot;Results for Random Forest </span><span class="ch">\n</span><span class="st">&quot;</span>)</a></code></pre></div>
<pre><code>## Results for Random Forest</code></pre>
<div class="sourceCode" id="cb393"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb393-1" data-line-number="1">rf_caret<span class="op">$</span>results[<span class="dv">1</span>,]</a></code></pre></div>
<pre><code>##   mtry   ROC  Sens Spec ROCSD SensSD SpecSD
## 1    2 0.814 0.864 0.55 0.146  0.104  0.239</code></pre>
<div class="sourceCode" id="cb395"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb395-1" data-line-number="1"><span class="kw">cat</span>(<span class="st">&quot;Results for Boosted Trees </span><span class="ch">\n</span><span class="st">&quot;</span>)</a></code></pre></div>
<pre><code>## Results for Boosted Trees</code></pre>
<div class="sourceCode" id="cb397"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb397-1" data-line-number="1">boosted_caret<span class="op">$</span>results[<span class="dv">1</span>,]</a></code></pre></div>
<pre><code>##   shrinkage interaction.depth n.minobsinnode n.trees   ROC  Sens  Spec ROCSD
## 1       0.1                 1             10      50 0.799 0.845 0.533 0.107
##   SensSD SpecSD
## 1   0.11  0.259</code></pre>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="classification-example.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="using-different-methods.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
